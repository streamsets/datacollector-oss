<?xml version="1.0" encoding="UTF-8"?>
<!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

-->
<!DOCTYPE task PUBLIC "-//OASIS//DTD DITA Task//EN" "task.dtd">
<task id="task_g1p_gqn_zx">
    <title>Configuring a Spark Evaluator</title>
    <taskbody>
        <context>
            <p><indexterm>Spark Evaluator
                processor<indexterm>configuring</indexterm></indexterm>Configure a Spark Evaluator
                to process data based on a custom Spark application.</p>
        </context>
        <steps>
            <step
                conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/1stStep-ReqField-StageLib">
                <cmd/>
            </step>
            <step>
                <cmd>On the <uicontrol>Spark</uicontrol> tab, configure the following
                    properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_ufq_xf4_zx">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Spark Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Parallelism</entry>
                                    <entry>For standalone mode, the number of partitions to create
                                        per batch of records. For example, if set to 4, then the
                                        Spark Transformer simultaneously runs 4 parallel jobs to
                                        process the batch. <p>Set the value based on the number of
                                            available processors on the <ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            /> machine.</p><note>Not used when the processor is
                                            included in a cluster pipeline. The Spark Transformer
                                            uses the number of partitions defined in the Kafka
                                            Consumer or MapR Streams Consumer origin.</note></entry>
                                </row>
                                <row>
                                    <entry>Application Name</entry>
                                    <entry>For standalone mode, the name of the Spark application.
                                        Spark displays this application name in the log files. <p>If
                                            you run pipelines that include multiple Spark Evaluator
                                            processors, be sure to use a unique application name for
                                            each to make debugging simpler.</p><p>Default is "SDC Spark App".
                                                <note>Not used when the processor is included in a cluster pipeline.</note></p></entry>
                                </row>
                                <row>
                                    <entry>Spark Transformer Class
                                        <xref
                                            href="../Processors/Spark-WriteApplication.dita#concept_lfl_dvd_1y">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_h4p_p5v_yq"/></xref></entry>
                                    <entry>Fully qualified name of the custom Spark class that
                                        implements the StreamSets Spark Transformer API. Enter the
                                        class name using the following
                                            format:<codeblock>com.streamsets.spark.&lt;custom class></codeblock><p>For
                                            example, let's assume that you developed a <codeph>GetCreditCardType</codeph> class that
                                            implemented the Spark Transformer API as follows:<codeblock>public class GetCreditCardType extends SparkTransformer implements Serializable {
...
}</codeblock></p><p>Then
                                            you would enter the class name as
                                            follows:<codeblock>com.streamsets.spark.GetCreditCardType</codeblock></p></entry>
                                </row>
                                <row>
                                    <entry>Init Method Arguments
                                        <xref
                                            href="../Processors/Spark-WriteApplication.dita#concept_lfl_dvd_1y">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_h3p_p6v_yq"/></xref></entry>
                                    <entry>Arguments to pass to the <codeph>init</codeph> method in
                                        the custom Spark class. Enter arguments as required by your
                                        custom Spark class.<p>In standalone mode, enter a constant
                                            or an expression for the argument value. In cluster
                                            mode, enter constant values only. In a cluster pipeline,
                                            the Spark Evaluator processor cannot evaluate Data
                                            Collector expressions defined in <codeph>init</codeph>
                                            method arguments.</p><p>Using <xref
                                                href="../Pipeline_Configuration/SimpleBulkEdit.dita#concept_alb_b3y_cbb"
                                                >simple or bulk edit mode</xref>, click the
                                                <uicontrol>Add</uicontrol> icon to add additional
                                            arguments.</p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
        </steps>
    </taskbody>
</task>
