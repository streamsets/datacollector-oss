<?xml version="1.0" encoding="UTF-8"?>
<!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_lfl_dvd_1y">
 <title>Developing the Spark Application</title>
 <conbody>
        <p><indexterm>Spark Evaluator processor<indexterm>writing the
                application</indexterm></indexterm><indexterm>Spark
                    application<indexterm>custom</indexterm></indexterm>To develop a custom Spark
            application, you write the Spark application and then package a JAR file containing the
            application.</p>
        <p>Use Java or Scala to write a custom Spark class that implements the StreamSets Spark
            Transformer API: <xref
                href="https://github.com/streamsets/datacollector-plugin-api/tree/master/streamsets-datacollector-spark-api"
                format="html" scope="external"/>.</p>
        <p>Include the following methods in the custom class:</p>
        <dl>
            <dlentry>
                <dt>init</dt>
                <dd>Optional. The <codeph>init</codeph> method is called once when the pipeline
                    starts to read arguments that you configure in the Spark Evaluator processor.
                    Use the <codeph>init</codeph> method to make connections to external systems or
                    to read configuration details or existing data from external systems.</dd>
            </dlentry>
            <dlentry>
                <dt>transform</dt>
                <dd>Required. The <codeph>transform</codeph> method is called for each batch of
                    records that the pipeline processes. The Spark Evaluator processor passes a
                    batch of data to the <codeph>transform</codeph> method as a Resilient
                    Distributed Dataset (RDD). The method processes the data according to the custom
                    code.</dd>
            </dlentry>
            <dlentry>
                <dt>destroy</dt>
                <dd>Optional. If you include an <codeph>init</codeph> method that makes connections
                    to external systems, you should also include the <codeph>destroy</codeph> method
                    to close the connections. The <codeph>destroy</codeph> method is called when the
                    pipeline stops.</dd>
            </dlentry>
        </dl>
        <p>When you finish writing the custom Spark class, package a JAR file containing the Spark
            application. Compile against the same stage library version that you use for the Spark
            Evaluator processor. For example, if you are using the Spark Evaluator processor
            included in the stage library for the Cloudera CDH version 5.9 distribution of Hadoop,
            build the application against Spark integrated into Cloudera CDH version 5.9.</p>
        <p>If you used Scala to write the custom Spark class, build the application so that it is
            compatible with Scala 2.10.</p>
    </conbody>
</concept>
