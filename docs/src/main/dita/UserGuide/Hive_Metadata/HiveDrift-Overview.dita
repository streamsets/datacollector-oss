<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
  
      http://www.apache.org/licenses/LICENSE-2.0
      
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_phk_bdf_2w">
    <title>Hive Drift Solution: Ingesting Drifting Data into Hive</title>
    <conbody>
        <p><indexterm>Hive Drift Solution<indexterm>overview</indexterm></indexterm>The Hive Drift
            Solution detects drift in incoming data and updates the corresponding Hive tables. The
            solution enables creating and updating Hive tables based on record requirements and
            writing data based on stage attributes in record headers. You can use the full
            functionality of the solution or individual pieces, as needed. </p>
        <p>You can use the Hive Drift Solution to process Avro data at this time. </p>
        <p>The solution incorporates the Hive Metadata processor, Hive Metastore destination, and
            the Hadoop FS destination as follows:</p>
        <p>
            <dl>
                <dlentry>
                    <dt>Drift detection</dt>
                    <dd>When processing records, the Hive Metadata processor detects columnar drift
                        and the need for new tables and partitions. It generates metadata records
                        that describe the necessary changes. </dd>
                    <dd>When the Hive Metastore destination receives a metadata record, it compares
                        the proposed changes with the latest Hive metadata, and creates and updates
                        Hive tables as needed.</dd>
                    <dd><ph
                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/D-HM-CreatesAndNot_PH"
                        /></dd>
                </dlentry>
                <dlentry>
                    <dt>Record-based writes</dt>
                    <dd>The Hive Metadata processor also adds the following information to the
                        header of each record. The Hadoop FS destination writes data to HDFS based
                        on these details: <ul id="ul_sn1_bjg_2w">
                            <li>Target directory - Based on user-defined expressions, the Hive
                                Metadata processor assembles the path on HDFS where each record
                                should be stored. It writes the generated path to a
                                "targetDirectory" stage attribute in each record header.<p>To write
                                    the record to the generated path, configure Hadoop FS to use the
                                    targetDirectory stage attribute. </p></li>
                            <li>Avro schema - The processor writes the Avro schema to the
                                "avroSchema" stage attribute in each record header. It generates new
                                Avro schemas when necessary based on the record structure.<p>To use
                                    the generated Avro schema, configure Hadoop FS to use the
                                    avroSchema stage attribute.</p></li>
                            <li>Roll files - When a change in Avro schema occurs, the processor
                                generates a roll indicator - the "roll" stage attribute. <p>To roll
                                    files based on schema changes, configure Hadoop FS use the roll
                                    stage attribute.</p></li>
                        </ul></dd>
                </dlentry>
            </dl>
        </p>
        <p>For example, say you use this solution to write sales data to Hive. A partial upgrade of
            the sales system adds several new fields to a subset of the incoming data. </p>
        <p>With this drift solution, the Hive Metadata processor notes the new fields in a metadata
            record and passes it to the Hive Metastore destination. The Hive Metastore destination
            adds the new columns to the Hive target table. Hadoop FS then writes the data to the
            updated table. When writing data without the new fields to the updated table, Hadoop FS
            inserts null values for the missing fields. </p>
    </conbody>
</concept>
