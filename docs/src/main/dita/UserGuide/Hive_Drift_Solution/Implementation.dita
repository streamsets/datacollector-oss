<?xml version="1.0" encoding="UTF-8"?>
<!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_y5w_dj3_fw">
 <title>Implementation Steps</title>
 <conbody>
  <p><indexterm>Drift Synchronization Solution for
                Hive<indexterm>implementation</indexterm></indexterm>To implement the <ph
                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/HiveDrift-ph"/>,
            perform the following steps:<ol id="ol_zr1_3j3_fw">
                <li> Configure the origin and any additional processors that you want to use. <ul
                        id="ul_qjn_kp4_hx">
                        <li>If using the JDBC Query Consumer as the origin, enable the creation of
                            JDBC header attributes. For more information, see <xref
                                href="../Origins/JDBCConsumer-HeaderHDS.dita#concept_tvf_tgp_fx"/>. </li>
                        <li>If data includes records with nested fields, add a Field Flattener to
                            flatten records before passing them to the Hive Metadata processor.</li>
                    </ul></li>
                <li>To capture columnar drift and to enable record-based writes, configure the Hive
                    Metadata processor:<ul id="ul_qsk_5x3_fw">
                        <li>Configure the Hive connection information.</li>
                        <li>Configure the database, table, and partition expressions.<p>You can
                                enter a single name or use an expression that evaluates to the names
                                to use. If necessary, you can use an Expression Evaluator earlier in
                                the pipeline to write the information to a record field or record
                                header attribute.</p></li>
                        <li>Configure the decimal field precision and scale expressions.<p>You can
                                use constants or expressions that evaluate to the same precision and
                                scale for all decimal fields. Or, you can create more complex
                                expressions that evaluate to different values for different fields.
                                </p><p>When processing data from the JDBC Query Consumer or the JDBC
                                Multitable Consumer with JDBC header attributes, use the default
                                expressions. </p></li>
                        <li>Specify the data format to use, Avro or Parquet.</li>
                        <li>Optionally configure advanced options, such as the maximum cache size,
                            time basis, and data time zone.</li>
                    </ul><p>For more information about the Hive Metadata processor, see <xref
                            href="../Processors/HiveMetadata.dita#concept_rz5_nft_zv"/>.</p></li>
                <li>To process metadata records generated by the processor and alter tables as
                    needed, connect the metadata output of the Hive Metadata processor to the Hive
                    Metastore destination.<p>
                        <note>While you might filter or route some records away from the Hive
                            Metastore destination, the destination must receive metadata records to
                            update Hive tables.</note>
                    </p></li>
                <li>Configure the Hive Metastore destination:<ul id="ul_k12_ty3_fw">
                        <li>Configure the Hive connection information.</li>
                        <li>Optionally configure cache information and how tables are updated.</li>
                    </ul><p>For more information about the Hive Metastore destination, see <xref
                            href="../Destinations/HiveMetastore.dita#concept_gcr_z2t_zv"/>.</p></li>
                <li>Connect the data output of the Hive Metadata processor to the Hadoop FS or MapR
                    FS destination to write records to the destination system using record header
                    attributes.</li>
                <li>Configure the Hadoop FS or MapR FS destination:<ol id="ul_n4p_dz3_fw">
                        <li>To write records using the targetDirectory header attribute, on the
                            Output Files tab, select Directory in Header.</li>
                        <li>To roll records based on a roll header attribute, on the Output Files
                            tab, select Use Roll Attribute, and for Roll Attribute Name, enter
                            “roll”.</li>
                        <li>To write records using the avroSchema header attribute, on the Data
                            Format tab, select the Avro data format, and then for the Avro Schema
                            Location property, select In Record Header.</li>
                    </ol><p>For more information about using record header attributes, see <xref
                            href="../Pipeline_Design/RecordBasedWrites-overview.dita#concept_lmn_gdc_1w"
                        />.</p><p>
                        <note>To compress Avro data, use the Avro compression option on the Data
                            Formats tab, rather than the compression codec property on the Output
                            Files tab.</note>
                    </p></li>
                <li>When processing Parquet data, perform the following additional steps: <ol
                        id="ul_bsd_v52_wz">
                        <li>On the General tab of the data-processing destination, select Produce
                            Events.</li>
                        <li>Connect a MapReduce executor to the resulting event stream and configure
                            the necessary connection information for the stage.</li>
                        <li>On the Jobs tab of the MapReduce executor, select the Convert Avro to
                            Parquet job type and add any additional job parameters that are
                            required.</li>
                        <li>On the Avro to Parquet tab, use the default Input Avro File
                            configuration, specify the Output Directory to use and optionally
                            configure the additional job properties. <ul id="ul_ykw_gfc_11b">
                                <li>Use the default Input Avro File expression - This allows the
                                    executor to process the file that the data processing
                                    destination just closed.</li>
                                <li>Specify the Output Directory to use - To write the Parquet files
                                    to the parent directory of the .avro temporary directory, use
                                    the following
                                    expression:<codeblock>${file:parentPath(file:parentPath(record:value('/filepath')))}</codeblock></li>
                                <li>Optionally configure the additional job properties.</li>
                            </ul></li>
                    </ol></li>
            </ol></p>
 </conbody>
</concept>
