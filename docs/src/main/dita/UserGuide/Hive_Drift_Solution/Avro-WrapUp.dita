<?xml version="1.0" encoding="UTF-8"?>
<!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_jlr_zlk_gw">
 <title>Processing Avro Data</title>
    <shortdesc>Now what happens when you start the pipeline?</shortdesc>
    <conbody>
        <p>This pipeline is set up to write data to different tables based on the table name in the
            "tag" attribute that was added to the record headers in the earlier pipeline. </p>
        <p>Say the table names are "weblog" and "service". For each record with "weblog" as the tag
            attribute, the Hive Metadata processor evaluates the fields in the record as follows:
                <ul id="ul_iml_1mk_gw">
                <li>If the fields match the existing Hive table, it just writes the necessary
                    information into the targetDirectory and avroSchema stage attributes, and Hadoop
                    FS writes the record to the weblog table.</li>
                <li>If a record includes a new field, the processor generates a metadata record that
                    the Hive Metastore destination uses to update the weblog table to include the
                    new column. It also writes information to stage attributes so Hadoop FS can
                    write the record to the updated weblog table.</li>
                <li>If a record has missing fields, the processor just writes information to stage
                    attributes, and Hadoop FS writes the record to HDFS with null values for the
                    missing fields.</li>
                <li>If a field has been renamed, the processor treats the field as a new field,
                    generating a metadata record that the Hive Metastore destination uses to update
                    the weblog table. When Hadoop FS writes the record, data is written to the new
                    field and a null value to the old field.</li>
                <li>If a data type changes for an existing field, the processor treats the record as
                    an error record.</li>
            </ul></p>
        <p>For each record with a "service" tag, the processor performs the same actions.</p>
        <note>If a record includes a new tag value, the Hive Metadata processor generates a metadata
            record that the Hive Metastore destination uses to create a new table. And Hadoop FS
            writes the record to the new table. So if you spin up a new web service, you don't need
            to touch this pipeline to have it handle the new data set. </note>
    </conbody>
</concept>
