<?xml version="1.0" encoding="UTF-8"?>
<!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_fzk_mmn_fw">
 <title>The Hive Metadata Processor</title>
 <conbody>
  <p>You set up the Kafka Consumer and connect it to the Hive Metadata processor. When you configure
            the processor, you have a few things to consider in addition to the basic connection
            details: <ol id="ol_fzm_bmv_fw">
                <li>Which database should the records be written to? <p>Hadoop FS will do the
                        writing, but the processor needs to know where the records should
                        go.</p><p>Let's write to the Hive default database. To do that, you can
                        leave the database property empty.</p></li>
                <li>What tables should the records be written to?<p>The pipeline supplying the data
                        to Kafka uses the "tag" header attribute to indicate the originating web
                        service. To use the tag attribute to write to tables, you use the following
                        expression for the table name:
                        <codeblock>${record:attribute('tag')}</codeblock></p></li>
                <li>What partitions, if any, do you want to use? <p>Let's create daily partitions
                        using datetime variables for the partition value expression as
                        follows:<codeblock>${YYYY()}-${MM()}-${DD()}</codeblock></p></li>
                <li>How do you want to configure the precision and scale for decimal fields?
                        <p>Though the data from the web services contains no decimal data that you
                        are aware of, to prevent new decimal data from generating error records,
                        configure the decimal field expressions. </p><p>The default expressions are
                        for data generated by the JDBC Query Consumer or the JDBC Multitable
                        Consumer. You can replace them with other expressions or with constants.
                    </p></li>
                <li>What type of data is being processed?<p>On the Data Format tab, select the Avro
                        data format.</p></li>
            </ol></p>
        <p> At this point, your pipeline would look like this: </p>
        <p><image href="../Graphics/HiveMeta-Ex-Processor.png" id="image_g5b_34n_fw" scale="55"
            /></p>
        <p>With this configuration, the Hadoop FS destination will write every record to the Hive
            table listed in the tag attribute and to the daily partition based on the time of
            processing.</p>
 </conbody>
</concept>
