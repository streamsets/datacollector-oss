<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_vhs_5tz_xp">
      <title>Reusable phrases for the book</title>
      <shortdesc>Use the following reusable phrases for conrefs in the book. <draft-comment
                  author="Loretta">Company name - not really using this</draft-comment></shortdesc>
      <conbody>
            <p>
                  <note>Do NOT use conrefs in headings - it causes problems in the webhelp
                        navigation panel.</note>
            </p>
            <p>Company name: <ph id="company">StreamSets</ph></p>
            <draft-comment author="Loretta">product name: trying to consistently use
                  this</draft-comment>
            <p>Full, init cap version of the product name: <ph id="pName-long">Data
                  Collector</ph></p>
            <p>DPM long only: <ph id="DPM-LongOnly">Dataflow Performance Manager</ph></p>
            <p>DPM short: <ph id="DPM-short">DPM</ph></p>
            <p>
                  <draft-comment author="Loretta"><uicontrol>ph-VaultFunctions-Usage</uicontrol> -
                        used in Vault-Step3-CallVault.dita</draft-comment>
            </p>
            <p><ph id="ph-VaultFunctions-Usage">username, password, and similar properties such as
                        AWS access key IDs and secret access keys. You can also use the functions in
                        HTTP headers and bodies when using HTTPS.</ph></p>
            <p>
                  <draft-comment author="Loretta"><uicontrol>ph-VaultOneFunction-Usage</uicontrol> -
                        used in Misc Functions for each Vault function</draft-comment>
            </p>
            <p><ph id="ph-VaultOneFunction-Usage">username, password, and similar properties such as
                        AWS access key IDs and secret access keys. You can also use the function in
                        HTTP headers and bodies when using HTTPS.</ph></p>
            <p>
                  <draft-comment author="Loretta">List of origins that are part of the Basic
                        installation. Used in BasicInstall_Overview and BasicInstall-LibList
                        (Available Stage Libraries).</draft-comment>
            </p>
            <p>
                  <ul id="BasicOrigins">
                        <li>Directory</li>
                        <li>File Tail</li>
                        <li>HTTP Client</li>
                        <li>SDC RPC</li>
                        <li>SFTP/FTP Client</li>
                        <li>UDP Source</li>
                  </ul>
            </p>
            <draft-comment author="Loretta">All processors except... </draft-comment>
            <p><ph id="BasicNotProcessor">except the Groovy Evaluator, Jython Evaluator, HBase
                        Lookup, and Redis Lookup.</ph></p>
            <p>
                  <draft-comment author="Loretta">List of destinations tha are part of the Basic
                        installation. Used in BasicInstall_Overview and BasicInstall-LibList.
                  </draft-comment>
            </p>
            <p>
                  <ul id="BasicDestinations">
                        <li>Local FS</li>
                        <li>SDC RPC</li>
                        <li>To Error</li>
                        <li>Trash</li>
                  </ul>
            </p>
            <p>
                  <draft-comment author="Loretta">KafkaSecure - used in KConsumer &amp; Producer
                        encryption areas. Original value: Kafka 0.9.0.0</draft-comment>
            </p>
            <p>Kafka version that supports SSL and SASL/Kerberos security: <ph id="KafkaSecure"
                        >Kafka 0.9.0.0</ph></p>
            <draft-comment author="Loretta">Using this in all DC Console topics:</draft-comment>
            <p>
                  <note id="Note-OptionDispay">Some icons and options might not display in the
                        console. The items that display are based on the task that you are
                        performing and roles assigned to your user account. </note>
            </p>
            <p>
                  <draft-comment author="Loretta">
                        <p>The following bullets are used in "Previewing a Single Stage" and
                              "Troubleshooting":</p>
                  </draft-comment>
            </p>
            <ul id="ul_EditPreview">
                  <li>The output data column for an origin.</li>
                  <li>The input data column for processors.</li>
            </ul>
            <p>
                  <draft-comment author="Loretta">The following bullets are the types of origins
                        that can be reset / that remember where you left off. These are used
                        currently in "Starting a Pipeline" and "Resetting an
                        Origin":</draft-comment>
                  <ul id="ul_saveOffset">
                        <li>Directory</li>
                        <li>Kafka Consumer</li>
                  </ul>
            </p>
            <p>
                  <draft-comment author="Loretta">The following bullets are CSV file types. Used in
                        "Configuring a Directory Origin", Directory-Data Formats and "Configuring
                        the Kafka Consumer" -- Make sure changes to this are copied to the Delimited
                        data format info later in this SAME FILE.</draft-comment>
            </p>
            <p>
                  <ul id="ul_delFileTypes">
                        <li><uicontrol>Default CSV</uicontrol> - File that includes comma-separated
                              values. Ignores empty lines in the file.</li>
                        <li><uicontrol>RFC4180 CSV</uicontrol> - Comma-separated file that strictly
                              follows RFC4180 guidelines.</li>
                        <li><uicontrol>MS Excel CSV</uicontrol> - Microsoft Excel comma-separated
                              file.</li>
                        <li><uicontrol>MySQL CSV</uicontrol> - MySQL comma separated file.</li>
                        <li><uicontrol>Tab-Separated Values</uicontrol> - File that includes
                              tab-separated values.</li>
                        <li><uicontrol>Custom</uicontrol> - File that uses user-defined delimiter,
                              escape, and quote characters.</li>
                  </ul>
            </p>
            <draft-comment author="Loretta">Use the following for invoking the expression editor. So
                  far, using in Config Expression Evaluator, Config Stream Selector, and Expression
                  Editor:</draft-comment>
            <p id="EEditor">Optionally, click <uicontrol>Ctrl + Space Bar</uicontrol> for help with
                  creating the expression. </p>
            <p>
                  <draft-comment author="Loretta">Using the following in the Configuring topic for
                        several processors (Expression Evaluator, Field Converter, Field Hasher, -
                        whichever ones allow wildcard use:</draft-comment>
            </p>
            <p id="wildcard">You can use the asterisk wildcard to represent array indices and map
                  elements. <xref
                        href="../Pipeline_Configuration/WildcardsArraysMaps.dita#concept_vqr_sqc_wr"
                              ><image href="../Graphics/icon_moreInfo.png" scale="10"/>
                  </xref>
            </p>
            <p>
                  <draft-comment author="Loretta"><uicontrol>DataFormats-ALL</uicontrol> - <b>no
                              longer all.</b> As of 7/14, Kafka Consumer has datagram, but no one
                        else does. Used in MapR Streams Consumer, possibly others.</draft-comment>
            </p>
            <p>
                  <ul id="DataFormats-ALL">
                        <li>Avro</li>
                        <li>Binary</li>
                        <li>Delimited</li>
                        <li>JSON</li>
                        <li>Log</li>
                        <li>Text</li>
                        <li>Protobuf</li>
                        <li>SDC Record <xref
                                    href="../Pipeline_Design/SDCRecordFormat.dita#concept_qkk_mwk_br">
                                    <image href="../Graphics/icon_moreInfo.png" scale="11"
                                          id="image_wjh_ycl_br"/></xref></li>
                        <li>XML</li>
                  </ul>
            </p>
            <p/>
            <p>
                  <draft-comment author="Loretta">DataFormats-Kafka - used in Kafka
                        Consumer</draft-comment>
            </p>
            <p>
                  <ul id="DataFormats-Kafka">
                        <li>Avro</li>
                        <li>Binary</li>
                        <li>Datagram</li>
                        <li>Delimited</li>
                        <li>JSON</li>
                        <li>Log</li>
                        <li>Text</li>
                        <li>Protobuf</li>
                        <li>SDC Record <xref
                                    href="../Pipeline_Design/SDCRecordFormat.dita#concept_qkk_mwk_br">
                                    <image href="../Graphics/icon_moreInfo.png" scale="11"
                                          id="image_drz_1nf_qw"/></xref></li>
                        <li>XML</li>
                  </ul>
            </p>
            <p/>
            <p>
                  <draft-comment author="Loretta">ORIGIN DATA FORMATS</draft-comment>
            </p>
            <p>
                  <draft-comment author="Loretta">AVRO - Used by Kafka Consumer, and Kinesis
                        Consumer and other message origins</draft-comment>
                  <dl>
                        <dlentry id="OriginDF-AVRO">
                              <dt>Avro</dt>
                              <dd>Generates a record for every message. </dd>
                              <dd>To ensure proper data processing, indicate if the message includes
                                    an Avro schema. </dd>
                              <dd>You can use the schema associated with the message or provide an
                                    alternate schema definition. Providing an alternate schema can
                                    improve performance.</dd>
                        </dlentry>
                  </dl>
            </p>
            <p>
                  <draft-comment author="Loretta">AVRO-Files - used by Directory and Amazon S3 and
                        other file origins except Hadoop FS</draft-comment>
            </p>
            <p>
                  <dl>
                        <dlentry id="OriginDF-AVROfiles">
                              <dt>Avro</dt>
                              <dd>Generates a record for every record. </dd>
                              <dd>The origin assumes each file includes an Avro schema definition.
                                    But you can optionally provide an alternate schema. </dd>
                              <dd>The origin reads files compressed by Avro-supported compression
                                    codecs without requiring additional configuration. To enable the
                                    origin to read files compressed by other codecs, use the
                                    compression format property in the stage.</dd>
                        </dlentry>
                  </dl>
            </p>
            <p>
                  <draft-comment author="Loretta">AVRO-HadoopFS - used by Hadoop FS origin -
                        MapReduce doesn't allow getting the schema from the files. Not adding the
                        avro-compression-codec </draft-comment>
            </p>
            <p>
                  <dl>
                        <dlentry id="AVRO-HadoopFS">
                              <dt>Avro</dt>
                              <dd>Generates a record for every record. </dd>
                              <dd>The origin uses the specified schema definition to process Avro
                                    data.</dd>
                              <dd>The origin reads files compressed by Avro-supported compression
                                    codecs without requiring additional configuration.</dd>
                        </dlentry>
                  </dl>
            </p>
            <p>
                  <draft-comment author="Loretta">OriginDF-Binary - used by Kafka Consumer and
                        Kinesis Consumer</draft-comment>
            </p>
            <p>
                  <dl>
                        <dlentry id="OriginDF-Binary">
                              <dt>Binary</dt>
                              <dd>Generates a record with a single byte array field at the root of
                                    the record. </dd>
                              <dd>When the data exceeds the user-defined maximum data size, the
                                    origin cannot process the data. Because the record is not
                                    created, the origin cannot pass the record to the pipeline to be
                                    written as an error record. Instead, the origin generates a
                                    stage error. </dd>
                        </dlentry>
                  </dl>
            </p>
            <p>
                  <draft-comment author="Loretta">O-DF-Datagram - Kafka Consumer only at this
                        point.</draft-comment>
            </p>
            <p>
                  <dl>
                        <dlentry id="O-DF-Datagram">
                              <dt>Datagram</dt>
                              <dd>Generates a record for every message. You can use the following
                                    datagram format types: collectd, NetFlow, or syslog. </dd>
                        </dlentry>
                  </dl>
            </p>
            <p>
                  <draft-comment author="Loretta"> - from Directory. DLEntries are conrefed by all
                        origins except HTTP Client. <note> Content for JSON, Text, and HTML copied
                              to HTTP Client and altered. When making changes here, check there as
                              well.</note></draft-comment>
            </p>
            <p>
                  <dl id="ORIGIN-DFormats">
                        <dlentry id="OriginDF-DELIM">
                              <dt>Delimited</dt>
                              <dd>Generates a record for each delimited line. You can use the
                                    following delimited format types:<ul id="ul_c12_1k2_gt">
                                          <li><uicontrol>Default CSV</uicontrol> - File that
                                                includes comma-separated values. Ignores empty lines
                                                in the file.</li>
                                          <li><uicontrol>RFC4180 CSV</uicontrol> - Comma-separated
                                                file that strictly follows RFC4180 guidelines.</li>
                                          <li><uicontrol>MS Excel CSV</uicontrol> - Microsoft Excel
                                                comma-separated file.</li>
                                          <li><uicontrol>MySQL CSV</uicontrol> - MySQL comma
                                                separated file.</li>
                                          <li><uicontrol>Tab-Separated Values</uicontrol> - File
                                                that includes tab-separated values.</li>
                                          <li><uicontrol>Custom</uicontrol> - File that uses
                                                user-defined delimiter, escape, and quote
                                                characters.</li>
                                    </ul></dd>
                              <dd>You can use a list or list-map root field type for delimited data,
                                    optionally including the header information when available. </dd>
                              <dd>You can also replace a string constant with null values.</dd>
                              <dd>When a record exceeds the maximum record length defined for the
                                    origin, the origin processes the object based on the error
                                    handling configured for the stage.</dd>
                              <dd>For more information about the root field types, see <xref
                                          href="../Pipeline_Design/DelimitedDataRootFieldTypes.dita#concept_zcg_bm4_fs"
                                    />.</dd>
                        </dlentry>
                        <dlentry id="OriginDF-DELIMFILE">
                              <dt>Delimited</dt>
                              <dd>Generates a record for each delimited line. You can use the
                                    following delimited format types:<ul id="ul_hbd_f2p_ht">
                                          <li><uicontrol>Default CSV</uicontrol> - File that
                                                includes comma-separated values. Ignores empty lines
                                                in the file.</li>
                                          <li><uicontrol>RFC4180 CSV</uicontrol> - Comma-separated
                                                file that strictly follows RFC4180 guidelines.</li>
                                          <li><uicontrol>MS Excel CSV</uicontrol> - Microsoft Excel
                                                comma-separated file.</li>
                                          <li><uicontrol>MySQL CSV</uicontrol> - MySQL comma
                                                separated file.</li>
                                          <li><uicontrol>Tab-Separated Values</uicontrol> - File
                                                that includes tab-separated values.</li>
                                          <li><uicontrol>Custom</uicontrol> - File that uses
                                                user-defined delimiter, escape, and quote
                                                characters.</li>
                                    </ul></dd>
                              <dd>You can use a list or list-map root field type for delimited data,
                                    optionally including the header information when available. </dd>
                              <dd>You can also replace a string constant with null values.</dd>
                              <dd>When a record exceeds the user-defined maximum record length, the
                                    origin cannot continue processing data in the file. Records
                                    already processed from the file are passed to the pipeline. The
                                    behavior of the origin is then based on the error handling
                                    configured for the stage:<ul id="ul_it3_g2p_ht">
                                          <li>Discard - The origin continues processing with the
                                                next file, leaving the partially-processed file in
                                                the directory. </li>
                                          <li>To Error - The origin continues processing with the
                                                next file. If a post-processing error directory is
                                                configured for the stage, the origin moves the
                                                partially-processed file to the error directory.
                                                Otherwise, it leaves the file in the directory.</li>
                                          <li>Stop Pipeline - The origin stops the pipeline. </li>
                                    </ul></dd>
                              <dd>For more information about the root field types, see <xref
                                          href="../Pipeline_Design/DelimitedDataRootFieldTypes.dita#concept_zcg_bm4_fs"
                                    />.</dd>
                        </dlentry>
                        <dlentry id="OriginDF-JSON">
                              <dt>JSON</dt>
                              <dd>Generates a record for each JSON object. You can process JSON
                                    files that include multiple JSON objects or a single JSON
                                    array.</dd>
                              <dd>When an object exceeds the maximum object length defined for the
                                    origin, the origin processes the object based on the error
                                    handling configured for the stage. </dd>
                        </dlentry>
                        <dlentry id="OriginDF-JSONFILE">
                              <dt>JSON</dt>
                              <dd>Generates a record for each JSON object. You can process JSON
                                    files that include multiple JSON objects or a single JSON
                                    array.</dd>
                              <dd>When an object exceeds the maximum object length defined for the
                                    origin, the origin cannot continue processing data in the file.
                                    Records already processed from the file are passed to the
                                    pipeline. The behavior of the origin is then based on the error
                                    handling configured for the stage:<ul id="ul_sjn_fxf_ht">
                                          <li>Discard - The origin continues processing with the
                                                next file, leaving the partially-processed file in
                                                the directory. </li>
                                          <li>To Error - The origin continues processing with the
                                                next file. If a post-processing error directory is
                                                configured for the stage, the origin moves the
                                                partially-processed file to the error directory.
                                                Otherwise, it leaves the file in the directory.</li>
                                          <li>Stop Pipeline - The origin stops the pipeline. </li>
                                    </ul></dd>
                        </dlentry>
                        <dlentry id="OriginDF-LOG">
                              <dt>Log</dt>
                              <dd>Generates a record for every log line. </dd>
                              <dd>When a line exceeds the user-defined maximum line length, the
                                    origin truncates longer lines. </dd>
                              <dd>You can include the processed log line as a field in the record.
                                    If the log line is truncated, and you request the log line in
                                    the record, the origin includes the truncated line.</dd>
                              <dd>You can define the log format or type to be read.</dd>
                        </dlentry>
                  </dl>
            </p>
            <p>
                  <draft-comment author="Loretta">OriginDF-ProtoMessage - JMS Consumer, Kinesis
                        Consumer, Kafka Consumer, RabbitMQ</draft-comment>
                  <dl>
                        <dlentry id="OriginDF-ProtoMessage">
                              <dt>Protobuf</dt>
                              <dd>Generates a record for every protobuf message. By default, the
                                    origin assumes messages contain multiple protobuf messages.</dd>
                              <dd>Protobuf messages must match the specified message type and be
                                    described in the descriptor file. </dd>
                              <dd>When the data for a record exceeds 1 MB, the origin cannot
                                    continue processing data in the message. The origin handles the
                                    message based on the stage error handling property and continues
                                    reading the next message. </dd>
                              <dd>For information about generating the descriptor file, see <xref
                                          href="../Pipeline_Design/Protobuf-Prerequisites.dita"
                                    />.</dd>
                        </dlentry>
                  </dl>
            </p>
            <draft-comment author="Loretta">OriginDF-ProtoFile - Amazon S3, Directory? Hadoop
                  FS?</draft-comment>
            <dl>
                  <dlentry id="OriginDF-ProtoFile">
                        <dt>Protobuf</dt>
                        <dd>Generates a record for every protobuf message. </dd>
                        <dd>Protobuf messages must match the specified message type and be described
                              in the descriptor file. </dd>
                        <dd>When the data for a record exceeds 1 MB, the origin cannot continue
                              processing data in the file. The origin handles the file based on file
                              error handling properties and continues reading the next file. </dd>
                        <dd>For information about generating the descriptor file, see <xref
                                    href="../Pipeline_Design/Protobuf-Prerequisites.dita"/>.</dd>
                  </dlentry>
            </dl>
            <p>
                  <dl>
                        <dlentry id="OriginDF-SDC">
                              <dt>SDC Record</dt>
                              <dd>Generates a record for every record. Use to process records
                                    generated by a <ph
                                          conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                    /> pipeline using the SDC Record data format.</dd>
                              <dd>For error records, the origin provides the original record as read
                                    from the origin in the original pipeline, as well as error
                                    information that you can use to correct the record. </dd>
                              <dd>When processing error records, the origin expects the error file
                                    names and contents as generated by the original pipeline.</dd>
                        </dlentry>
                        <dlentry id="OriginDF-TEXT">
                              <dt>Text</dt>
                              <dd>Generates a record for each line of text or for each section of
                                    text based on a custom delimiter.</dd>
                              <dd>When a line or section exceeds the maximum line length defined for
                                    the origin, the origin truncates it. The origin adds a boolean
                                    field named Truncated to indicate if the line was
                                    truncated.</dd>
                              <dd>For more information about processing text with a custom
                                    delimiter, see <xref
                                          href="../Pipeline_Design/TextCDelim.dita#concept_lg2_gcg_jx"
                                    />.</dd>
                        </dlentry>
                  </dl>
                  <draft-comment author="Loretta">This is for JMS Consumer only.</draft-comment>
                  <dl>
                        <dlentry id="OriginDF-JMS-TEXT">
                              <dt>Text</dt>
                              <dd>Generates a record for each line of text or for each section of
                                    text based on a custom delimiter. Reads text data of the
                                    BytesMessage format. </dd>
                              <dd>When a line or section exceeds the maximum line length defined for
                                    the origin, the origin truncates it. The origin adds a boolean
                                    field named Truncated to indicate if the line was
                                    truncated.</dd>
                              <dd>For more information about processing text with a custom
                                    delimiter, see <xref
                                          href="../Pipeline_Design/TextCDelim.dita#concept_lg2_gcg_jx"
                                    />.</dd>
                        </dlentry>
                        <dlentry>
                              <dt/>
                              <dd>
                                    <draft-comment author="Loretta"
                                                ><uicontrol>OriginDF-WFile</uicontrol> - no
                                          checksum. Used by Directory</draft-comment>
                              </dd>
                        </dlentry>
                  </dl>
                  <dl>
                        <dlentry id="OriginDF-WFile">
                              <dt>Whole File</dt>
                              <dd>Streams whole files from the origin system to the destination
                                    system. </dd>
                              <dd>The origin generates two fields: one for a file reference and one
                                    for file information. For more information, see <xref
                                          href="../Pipeline_Design/WholeFile.dita#concept_nfc_qkh_xw"
                                    />.</dd>
                        </dlentry>
                        <dlentry>
                              <dt/>
                              <dd>
                                    <draft-comment author="Loretta"
                                                ><uicontrol>OriginDF-WFile-Checksum</uicontrol> -
                                          used in s3 origin</draft-comment>
                              </dd>
                        </dlentry>
                        <dlentry id="OriginDF-WFile-Checksum">
                              <dt>Whole File</dt>
                              <dd>Streams whole files from the origin system to the destination
                                    system. The origin uses checksums to verify the integrity of
                                    data transmission.</dd>
                              <dd>The origin generates two fields: one for a file reference and one
                                    for file information. For more information, see <xref
                                          href="../Pipeline_Design/WholeFile.dita#concept_nfc_qkh_xw"
                                    />.</dd>
                        </dlentry>
                        <dlentry id="OriginDF-XML">
                              <dt>XML</dt>
                              <dd>Generates records based on the location of the XML element that
                                    you define as the record delimiter. If you do not define a
                                    delimiter element, the origin treats the XML file as a single
                                    record.</dd>
                              <dd>When a record exceeds the user-defined maximum record length, the
                                    origin skips the record and continues processing with the next
                                    record. It sends the skipped record to the pipeline for error
                                    handling. </dd>
                              <dd>If you want to process invalid XML documents, you can try using
                                    the text data format with custom delimiters. For more
                                    information, see <xref
                                          href="../Pipeline_Design/TextCDelim-XMLdata.dita#concept_okt_kmg_jx"
                                    />.</dd>
                        </dlentry>
                        <dlentry id="OriginDF-XMLFILE">
                              <dt>XML</dt>
                              <dd>Generates records based on the location of the XML element that
                                    you define as the record delimiter. If you do not define a
                                    delimiter element, the origin treats the XML file as a single
                                    record.</dd>
                              <dd>When a record exceeds the user-defined maximum record length, the
                                    origin cannot continue processing data in the file. Records
                                    already processed from the file are passed to the pipeline. The
                                    behavior of the origin is then based on the error handling
                                    configured for the stage:<ul id="ul_rnb_qdp_ht">
                                          <li>Discard - The origin continues processing with the
                                                next file, leaving the partially-processed file in
                                                the directory. </li>
                                          <li>To Error - The origin continues processing with the
                                                next file. If a post-processing error directory is
                                                configured for the stage, the origin moves the
                                                partially-processed file to the error directory.
                                                Otherwise, it leaves the file in the directory.</li>
                                          <li>Stop Pipeline - The origin stops the pipeline. </li>
                                    </ul>If you want to process invalid XML documents, you can try
                                    using the text data format with custom delimiters. For more
                                    information, see <xref
                                          href="../Pipeline_Design/TextCDelim-XMLdata.dita#concept_okt_kmg_jx"
                                    />.</dd>
                        </dlentry>
                  </dl>
            </p>
            <p>
                  <draft-comment author="Loretta">S3origin-CopyMove1 and 2 used in Amazon S3 origin
                        Configuring.</draft-comment>
            </p>
            <p id="S3origin-CopyMove1">You can copy or move the object to another prefix or bucket.
                  When you use another prefix, enter the prefix. When you use another bucket, enter
                  a prefix and bucket.</p>
            <p id="S3origin-CopyMove2">Copying the object leaves the original object in place. </p>
            <p>
                  <draft-comment author="Loretta">JDBC Consumer and Oracle CDC use these in the
                        overview:</draft-comment>
            </p>
            <p id="JDBC-legacyInfo"><ph id="ph-JDBC-legacyInfo">To use a JDBC version older than
                        4.0, you can specify the driver class name and define a health check
                        query.</ph></p>
            <p>
                  <draft-comment author="Loretta">Oracle CDC Client uses this in Initial Change and
                        Configuring</draft-comment>
            </p>
            <p><ph id="OracleCDC-DateFormat">Use the following format: <codeph>DD-MM-YYYY
                              HH24:MI:SS</codeph>.</ph></p>
            <p>
                  <draft-comment author="Loretta">Oracle CDC Client uses this in the overview and
                        Initial Change.</draft-comment>
            </p>
            <p><ph><ph id="OracleCDC-fulldata"><ph>use the JDBC Consumer origin in a separate
                                    pipeline to read the existing data before you start the pipeline
                                    with Oracle CDC Client</ph></ph></ph></p>
            <p>
                  <draft-comment author="Loretta">UDP to Kafka origin and SDC RPC to Kafka origins
                        use the following text in the Pipeline Configuration topics.
                              <uicontrol>O-ph-PipeConfig</uicontrol> and </draft-comment>
            </p>
            <p><ph id="O-ph-PipeConfig">The origin does not pass records to its output port, so you
                        cannot perform additional processing or write the data to other destination
                        systems.</ph>
            </p>
            <p id="O-p-PipeConfig">However, since a pipeline requires a destination, you should
                  connect the origin to the Trash destination to satisfy pipeline validation
                  requirements.</p>
            <p>
                  <draft-comment author="Loretta">PROCESSOR INFO â€“-</draft-comment>
            </p>
            <p>
                  <draft-comment author="Loretta">P-HM-CompatChanges - The following is used in a
                        couple places by the Hive Metadata processor: </draft-comment>
            </p>
            <p id="P-HM-CompatChanges ">Compatible changes include new tables and partitions, and
                  the addition or removal of fields in the record. Changes in data type are not
                  compatible.</p>
            <p>
                  <draft-comment author="Loretta">The note <uicontrol>P-HM-MaxHivePS</uicontrol> is
                        used in HM processor > Configuring.</draft-comment>
            </p>
            <p>
                  <note id="P-HM-MaxHivePS">At this time, the maximum precision and scale for
                        decimal data in Hive is 38.</note>
            </p>
            <p>
                  <draft-comment author="Loretta">D-HM-CreatesAndNot_Note , D-HM-CreatesAndNot_PH:
                        the following is used in a couple places.</draft-comment>
            </p>
            <note id="D-HM-CreatesAndNot_Note"><ph id="D-HM-CreatesAndNot_PH">The destination can
                        create tables and partitions. It can add columns to tables and ignore
                        existing columns. It does not drop existing columns from tables.</ph></note>
            <p>
                  <draft-comment author="Loretta"><uicontrol>P-HM-phPartitionDF</uicontrol> The
                        following sentence is used in Hive Metadata processor - Configuring, and
                        Database, Table, and Partition Expressions.</draft-comment>
            </p>
            <p>
                  <ph id="P-HM-phPartitionDF">You can use the Int, Bigint, and String data formats
                        for partition data.</ph></p>
            <p>
                  <draft-comment author="Loretta"><uicontrol>P-ListPivot-AddPivoters</uicontrol> -
                        The following phrase is used in the List Pivoter overview &amp; generated
                        records. </draft-comment>
            </p>
            <p><ph id="P-ListPivot-AddPivoters">To pivot additional list fields or nested lists, use
                        additional List Pivoters.</ph>
            </p>
            <p>
                  <draft-comment author="Loretta">Scripting processors - - Groovy and Jython use
                        both, JavaScript uses Record by Record only
                              (<uicontrol>P-ProcessM-RbyR</uicontrol>).</draft-comment>
            </p>
            <p>
                  <dl>
                        <dlentry id="P-ProcessMode-RbyR">
                              <dt>Record by Record</dt>
                              <dd>The processor calls the script for each record. The processor
                                    passes the record to the script as a map and processes each
                                    record individually. </dd>
                              <dd>The script does not require error handling logic. Error records
                                    are passed to the processor for error handling. The processor
                                    handles error records based on the On Record Error
                                    property.</dd>
                              <dd>Use this mode to avoid including error handling logic in the code.
                                    Since this mode calls the script for each record, pipeline
                                    performance will be negatively affected. </dd>
                        </dlentry>
                        <dlentry id="P-ProcessM-BbyB">
                              <dt>Batch by Batch</dt>
                              <dd>The processor calls the script for each batch. The processor
                                    passes the batch to the script as a list and processes the batch
                                    at one time. </dd>
                              <dd>Include error handling logic in the script. Without error handling
                                    logic, a single error record sends the entire batch to the
                                    processor for error handling. The processor handles error
                                    records based on the On Record Error property.</dd>
                              <dd>Use this mode to improve performance by processing a batch of data
                                    at a time.</dd>
                        </dlentry>
                  </dl>
            </p>
            <p>
                  <draft-comment author="Loretta"><uicontrol>P-DL-ScriptObjects</uicontrol> -
                        Groovy, JavaScript, Jython Processor scripting objects. All three use these,
                        and they each have their own Record object versions:</draft-comment>
            </p>
            <p>
                  <dl id="P-DL-ScriptObjects">
                        <dlentry>
                              <dt>state</dt>
                              <dd>An object to store information between invocations of this script.
                                    A state is a map object that includes a collection of key/value
                                    pairs. You can use the state object to cache data such as
                                    lookups or counters.</dd>
                              <dd>The state object functions much like an instance variable: <ul
                                          id="ul_bh4_1yr_kv">
                                          <li>You need to populate the object - it has no default
                                                value.</li>
                                          <li>The information is transient and is lost when the
                                                pipeline stops or restarts.</li>
                                          <li>The state object is available only for the instance of
                                                the processor stage it is defined in. If the
                                                pipeline executes in cluster mode, the state object
                                                is not shared across nodes.</li>
                                    </ul></dd>
                              <dd>
                                    <note type="warning">The state object is best used for a fixed
                                          or static set of data. Adding to the cache on every record
                                          or batch can quickly consume the memory allocated to <ph
                                                conref="#concept_vhs_5tz_xp/pName-long"/> and cause
                                          out of memory exceptions. </note>
                              </dd>
                        </dlentry>
                        <dlentry>
                              <dt>log</dt>
                              <dd>An object to write messages to the log. Includes four methods:
                                          <codeph>info()</codeph>, <codeph>warn()</codeph>,
                                          <codeph>debug()</codeph>, and <codeph>trace()</codeph>. </dd>
                              <dd>The signature of the four methods is as follows:
                                    <codeblock>(message-template, arguments...) </codeblock>The
                                    message template can have positional variables denoted by curly
                                    brackets: { }. The arguments are replaced in the message
                                    template curly brackets in positional manner, i.e., this is the
                                    first argument in the first { } occurrence, and so on.</dd>
                        </dlentry>
                        <dlentry>
                              <dt>output</dt>
                              <dd>An object that writes the record to the output batch. Includes a
                                          <codeph>write(Record)</codeph> method.</dd>
                        </dlentry>
                        <dlentry>
                              <dt>error</dt>
                              <dd>An object that passes error records to the processor for error
                                    handling. Includes a <codeph>write(Record, message)</codeph>
                                    method.</dd>
                        </dlentry>
                        <dlentry>
                              <dt>sdcFunctions</dt>
                              <dd>An object that runs functions that evaluate or modify data.
                                    Includes a <codeph>getFieldNull(Record, 'field path')</codeph>
                                    method that checks if a field is assigned a constant such as
                                    NULL_INTEGER or NULL_STRING. </dd>
                        </dlentry>
                  </dl>
            </p>
            <p>
                  <draft-comment author="Loretta">DESTINATION DATA FORMATS. Individual items are
                        conrefed, not the whole list. (8/10/16) </draft-comment>
            </p>
            <p>
                  <dl>
                        <dlentry>
                              <dt/>
                              <dd>
                                    <draft-comment author="Loretta"><uicontrol>AvroFlume</uicontrol>
                                          is used only by Flume. <b>AvroFile</b> used by Hadoop FS,
                                          Local and S3, and other file destinations. <b>AvroMess</b>
                                          used by Kafka and Kinesis Producer and other message
                                          destinations. When making changes to these, add updates to
                                          the Flume data formats topic when necessary.
                                    </draft-comment>
                              </dd>
                        </dlentry>
                        <dlentry id="Avro-Flume">
                              <dt>Avro</dt>
                              <dd>The destination writes records based on the Avro schema in the
                                    avroSchema record header attribute or the Avro schema that you
                                    provide. </dd>
                              <dd>You can optionally include the schema definition as part of the
                                    Flume event. Omitting the schema definition can improve
                                    performance, but requires the appropriate schema management to
                                    avoid losing track of the schema associated with the data.</dd>
                              <dd>You can also compress data with an Avro-supported compression
                                    codec. When using Avro compression, do not use other compression
                                    available in the destination. </dd>
                        </dlentry>
                        <dlentry id="DEST-DF-AvroFILE">
                              <dt>Avro</dt>
                              <dd>The destination writes records based on the Avro schema in the
                                    avroSchema record header attribute or the Avro schema that you
                                    provide. The schema definition is included in each file.</dd>
                              <dd>You can compress data with an Avro-supported compression codec.
                                    When using Avro compression, avoid using other compression
                                    available in the destination. </dd>
                        </dlentry>
                        <dlentry id="D-DF-AvroMess">
                              <dt>Avro</dt>
                              <dd>The destination writes records based on the Avro schema in the
                                    avroSchema record header attribute or the Avro schema that you
                                    provide. </dd>
                              <dd>You can optionally include the schema definition as part of the
                                    message. Omitting the schema definition can improve performance,
                                    but requires the appropriate schema management to avoid losing
                                    track of the schema associated with the data.</dd>
                              <dd>You can compress data with an Avro-supported compression codec.
                                    When using Avro compression, avoid using other compression
                                    available in the destination. </dd>
                        </dlentry>
                  </dl>
                  <draft-comment author="Loretta"><b>DESTDataFormat-Binary</b> - the following is
                        just for Kafka Producer and Amazon S3.</draft-comment>
                  <dl>
                        <dlentry id="DESTDataFormat-Binary">
                              <dt>Binary</dt>
                              <dd>The destination writes binary data from a single field in the
                                    record. </dd>
                        </dlentry>
                        <dlentry id="DEST-DataF-Delim">
                              <dt>Delimited</dt>
                              <dd>The destination writes records as delimited data. When you use
                                    this data format, the root field must be list or list-map.</dd>
                        </dlentry>
                        <dlentry id="DEST-DataF-JSON">
                              <dt>JSON</dt>
                              <dd>The destination writes records as JSON data. You can use one of
                                    the following formats:<ul id="ul_dd1_5y1_wr">
                                          <li>Array - Each file includes a single array. In the
                                                array, each element is a JSON representation of each
                                                record.</li>
                                          <li>Multiple objects - Each file includes multiple JSON
                                                objects. Each object is a JSON representation of a
                                                record. </li>
                                    </ul></dd>
                        </dlentry>
                  </dl>
            </p>
            <draft-comment author="Loretta"><b>DestDF-ProtoMess</b> - Amazon S3, Directory? Hadoop
                  FS?</draft-comment>
            <dl>
                  <dlentry id="DestDF-ProtoMess">
                        <dt>Protobuf</dt>
                        <dd>Writes one record in a message. Uses the user-defined message type and
                              the definition of the message type in the descriptor file to generate
                              the message. </dd>
                        <dd>For information about generating the descriptor file, see <xref
                                    href="../Pipeline_Design/Protobuf-Prerequisites.dita"/>.</dd>
                  </dlentry>
            </dl>
            <draft-comment author="Loretta"><b>DestDF-ProtoFile</b> - Amazon S3, Directory? Hadoop
                  FS?</draft-comment>
            <dl>
                  <dlentry id="DestDF-ProtoFile">
                        <dt>Protobuf</dt>
                        <dd>Writes a batch of messages in each file. </dd>
                        <dd>Uses the user-defined message type and the definition of the message
                              type in the descriptor file to generate the messages in the file. </dd>
                        <dd>For information about generating the descriptor file, see <xref
                                    href="../Pipeline_Design/Protobuf-Prerequisites.dita"/>.</dd>
                  </dlentry>
                  <dlentry id="DEST-DataF-SDC">
                        <dt>SDC Record</dt>
                        <dd>The destination writes records in the SDC Record data format. </dd>
                  </dlentry>
            </dl>
            <draft-comment author="Loretta"><b>DESTDataF-Text</b> and others in this list are used
                  by Kafka, Flume, Hadoop FS, Local FS, Kinesis Firehose. </draft-comment>
            <dl>
                  <dlentry id="DESTDataF-Text">
                        <dt>Text</dt>
                        <dd>The destination writes a single text field of a record. When you
                              configure the stage, you select the field to use. When necessary,
                              merge record data into the field earlier in the pipeline. </dd>
                  </dlentry>
            </dl>
            <p>
                  <draft-comment author="Loretta"><uicontrol>D-DF-WholeFile</uicontrol> - must be
                        used by S3. <uicontrol>D-DF-WholeF-Basic</uicontrol> is used by Local FS.
                              <uicontrol>D-DF-WholeF-HDFS</uicontrol> is used by Hadoop FS and MapR
                        FS.</draft-comment>
            </p>
            <dl>
                  <dlentry id="D-DF-WholeFile">
                        <dt>Whole File</dt>
                        <dd>Streams whole files to the destination system. The destination writes
                              the data to the file and location defined in the stage. If a file of
                              the same name already exists, you can configure the destination to
                              overwrite the existing file or send the current file to error.</dd>
                        <dd>Target files use the default permissions defined in the destination
                              system.</dd>
                        <dd>For more information about the whole file data format, see <xref
                                    href="../Pipeline_Design/WholeFile.dita#concept_nfc_qkh_xw"
                              />.</dd>
                  </dlentry>
                  <dlentry id="D-DF-WholeF-Basic">
                        <dt>Whole File</dt>
                        <dd>Streams whole files to the destination system. The destination writes
                              the data to the file and location defined in the stage. If a file of
                              the same name already exists, you can configure the destination to
                              overwrite the existing file or send the current file to error.</dd>
                        <dd>By default, target files use the default access permissions for the
                              destination system. You can specify an expression that defines access
                              permissions. </dd>
                        <dd>For more information about the whole file data format, see <xref
                                    href="../Pipeline_Design/WholeFile.dita#concept_nfc_qkh_xw"
                              />.</dd>
                  </dlentry>
                  <dlentry id="D-DF-WholeF-HDFS">
                        <dt>Whole File</dt>
                        <dd>Streams whole files to the destination system. The destination writes
                              the data to the file and location defined in the stage. If a file of
                              the same name already exists, you can configure the destination to
                              overwrite the existing file or send the current file to error.</dd>
                        <dd>By default, target files use the default access permissions for the
                              destination system. You can specify an expression that defines access
                              permissions. </dd>
                        <dd>Using this data format requires setting the <wintitle>File
                                    Type</wintitle> property to <uicontrol>Whole
                              File</uicontrol>.</dd>
                        <dd>For more information about the whole file data format, see <xref
                                    href="../Pipeline_Design/WholeFile.dita#concept_nfc_qkh_xw"
                              />.</dd>
                  </dlentry>
            </dl>
            <p>
                  <draft-comment author="Loretta">The note/tip
                              <uicontrol>WF-TIP-DataPreview</uicontrol> is used in Whole File -
                        Additional Processors. <uicontrol>WF-ph-DataPreview</uicontrol> is used in
                        Whole File Records.</draft-comment>
            </p>
            <p>
                  <note type="tip" id="WF-TIP-DataPreview"><ph id="WF-ph-DataPreview">You can use
                              data preview to determine the information and field names that are
                              included in the fileInfo field.</ph> The information and field names
                        can differ based on the origin system.</note>
            </p>
            <p/>
            <p>
                  <draft-comment author="Loretta">The following paragraphs are used for Expressions
                        (Pipeline Config chapter) and Expression Language (appendix).
                  </draft-comment>
            </p>
            <p id="EXP-p1">Use the expression language to configure expressions and conditions in
                  processors, such as the Expression Evaluator or Stream Selector. Some destination
                  properties also require the expression language, such as the directory template
                  for Hadoop FS or Local FS. </p>
            <p id="EXP-p2">Optionally, you can use the expression language to define any stage or
                  pipeline property that represents a numeric or string value. This allows you to
                  use constants or runtime properties throughout the pipeline. You can use
                  expression completion to determine where you can use an expression and the
                  expression elements that you can use in that location. </p>
            <p id="EXP-p3">You can use the following elements in an expression:<ul
                        id="ul_w34_vxl_2s">
                        <li>Constants</li>
                        <li>Datetime variables</li>
                        <li>Field names</li>
                        <li>Functions</li>
                        <li>Literals</li>
                        <li>Operators</li>
                        <li>Runtime properties</li>
                        <li>Runtime resources</li>
                  </ul></p>
            <draft-comment author="Loretta">The following bullets are used in Troubleshooting >
                  Accessing Error Message and Tutorial > Run the Extended Pipeline</draft-comment>
            <ul id="ul_ErrorInformation">
                  <li>errorStage - The stage in the pipeline that generated the error.</li>
                  <li>errorCode - The error code associated with the error message.</li>
                  <li>errorMessage - The text of the error message. </li>
            </ul>
            <p>
                  <draft-comment author="Loretta">The following is used in the Tutorial chapter, in
                        tables in the Creating a Pipeline... topic and the Write to the Destination
                        topic.</draft-comment>
            </p>
            <p id="FilePrefix">By default, the files are prefixed with "SDC" and an expression that
                  returns the <ph conref="#concept_vhs_5tz_xp/pName-long"/> ID, but that's more than
                  we need here. </p>
            <draft-comment author="Loretta">The following is used in HDFS origin and destination
                  overviews:</draft-comment>
            <p id="HDFS_user_props">When necessary, you can enable Kerberos authentication or use an
                  HDFS user to connect to HDFS. You can also use HDFS configuration files and add
                  other HDFS configuration properties as needed. </p>
            <draft-comment author="Loretta">list of HDFS configuration files used by Hadoop FS
                  origin - used in Hadoop Properties and Configuring Hadoop FS
                  origin.</draft-comment>
            <p>
                  <ul id="ul-HDFSfiles_HDFSorigin">
                        <li>core-site.xml</li>
                        <li>hdfs-site.xml </li>
                        <li>yarn-site.xml</li>
                        <li>mapred-site.xml</li>
                  </ul>
                  <draft-comment author="Loretta" id="ul_">list of HDFS configuration files used by
                        Hadoop FS destination - used in Hadoop Properties and Configuring Hadoop FS
                        destination.</draft-comment>
                  <ul id="HDFSfiles_HDFSdest">
                        <li>core-site.xml</li>
                        <li>hdfs-site.xml </li>
                  </ul>
            </p>
            <draft-comment author="Loretta">list of HDFS configuration files used by HBase
                  destination - used in Hadoop Properties and Configuring HBase
                  destination.</draft-comment>
            <ul id="HDFSfiles_HBasedest">
                  <li>hbase-site.xml</li>
            </ul>
            <draft-comment author="Loretta">List of config files used by Hive Streaming - used in
                  Hive Properties &amp; Configuring Hive Streaming dest. Also in Hive Metadata
                  processor - Hive properties &amp; configuring.</draft-comment>
            <ul id="HiveStreamingFiles">
                  <li>core-site.xml</li>
                  <li>hdfs-site.xml</li>
                  <li>hive-site.xml</li>
            </ul>
            <p>
                  <draft-comment author="Loretta">The following DD is used by </draft-comment>
            </p>
            <p>
                  <ol id="OL-HiveConfigSteps">
                        <li>Store the files or a symlink to the files in the <ph
                                    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                              /> resources directory or elsewhere in a path local to the <ph
                                    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                              />.</li>
                        <li>If the files are stored in the resources directory, specify a relative
                              path to the files in the stage. If the files are stored outside of the
                              resources directory, specify an absolute path to the files. <note>For
                                    a Cloudera Manager installation, <ph
                                          conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                    /> automatically creates a symlink to the files named
                                          <codeph>hive-conf</codeph>. Enter
                                          <codeph>hive-conf</codeph> for the location of the files
                                    in the stage.</note></li>
                  </ol>
            </p>
            <p>
                  <draft-comment author="Loretta">DRIFT-IgnoreWMissing - Using this in the Data
                        Drift Functions topic:</draft-comment>
            </p>
            <p id="DRIFT-ignoreWMissing">Use the ignoreWhenMissing flag to determine the behavior
                  when the field is missing. When set to "true", a missing field causes no errors.
                  When set to "false", a missing field generates an alert for the record missing the
                  field, and for the next record that includes the field.</p>
            <p>
                  <draft-comment author="Loretta"><uicontrol>D-HM-Overview1</uicontrol> - used by
                        Hadoop FS and MapR FS overviews</draft-comment>
            </p>
            <p><ph id="D-HM-Overview1">you can define a directory template and time basis to
                        determine the output directories that the destination creates and the files
                        where records are written. </ph></p>
            <p>
                  <draft-comment author="Loretta"><uicontrol>D-HM-HiveSolution</uicontrol>,
                              <uicontrol>D-HM-Overview2</uicontrol>
                        <uicontrol>D-HM-compression</uicontrol> - used by Hadoop FS and MapR FS
                        oreviews</draft-comment>
            </p>
            <p id="D-HM-HiveSolutionOverview1">As part of the Hive Drift Solution, you can
                  alternatively use record header attributes to perform record-based writes. You can
                  write records to the specified directory, use the defined Avro schema, and roll
                  files based on record header attributes. For more information, see <xref
                        href="../Destinations/RecordHeaderAttributes.dita#concept_lmn_gdc_1w"/>.</p>
            <p id="D-HM-Overview2">You can define a file prefix, the data time zone, and properties
                  that define when the destination closes a file. You can specify the amount of time
                  that a record can be written to its associated directory and what happens to late
                  records.</p>
            <p id="D-HM-compression">You can use Gzip, Bzip2, Snappy, LZ4, and other compression
                  formats to write output files. </p>
            <p>
                  <draft-comment author="Loretta"><uicontrol>D-HLM-Dirtemp-targetDir</uicontrol>
                        used in HadoopFS, LocalFS, MapR FS</draft-comment>
            </p>
            <p id="D-HLM-Dirtemp-targetDir">You can alternatively write records to directories based
                  on the targetDirectory record header attribute. Using the targetDirectory
                  attribute disables the ability to define directory templates.</p>
            <p>
                  <draft-comment author="Loretta"><uicontrol>D-HLM-TimeB-targetDir</uicontrol> used
                        in HadoopFS, LocalFS, MapR FS</draft-comment>
            </p>
            <p><ph id="D-HLM-TimeB-targetDir">When using the targetDirectory record header attribute
                        to write records, the time basis determines only whether a record is
                        late.</ph></p>
            <p>
                  <draft-comment author="Loretta"><uicontrol>D-GenAttributes</uicontrol> used in
                        Record Header Attributes for RB Writes and Generating RH
                        Attributes</draft-comment>
            </p>
            <p><ph id="D-GenAttributes">The Hive Metadata processor automatically generates record
                        header attributes for Hadoop FS and MapR FS to use as part of the Hive Drift
                        Solution. For all other destinations, you can use the Expression Evaluator
                        to add record header attributes.</ph>
            </p>
            <draft-comment author="alisontaylor">Directory Templates - used for Hadoop FS, Local FS,
                  and MapR FS destinations</draft-comment>
            <p id="DirectoryTemplate_Intro">When you define a directory template, you can use a mix
                  of constants, field values, and datetime variables. You can use the
                        <codeph>every</codeph> function to create new directories at regular
                  intervals based on seconds or minutes. You can also use the
                        <codeph>record:valueOrDefault</codeph> function to use field values or a
                  default in the directory template. </p>
            <p id="DirectoryTemplate_Example">For example, the following directory template creates
                  output directories for event data based on the state and timestamp of a record
                  with hours as the smallest unit of measure, creating a new directory every twelve
                  hours:<codeblock> /outputfiles/${record:valueOrDefault("/State", "unknown")}/${YY()}-${MM()}-${DD()}-${every(12,hh())}</codeblock></p>
            <p id="DirectoryTemplate_DL">You can use the following elements in a directory template:<dl>
                        <dlentry>
                              <dt>Constants</dt>
                              <dd>You can use any constant, such as "output" or "lateRecords."</dd>
                        </dlentry>
                        <dlentry>
                              <dt>Datetime Variables</dt>
                              <dd>You can use datetime variables, such as <codeph>${YYYY()}</codeph>
                                    or <codeph>${DD()}</codeph>. The destination creates directories
                                    as needed, based on the smallest datetime variable that you use.
                                    For example, if the smallest variable is hours, then the
                                    directories are created for every hour of the day that receives
                                    output records.</dd>
                              <dd>When you use datetime variables in an expression, use all of the
                                    datetime variables between one of the year variables and the
                                    smallest variable that you want to use. For example, to create
                                    directories on a daily basis for a Hadoop FS destination, use a
                                    year variable, a month variable, and then a day variable. You
                                    might use one of the following datetime variable progressions: </dd>
                              <dd>
                                    <codeblock>${YYYY()}-${MM()}-${DD()}
${YY()}_${MM()}_${DD()}</codeblock>
                              </dd>
                              <dd>For details about datetime variables, see <xref
                                          href="../Expression_Language/DateTimeVariables.dita#concept_gh4_qd2_sv"
                                    />.</dd>
                        </dlentry>
                        <dlentry>
                              <dt>every() function</dt>
                              <dd>You can use the <codeph>every()</codeph> function in a directory
                                    template to create directories at regular intervals based on
                                    minutes or seconds. The intervals should be a submultiple or
                                    integer factor of 60. For example, you can create directories
                                    every 15 minutes or 30 seconds. </dd>
                              <dd>Use the <codeph>every()</codeph> function to replace the smallest
                                    datetime variable used in the template.</dd>
                              <dd>For example, the following directory template creates directories
                                    every 5
                                    minutes:<codeblock>/HDFS_output/${YYYY()}-${MM()}-${DD()}-${hh()}-${every(5,mm())}</codeblock></dd>
                              <dd>For details about the <codeph>every()</codeph> function, see <xref
                                          href="../Expression_Language/MiscFunctions.dita#concept_ddw_ld1_1s"
                                    />.</dd>
                        </dlentry>
                        <dlentry>
                              <dt>record:valueOrDefault function</dt>
                              <dd>You can use the following expression to use the value of a field
                                    and the specified default value if the field does not exist or
                                    if the field is null:
                                    <codeblock>${record:valueOrDefault(&lt;field path>, &lt;default value>)}</codeblock></dd>
                              <dd>For example, the following directory template creates a directory
                                    based on the product field every day, and if the product field
                                    is empty or null, uses Misc in the directory path:
                                    <codeblock>/${record:valueOrDefault("/Product", "Misc")}/${YY()}-${MM()}-${DD()}</codeblock></dd>
                              <dd>This template might create the following
                                    paths:<codeblock>/Shirts/2015-07-31 
/Misc/2015-07-31</codeblock></dd>
                        </dlentry>
                  </dl></p>
            <draft-comment author="alisontaylor">Time Basis - used for Hadoop FS, Local FS, and MapR
                  FS destinations</draft-comment>
            <dl id="TimeBasis">
                  <dlentry>
                        <dt>Processing Time</dt>
                        <dd>When you use processing time as the time basis, the destination creates
                              directories based on the processing time and the directory template,
                              and writes records to the directories based on when they are
                              processed.</dd>
                        <dd>For example, say a directory template creates directories every minute
                              and the time basis is the time of processing. Then, directories are
                              created for every minute that the destination writes output records.
                              And the output records are written to the directory for that minute of
                              processing. </dd>
                        <dd>To use the processing time as the time basis, use the following
                              expression: <codeph>${time:now()}</codeph>. This is the default time
                              basis. </dd>
                  </dlentry>
                  <dlentry>
                        <dt>Record Time</dt>
                        <dd>When you use the time associated with a record as the time basis, you
                              specify a Date field in the record. The destination creates
                              directories based on the datetimes associated with the records and
                              writes the records to the appropriate directories. </dd>
                        <dd>For example, say a directory template creates directories every hour and
                              the time basis is based on the record. Then, directories are created
                              for every hour associated with output records and the destination
                              writes the records to the related output directory. </dd>
                        <dd>To use a time associated with the record, use an expression that calls a
                              field and resolves to a datetime value, such as
                                    <codeph>${record:value("/Timestamp")}</codeph>. </dd>
                  </dlentry>
            </dl>
            <draft-comment author="alisontaylor">Late Records and Late Record Handling - used for
                  Hadoop FS, Local FS, and MapR FS destinations</draft-comment>
            <p id="LateRecordsOverview">When you use a record time as the time basis, you can define
                  a time limit for records to be written to their associated output file. When the
                  destination creates a new output file in a new directory, the previous output file
                  is kept open for the specified late record time limit. When records that belong in
                  that file arrive within the time limit, the destination writes the records to the
                  open output file. When the late record time limit is reached, the output file is
                  closed and any record that arrives past this limit is considered late.</p>
            <note type="tip" id="LateRecordsTip">The late records properties are not applicable if
                  you use processing time as the time basis. If you use processing time, set the
                  late record time limit to one second.</note>
            <p id="LateRecordsError">You can send late records to a late records file or to the
                  stage for error handling. When you send records to a late records file, you define
                  a late records directory template. </p>
            <p id="LateRecordsExample1">For example, you use a record time as the time basis,
                  configure a one hour late record time limit, configure late records to be sent to
                  the stage for error handling, and use the default directory template value:
                  <codeblock>/tmp/out/${YYYY()}-${MM()}-${DD()}-${hh()} </codeblock></p>
            <p id="LateRecordsExample2">The first records that arrive have a datetime between the
                  hours of 02:00 and 02:59, and so are written to an output file in the 02
                  directory. When records with a datetime between the hours of 03:00 and 03:59
                  arrive, the destination creates a new file in an 03 directory. The destination
                  keeps the file in the 02 directory open for another hour. </p>
            <p id="LateRecordsExample3">If a record with a datetime between the hours of 02:00 and
                  02:59 arrives before the hour time limit, the destination writes the record to the
                  open file in the 02 directory. After one hour, the destination closes the output
                  file in the 02 directory. Any records with a datetime between the hours of 02:00
                  and 02:59 that arrive after the one hour time limit are considered late. The late
                  records are sent to the stage for error handling.</p>
            <draft-comment author="alisontaylor">Timeout to Close Idle Files - used for Hadoop FS,
                  Local FS, and MapR FS destinations</draft-comment>
            <p id="IdleTimeoutOverview">You might want to configure an idle timeout when output
                  files remain open and idle for too long, thus delaying another system from
                  processing the files.</p>
            <p id="IdleTimeoutReasons">Output files might remain idle for too long for the following
                  reasons: <ul id="ul_pcq_5l3_mw">
                        <li>You configured the maximum number of records to be written to output
                              files or the maximum size of output files, but records have stopped
                              arriving. An output file that has not reached the maximum number of
                              records or the maximum file size stays open until more records
                              arrive.</li>
                        <li>You configured a date field in the record as the time basis and have
                              configured a late record time limit, but records arrive in
                              chronological order. When a new directory is created, the output file
                              in the previous directory remains open for the configured late record
                              time limit. However, no records are ever written to the open file in
                              the previous directory.<p>For example, when a record with a datetime
                                    of 03:00 arrives, the destination creates a new file in a new 03
                                    directory. The previous file in the 02 directory is kept open
                                    for the late record time limit, which is an hour by default.
                                    However, when records arrive in chronological order, no records
                                    that belong in the 02 directory arrive after the 03 directory is
                                    created. </p></li>
                  </ul></p>
            <p id="IdleTimeoutSummary">In either situation, configure an idle timeout so that other
                  systems can process the files sooner, instead of waiting for the configured
                  maximum records, maximum file size, or late records conditions to occur. </p>
            <p>
                  <draft-comment author="Loretta">Record Header Attributes - Hadoop FS, Local FS,
                        MapR FS. But Local and MapR FS don't have full functionality yet
                        7/6</draft-comment>
            </p>
            <p><ph id="D-HLM-Attributes1">To use record header attributes for record-based writes,
                        you configure the destination to use the header attribute, and you ensure
                        that the record headers include the header attribute.</ph>
            </p>
            <p id="D-HLM-Attributes2">Use the Expression Evaluator processor to add stage attributes
                  to record headers. </p>
            <p>
                  <draft-comment author="Loretta">** there were 2 versions, but now there's just the
                        one in Record Header Attributes. Delete or keep as conref for some reason?
                        LC, 7/6 ** Two versions of this, first for Hadoop FS, 2nd for Local &amp;
                        MapR FS. This is so we don't forget to update Hadoop FS when updating the
                        others? </draft-comment>
            </p>
            <p>
                  <dl id="D-HadoopFS-AttrDL">
                        <dlentry>
                              <dt>targetDirectory</dt>
                              <dd>The targetDirectory header attribute defines the directory where
                                    the record is written. If the directory does not exist, the
                                    destination creates the directory. The targetDirectory header
                                    attribute replaces the Directory Template property in the
                                    destination.</dd>
                              <dd>When you use targetDirectory to provide the directory, the time
                                    basis configured for the destination is used only for
                                    determining whether a record is late. Time basis is not used to
                                    determine the output directories to create or to write records
                                    to directories.</dd>
                              <dd>To use the targetDirectory header attribute, on the
                                          <wintitle>Output</wintitle> tab, select
                                          <uicontrol>Directory in Header</uicontrol>.</dd>
                        </dlentry>
                        <dlentry>
                              <dt>avroSchema</dt>
                              <dd>The avroSchema header attribute defines the Avro schema for the
                                    record. When you use this header attribute, you cannot define an
                                    Avro schema to use in the destination. </dd>
                              <dd>To use the avroSchema header attribute, on the
                                          <wintitle>Avro</wintitle> tab, select <uicontrol>Load
                                          Schema from Header</uicontrol>.</dd>
                        </dlentry>
                        <dlentry>
                              <dt>roll</dt>
                              <dd>The roll attribute, when present in the record header, triggers a
                                    roll of the file. </dd>
                              <dd>You can define the name of the roll header attribute. When you use
                                    the Hive Metadata processor to generate the roll header
                                    attribute, use the default "roll" attribute name.</dd>
                              <dd>To use a roll header attribute, on the <wintitle>Output</wintitle>
                                    tab, select <uicontrol>Use Roll Attribute</uicontrol> and define
                                    the name of the attribute. </dd>
                        </dlentry>
                  </dl>
            </p>
            <p>
                  <draft-comment author="Loretta"><uicontrol>D-LocalMapRFS-AttrDL</uicontrol> Local
                        FS and MapR FS versions â€“ NOT USED? DELETE?</draft-comment>
            </p>
            <p>
                  <dl>
                        <dlentry>
                              <dt>targetDirectory</dt>
                              <dd>The targetDirectory header attribute defines the directory where
                                    the record is written. If the directory does not exist, the
                                    destination creates the directory. The targetDirectory header
                                    attribute replaces the Directory Template property in the
                                    destination.</dd>
                              <dd>When you use targetDirectory to provide the directory, the time
                                    basis configured for the destination is used only for
                                    determining whether a record is late. Time basis is not used to
                                    determine the output directories to create or to write records
                                    to directories.</dd>
                              <dd>To use the targetDirectory header attribute, on the
                                          <wintitle>Output</wintitle> tab, select
                                          <uicontrol>Directory in Header</uicontrol>.</dd>
                        </dlentry>
                        <dlentry>
                              <dt>avroSchema</dt>
                              <dd>The avroSchema header attribute defines the Avro schema for the
                                    record. When you use this header attribute, you cannot define an
                                    Avro schema to use in the destination. </dd>
                              <dd>To use the avroSchema header attribute, on the
                                          <wintitle>Avro</wintitle> tab, select <uicontrol>Load
                                          Schema from Header</uicontrol>.</dd>
                        </dlentry>
                        <dlentry>
                              <dt>roll</dt>
                              <dd>The roll attribute, when present in the record header, triggers a
                                    roll of the file. </dd>
                              <dd>You can define the name of the roll header attribute. When you use
                                    the Hive Metadata processor to generate the roll header
                                    attribute, use the default "roll" attribute name.</dd>
                              <dd>To use a roll header attribute, on the <wintitle>Output</wintitle>
                                    tab, select <uicontrol>Use Roll Attribute</uicontrol> and define
                                    the name of the attribute. </dd>
                        </dlentry>
                  </dl>
            </p>
            <p>
                  <draft-comment author="Loretta">ul-KERBprops - Kerberos properties in Data
                        Collector config file:</draft-comment>
            </p>
            <p>
                  <ul id="ul-KERBprops">
                        <li>kerberos.client.enabled</li>
                        <li>kerberos.client.principal</li>
                        <li>kerberos.client.keytab</li>
                  </ul>
            </p>
            <draft-comment author="Loretta">UseDefaults: Using this phrase throughout the tutorial
                  as a disclaimer:</draft-comment>
            <p id="UseDefaults">Use the defaults for properties that aren't listed:</p>
            <p>
                  <draft-comment author="Loretta">MapR-Prereq - Using in all MapR stages to make
                        sure they configure SDC.</draft-comment>
            </p>
            <p id="MapRPrereq">Before you use any MapR stage in a pipeline, you must perform
                  additional steps to enable <ph conref="#concept_vhs_5tz_xp/pName-long"/> to
                  process MapR data. For more information, see <xref
                        href="../Install_Config/MapR-Prerequisites.dita#concept_jgs_qpg_2v"/>.</p>
            <draft-comment author="alisontaylor">MapR-PrereqHiveStream - Use in Hive Streaming
                  destination for MapR library to make sure they configure SDC.</draft-comment>
            <p id="MapRPrereqHiveStream">Before you use the Hive Streaming destination with the MapR
                  library in a pipeline, you must perform additional steps to enable <ph
                        conref="#concept_vhs_5tz_xp/pName-long"/> to process MapR data. For more
                  information, see <xref
                        href="../Install_Config/MapR-Prerequisites.dita#concept_jgs_qpg_2v"/>. </p>
            <draft-comment author="alisontaylor">Lookup Cache - used for Redis Lookup and HBase
                  Lookup processors</draft-comment>
            <p id="LookupCacheOverview">The processor caches key-value pairs until the cache reaches
                  the maximum size or the expiration time. When the first limit is reached, the
                  processor evicts key-value pairs from the cache.</p>
            <p id="LookupCacheOptions">You can configure the following ways to evict key-value pairs
                  from the cache:<dl>
                        <dlentry>
                              <dt>Size-based eviction</dt>
                              <dd>Configure the maximum number of key-value pairs that the processor
                                    caches. When the maximum number is reached, the processor evicts
                                    the oldest key-value pairs from the cache.</dd>
                        </dlentry>
                        <dlentry>
                              <dt>Time-based eviction</dt>
                              <dd>Configure the amount of time that a key-value pair can remain in
                                    the cache without being written to or accessed. When the
                                    expiration time is reached, the processor evicts the key from
                                    the cache. The eviction policy determines whether the processor
                                    measures the expiration time since the last write of the value
                                    or since the last access of the value.</dd>
                              <dd>For example, you set the eviction policy to expire after the last
                                    access and set the expiration time to 60 seconds. After the
                                    processor does not access a key-value pair for 60 seconds, the
                                    processor evicts the key-value pair from the cache.</dd>
                        </dlentry>
                  </dl></p>
            <p id="LookupCacheSummary">When you stop the pipeline, the processor clears the
                  cache.</p>
            <draft-comment author="alisontaylor">Used in Field Type Converter chapter and in Data
                  Preview chapter.</draft-comment>
            <note id="PreviewDateFormat"><ph id="ph-PreviewDateFormat">Data preview displays date,
                        datetime, and time data using the default format of the browser locale. For
                        example, if the browser uses the en_US locale, preview displays dates using
                        the following format: MMM d, y h:mm:ss a.</ph></note>
            <draft-comment author="alisontaylor">Used in the Function types sections in the
                  Expression Language chapter.</draft-comment>
            <p id="FunctionArgument">You can replace any argument with a literal or an expression
                  that evaluates to the argument. String literals must be enclosed in single or
                  double quotation marks.</p>
            <p>
                  <draft-comment author="alisontaylor">AWS Credentials for destinations: Amazon S3,
                        Kinesis Firehose, and Kinesis Producer. </draft-comment>
            </p>
            <dl id="AWSCredentials-Destinations">
                  <dlentry>
                        <dt>IAM roles</dt>
                        <dd>When <ph
                                    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                              /> runs on an Amazon EC2 instance, you can use the AWS Management
                              Console to configure an IAM role for the EC2 instance. <ph
                                    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                              /> uses the IAM instance profile credentials to automatically connect
                              to AWS. </dd>
                        <dd>When you use IAM roles, you do not need to specify the Access Key ID and
                              Secret Access Key properties in the destination. </dd>
                        <dd>For more information about assigning an IAM role to an EC2 instance, see
                              the Amazon EC2 documentation.</dd>
                  </dlentry>
                  <dlentry>
                        <dt>AWS access key pairs</dt>
                        <dd>
                              <p>When <ph
                                          conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                    /> does not run on an Amazon EC2 instance or when the EC2
                                    instance doesnâ€™t have an IAM role, you must specify the
                                          <uicontrol>Access Key ID</uicontrol> and <uicontrol>Secret
                                          Access Key</uicontrol> properties in the destination.</p>
                        </dd>
                  </dlentry>
            </dl>
            <draft-comment author="alisontaylor">AWS Credentials for origins: Amazon S3 and Kinesis
                  Consumer</draft-comment>
            <dl id="AWSCredentials-Origins">
                  <dlentry>
                        <dt>IAM roles</dt>
                        <dd>When <ph
                                    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                              /> runs on an Amazon EC2 instance, you can use the AWS Management
                              Console to configure an IAM role for the EC2 instance. <ph
                                    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                              /> uses the IAM instance profile credentials to automatically connect
                              to AWS. </dd>
                        <dd>When you use IAM roles, you do not need to specify the Access Key ID and
                              Secret Access Key properties in the origin.</dd>
                        <dd>For more information about assigning an IAM role to an EC2 instance, see
                              the Amazon EC2 documentation.</dd>
                  </dlentry>
                  <dlentry>
                        <dt>AWS access key pairs</dt>
                        <dd>When <ph
                                    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                              /> does not run on an Amazon EC2 instance or when the EC2 instance
                              doesnâ€™t have an IAM role, you must specify the <uicontrol>Access Key
                                    ID</uicontrol> and <uicontrol>Secret Access Key</uicontrol>
                              properties in the origin.</dd>
                  </dlentry>
            </dl>
            <draft-comment author="alisontaylor">Additional Drivers: used by main install and
                  Cloudera Manager install</draft-comment>
            <p id="AdditionalDrivers_list">Before you use the following stages, you need to install drivers for the implementation that
                  you want to use: <ul id="ul_k3t_mrw_jx">
                        <li>JDBC Consumer origin</li>
                        <li>JMS Consumer origin</li>
                        <li>Oracle CDC Client origin</li>
                        <li>JDBC Lookup processor</li>
                        <li>JDBC Tee processor</li>
                        <li>JDBC Producer destination</li>
                  </ul></p>
            <p id="AdditionalDrivers_example">For example, to use the JDBC Consumer or JDBC Producer with Oracle, install the Oracle JDBC
                  drivers.</p>
            <p id="AdditionalDrivers_optional">Additional drivers for other stages are optional. For example, you can install custom classes
                  for the JavaScript or Jython Evaluators. </p>
      </conbody>
</concept>
