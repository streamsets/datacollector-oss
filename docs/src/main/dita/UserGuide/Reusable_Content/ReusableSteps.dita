<?xml version="1.0" encoding="UTF-8"?>
<!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

-->
<!DOCTYPE task PUBLIC "-//OASIS//DTD DITA General Task//EN" "generalTask.dtd">
<task id="task_kzs_5vz_sq">
    <title>Reusable Steps</title>
    <shortdesc>You can conref these steps. Don't change anything in this file without checking where
        it is used.</shortdesc>
    <taskbody>
        <context/>
        <steps>
            <step>
                <cmd>
                    <draft-comment author="Loretta">Install RPM steps - used in Full Install > RPM,
                        Core install > RPM, and first step used in Upgrade using RPM > Step
                        3.</draft-comment>
                </cmd>
            </step>
            <step id="RPMinstall-download">
                <cmd>Use the following URL to download the <ph
                        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                    /> RPM package for your operating system from the StreamSets website: <xref
                        href="https://streamsets.com/opensource" format="html" scope="external"
                    />:</cmd>
                <choices id="choices_ywl_kjk_1cb">
                    <choice>For <ph
                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/ph_LinuxEL6_RPM"
                        />, download the RPM EL6 package.</choice>
                    <choice>For <ph
                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/ph_LinuxEL7_RPM"
                        />, download the RPM EL7 package.</choice>
                </choices>
            </step>
            <step id="RPMinstall-extract">
                <cmd>Use the following command to extract the file to the desired location:</cmd>
                <info id="infoRPMinstall-extract">
                    <codeblock>tar xf streamsets-datacollector-&lt;version>-&lt;operating_system>-all-rpms.tar</codeblock>
                    <p>For example, to extract version <ph
                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/version"
                        /> on CentOS 7, use the following
                        command:<codeblock>tar xf streamsets-datacollector-<ph conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/version"/>-el7-all-rpms.tar</codeblock></p>
                </info>
            </step>
            <step>
                <cmd id="PIPE_PROPS">
                    <draft-comment author="Loretta"><uicontrol>PIPELINE
                        PROPERTIES</uicontrol></draft-comment>
                </cmd>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta">The following two step are used in Configuring a
                        Pipeline and the tutorial pipeline config topic.</draft-comment>
                </cmd>
            </step>
            <step>
                <cmd>From the Pipeline Repository view, click the <uicontrol>Add</uicontrol>
                    icon.</cmd>
            </step>
            <step id="CreatePipeline1">
                <cmd><ph product="SDC">From the <wintitle>Home</wintitle> page or <wintitle>Getting
                            Started</wintitle> page, click <uicontrol>Create New
                            Pipeline</uicontrol>.</ph><ph product="DPM">From the Pipeline Repository
                        view, click the <uicontrol>Add</uicontrol> icon.</ph></cmd>
                <info product="SDC">
                    <note type="tip">To get to the <wintitle>Home</wintitle> page, click the Home
                        icon.</note>
                </info>
            </step>
            <step id="CreatePipeline2">
                <cmd>In the <wintitle>New Pipeline</wintitle> window, enter a pipeline title and
                    optional description, and select where you want the pipeline to run:</cmd>
                <choices id="choices_gxy_lg2_rbb">
                    <choice><ph conref="ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> -
                        Select to design a standalone or cluster execution mode pipeline that runs
                        on <ph conref="ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                        />.</choice>
                    <choice><ph conref="ReusablePhrases.dita#concept_vhs_5tz_xp/Edge-Long"/> -
                        Select to design an edge execution mode pipeline that runs on <ph
                            conref="ReusablePhrases.dita#concept_vhs_5tz_xp/Edge-Long"/>.</choice>
                </choices>
            </step>
            <step product="DPM" id="CreatePipeline-Templates">
                <cmd>Then, specify how you want to develop the pipeline:</cmd>
                <choices id="choices_ppt_2dh_bcb">
                    <choice>Blank pipeline - Select to create a pipeline from scratch.</choice>
                    <choice>Pipeline template - Select to use a template as a basis for the
                        pipeline.</choice>
                </choices>
            </step>
            <step id="CreatePipeline3">
                <cmd>Click <uicontrol>Save</uicontrol>.</cmd>
                <stepresult>The pipeline canvas displays the pipeline title, the generated pipeline
                    ID, and an error icon. The error icon indicates that you need to configure error
                    handling for the pipeline. The Properties panel displays the pipeline
                    properties. </stepresult>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>DATA
                        PREVIEW</uicontrol></draft-comment>
                    <draft-comment author="Loretta">The steps below are for data preview. They are
                        used in "Previewing a Single Stage" and "Previewing Multiple
                        Stages."</draft-comment>
                </cmd>
            </step>
            <step id="StartPreview">
                <cmd>Above the pipeline canvas, click the <uicontrol>Preview</uicontrol> icon:
                        <image href="../Graphics/icon_Preview.png" id="image_tfg_tf4_zs" scale="70"
                    />.</cmd>
                <info>If the Preview icon is disabled, check the Issues list for unconnected stages
                    and required properties that are not defined.</info>
            </step>
            <step id="Preview-Source">
                <cmd>In the <wintitle>Preview Configuration</wintitle> dialog box, configure the
                    following properties, then click <uicontrol>Run Preview</uicontrol>.</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_mjq_mfm_cs">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Preview Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Preview Source</entry>
                                    <entry>Source data for the preview:<ul id="ul_db2_4fm_cs">
                                            <li>Configured Source - Provides data from the origin
                                                system.</li>
                                            <li>Snapshot Data - Uses available snapshot data.</li>
                                        </ul></entry>
                                </row>
                                <row>
                                    <entry>Preview Batch Size</entry>
                                    <entry>Number of records to use in the preview. Honors values up
                                        to the <ph
                                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                        /> preview batch size. <p>Default is 10. The <ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            /> default is 10.</p></entry>
                                </row>
                                <row>
                                    <entry>Preview Timeout</entry>
                                    <entry>Milliseconds to wait for preview data. Use to limit the
                                        time data preview waits for data to arrive at the origin.
                                        Relevant for transient origins only. </entry>
                                </row>
                                <row>
                                    <entry>Write to Destinations and Executors</entry>
                                    <entry>Determines whether the preview passes data to
                                        destinations or executors.<p>By default, does not pass data
                                            to destination or executor stages.</p></entry>
                                </row>
                                <row>
                                    <entry>Execute Pipeline Lifecycle Events</entry>
                                    <entry>Triggers the generation of any appropriate pipeline
                                        events, typically the Start event. If the event is
                                        configured to be used, event consumption is also
                                        triggered.</entry>
                                </row>
                                <row>
                                    <entry>Show Record/Field Header</entry>
                                    <entry>Displays record header attributes and field attributes
                                        when in List view. Attributes do not display in Table
                                        view.</entry>
                                </row>
                                <row>
                                    <entry>Show Field Type</entry>
                                    <entry>Displays the data type for fields in List view. Field
                                        types do not display in Table view.</entry>
                                </row>
                                <row>
                                    <entry>Snapshot Data</entry>
                                    <entry>When using a snapshot for source data, select the
                                        snapshot to use. </entry>
                                </row>
                                <row>
                                    <entry>Remember the Configuration</entry>
                                    <entry>Stores the current preview configuration for use every
                                        time you request a preview for this pipeline. <p>After you
                                            run data preview, you can change this option in the
                                            Preview panel by selecting the Preview Configuration
                                            icon (<image
                                                href="../Graphics/icon_PrevPreviewConfig.png"
                                                id="image_qzd_tnf_vs" scale="80"/>) and clearing the
                                            option. The change takes effect the next time you run
                                            data preview.</p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
                <stepresult>The Preview panel highlights the origin stage and displays preview data
                    in table view. Since this is the origin of the pipeline, no input data displays.
                        <p>To view preview data in list view, click the <uicontrol>List
                            View</uicontrol> icon: <image href="../Graphics/icon_PrevListView.png"
                            id="image_ids_rc4_xs" scale="80"/>.</p></stepresult>
            </step>
            <step id="DeletePreviewRecord">
                <cmd>To delete a record that you do not want to use, click the
                        <uicontrol>Delete</uicontrol> icon.</cmd>
            </step>
            <step id="RefreshPreview">
                <cmd>To refresh the preview, click the <uicontrol>Refresh Preview</uicontrol> icon:
                        <image href="../Graphics/icon_PrevRefresh.png" id="image_nys_3ff_vs"
                        scale="90"/>.</cmd>
                <info>Based on the origin, refreshing the preview either provides a new set of data
                    or reverts any changes to the existing data.</info>
            </step>
            <step id="RevertRefreshClose2">
                <cmd>To exit data preview and return to pipeline configuration, click
                        <uicontrol>Close Preview</uicontrol>.</cmd>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta">The following step is used in "Previewing a
                        Single Stage" and "Reviewing Snapshot Data"</draft-comment>
                </cmd>
            </step>
            <step id="NextStage">
                <cmd>To view data for the next stage, click the <uicontrol>Next Stage</uicontrol>
                    icon. Or, to view data for a different stage, select the stage in the pipeline
                    canvas.</cmd>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol id="ORIGIN_INFO"
                        >ORIGINS</uicontrol></draft-comment>
                </cmd>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>1stStepErrorHandling</uicontrol> -
                        Use this step for origins that are not cluster origins. Standalone
                        only</draft-comment>
                </cmd>
            </step>
            <step id="1stStepErrorHandling">
                <cmd>In the Properties panel, on the <wintitle>General</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_vlh_bgh_hr">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>General Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Name</entry>
                                    <entry>Stage name.</entry>
                                </row>
                                <row>
                                    <entry>Description</entry>
                                    <entry>Optional description.</entry>
                                </row>
                                <row>
                                    <entry>On Record Error <xref
                                            href="../Pipeline_Design/ErrorHandling-Stage.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_z4j_byn_lw"/></xref></entry>
                                    <entry>Error record handling for the stage: <ul
                                            id="ul_mmh_bgh_hr">
                                            <li>Discard - Discards the record.</li>
                                            <li>Send to Error - Sends the record to the pipeline for
                                                error handling.</li>
                                            <li>Stop Pipeline - Stops the pipeline. </li>
                                        </ul></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>1stStep-ClusterOrigin</uicontrol> -
                        Use for origins that ARE cluster origins - points out stop pipeline is not
                        valid.</draft-comment>
                </cmd>
            </step>
            <step id="1stStep-ClusterOrigin">
                <cmd>In the Properties panel, on the <wintitle>General</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_u44_lsn_xs">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>General Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Name</entry>
                                    <entry>Stage name.</entry>
                                </row>
                                <row>
                                    <entry>Description</entry>
                                    <entry>Optional description.</entry>
                                </row>
                                <row>
                                    <entry>Stage Library</entry>
                                    <entry>Library version that you want to use. </entry>
                                </row>
                                <row>
                                    <entry>On Record Error <xref
                                            href="../Pipeline_Design/ErrorHandling-Stage.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_gyg_4yn_lw"/></xref></entry>
                                    <entry>Error record handling for the stage: <ul
                                            id="ul_mp4_lsn_xs">
                                            <li>Discard - Discards the record.</li>
                                            <li>Send to Error - Sends the record to the pipeline for
                                                error handling.</li>
                                            <li>Stop Pipeline - Stops the pipeline. Not valid for
                                                cluster pipelines.</li>
                                        </ul></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"
                            ><uicontrol>1stStep-StageLib-EHandling</uicontrol> - Use this step for
                        non-cluster origins with stage libraries. </draft-comment>
                </cmd>
            </step>
            <step id="1stStep-StageLib-EHandling">
                <cmd>In the Properties panel, on the <wintitle>General</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_rv1_q3d_3t">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>General Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Name</entry>
                                    <entry>Stage name.</entry>
                                </row>
                                <row>
                                    <entry>Description</entry>
                                    <entry>Optional description.</entry>
                                </row>
                                <row>
                                    <entry>Stage Library</entry>
                                    <entry>Library version that you want to use. </entry>
                                </row>
                                <row>
                                    <entry>On Record Error <xref
                                            href="../Pipeline_Design/ErrorHandling-Stage.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_ur4_4yn_lw"/></xref></entry>
                                    <entry>Error record handling for the stage: <ul
                                            id="ul_kw1_q3d_3t">
                                            <li>Discard - Discards the record.</li>
                                            <li>Send to Error - Sends the record to the pipeline for
                                                error handling.</li>
                                            <li>Stop Pipeline - Stops the pipeline. Not valid for
                                                cluster pipelines.</li>
                                        </ul></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>O-AVROFILE</uicontrol> - The
                        following step is used by any file-based origin that reads from Avro and
                        includes compression – Should be S3, Directory, File Tail, Kinesis Consumer
                        origin? Origins that do not include compression conref rows from here. –
                        Hadoop FS, </draft-comment>
                </cmd>
            </step>
            <step id="O-AVRO-FILE">
                <cmd>For Avro data, on the <wintitle>Data Format</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_fv4_ggf_lx">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Avro Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row id="O-row-AvroSLocation">
                                    <entry>Avro Schema Location</entry>
                                    <entry>Location of the Avro schema definition to use when
                                        processing data:<ul id="ul_gv4_ggf_lx">
                                            <li>Message/Data Includes Schema - Use the schema in the
                                                file.</li>
                                            <li>In Pipeline Configuration - Use the schema provided
                                                in the stage configuration.</li>
                                            <li>Confluent Schema Registry - Retrieve the schema from
                                                the Confluent Schema Registry.</li>
                                        </ul><p>Using a schema in the stage configuration or in the
                                            Confluent Schema Registry can improve
                                        performance.</p></entry>
                                </row>
                                <row id="O-row-AvroSc">
                                    <entry>Avro Schema</entry>
                                    <entry>Avro schema definition used to process the data.
                                        Overrides any existing schema definitions associated with
                                        the data. <p>You can optionally use the runtime:loadResource
                                            function to use a schema definition stored in a runtime
                                            resource file. </p></entry>
                                </row>
                                <row id="O-row-A-SchemaRegURL">
                                    <entry>Schema Registry URLs</entry>
                                    <entry>Confluent Schema Registry URLs used to look up the
                                        schema. To add a URL, click <uicontrol>Add</uicontrol>. Use
                                        the following format to enter the
                                        URL:<codeblock>http://&lt;host name>:&lt;port number></codeblock></entry>
                                </row>
                                <row id="O-row-A-LookupSchema">
                                    <entry>Lookup Schema By</entry>
                                    <entry>Method used to look up the schema in the Confluent Schema
                                            Registry:<ul id="ul_hv4_ggf_lx">
                                            <li>Subject - Look up the specified Avro schema
                                                subject.</li>
                                            <li>Schema ID - Look up the specified Avro schema ID.
                                            </li>
                                        </ul>Overrides any existing schema definitions associated
                                        with the data. </entry>
                                </row>
                                <row id="O-row-A-SchemaSubj">
                                    <entry>Schema Subject</entry>
                                    <entry>Avro schema subject to look up in the Confluent Schema
                                            Registry.<p>If the specified subject has multiple schema
                                            versions, the origin uses the latest schema version for
                                            that subject. To use an older version, find the
                                            corresponding schema ID, and then set the
                                                <uicontrol>Look Up Schema By</uicontrol> property to
                                            Schema ID.</p></entry>
                                </row>
                                <row id="O-row-A-SchemaID">
                                    <entry>Schema ID</entry>
                                    <entry>Avro schema ID to look up in the Confluent Schema
                                        Registry.</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>O-AVRO-Mess</uicontrol> - The
                        following step is used by origins that read Avro data from messages and have
                        compression. Kinesis Consumer. Origins without compression uses rows - JMS,
                        Kafka.</draft-comment>
                </cmd>
            </step>
            <step id="O-AVRO-Mess">
                <cmd>For Avro data, on the <wintitle>Data Format</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_acd_2qd_3t">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Avro Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row id="Mess-row-AvroSchemaLoc">
                                    <entry>Avro Schema Location</entry>
                                    <entry>Location of the Avro schema definition to use when
                                        processing data:<ul id="ul_k2p_vx2_lx">
                                            <li>Message/Data Includes Schema - Use the schema in the
                                                message.</li>
                                            <li>In Pipeline Configuration - Use the schema provided
                                                in the stage configuration.</li>
                                            <li>Confluent Schema Registry - Retrieve the schema from
                                                the Confluent Schema Registry.</li>
                                        </ul><p>Using a schema in the stage configuration or in the
                                            Confluent Schema Registry can improve
                                        performance.</p></entry>
                                </row>
                                <row id="Mess-row-AvroSchema">
                                    <entry>Avro Schema</entry>
                                    <entry>Avro schema definition used to process the data.
                                        Overrides any existing schema definitions associated with
                                        the data. <p>You can optionally use the runtime:loadResource
                                            function to use a schema definition stored in a runtime
                                            resource file. </p></entry>
                                </row>
                                <row id="Mess-row-SchemaReg">
                                    <entry>Schema Registry URLs</entry>
                                    <entry>Confluent Schema Registry URLs used to look up the
                                        schema. To add a URL, click <uicontrol>Add</uicontrol>. Use
                                        the following format to enter the
                                        URL:<codeblock>http://&lt;host name>:&lt;port number></codeblock></entry>
                                </row>
                                <row id="Mess-row-LookupSchema">
                                    <entry>Lookup Schema By</entry>
                                    <entry>Method used to look up the schema in the Confluent Schema
                                            Registry:<ul id="ul_pb1_5df_lx">
                                            <li>Subject - Look up the specified Avro schema
                                                subject.</li>
                                            <li>Schema ID - Look up the specified Avro schema ID. </li>
                                            <li>Embedded Schema ID - Look up the Avro schema ID
                                                embedded in each message.</li>
                                        </ul>Overrides any existing schema definitions associated
                                        with the message. </entry>
                                </row>
                                <row id="Mess-row-SchemaSub">
                                    <entry>Schema Subject</entry>
                                    <entry>Avro schema subject to look up in the Confluent Schema
                                            Registry.<p>If the specified subject has multiple schema
                                            versions, the origin uses the latest schema version for
                                            that subject. To use an older version, find the
                                            corresponding schema ID, and then set the
                                                <uicontrol>Look Up Schema By</uicontrol> property to
                                            Schema ID.</p></entry>
                                </row>
                                <row id="Mess-row-SchemaID">
                                    <entry>Schema ID</entry>
                                    <entry>Avro schema ID to look up in the Confluent Schema
                                        Registry.</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="alisontaylor"><uicontrol>O-AVRO-KAFKA</uicontrol> used by
                        Kafka Consumer</draft-comment>
                </cmd>
            </step>
            <step id="O-AVRO-KAFKA">
                <cmd>For Avro data, on the <wintitle>Data Format</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_acd_9qd_4t">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Avro Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Avro Schema Location</entry>
                                    <entry>Location of the Avro schema definition to use when
                                        processing data:<ul id="ul_k6p_vx3_lx">
                                            <li>Message/Data Includes Schema - Use the schema in the
                                                message.</li>
                                            <li>In Pipeline Configuration - Use the schema provided
                                                in the stage configuration.</li>
                                            <li>Confluent Schema Registry - Retrieve the schema from
                                                the Confluent Schema Registry.</li>
                                        </ul><p>Using a schema in the stage configuration or in the
                                            Confluent Schema Registry can improve
                                        performance.</p></entry>
                                </row>
                                <row>
                                    <entry>Avro Schema</entry>
                                    <entry>Avro schema definition used to process the data.
                                        Overrides any existing schema definitions associated with
                                        the data. <p>You can optionally use the runtime:loadResource
                                            function to use a schema definition stored in a runtime
                                            resource file. </p></entry>
                                </row>
                                <row>
                                    <entry>Schema Registry URLs </entry>
                                    <entry>Confluent Schema Registry URLs used to look up the
                                        schema. To add a URL, click <uicontrol>Add</uicontrol>. Use
                                        the following format to enter the
                                        URL:<codeblock>http://&lt;host name>:&lt;port number></codeblock></entry>
                                </row>
                                <row>
                                    <entry>Lookup Schema By</entry>
                                    <entry>Method used to look up the schema in the Confluent Schema
                                            Registry:<ul id="ul_pb2_9df_lx">
                                            <li>Subject - Look up the specified Avro schema
                                                subject.</li>
                                            <li>Schema ID - Look up the specified Avro schema ID. </li>
                                            <li>Embedded Schema ID - Look up the Avro schema ID
                                                embedded in each message. You must configure the
                                                origin to use the Confluent method to deserialize
                                                the messages on the <uicontrol>Kafka</uicontrol>
                                                tab.</li>
                                        </ul>Overrides any existing schema definitions associated
                                        with the message. </entry>
                                </row>
                                <row>
                                    <entry>Schema Subject</entry>
                                    <entry>Avro schema subject to look up in the Confluent Schema
                                            Registry.<p>If the specified subject has multiple schema
                                            versions, the origin uses the latest schema version for
                                            that subject. To use an older version, find the
                                            corresponding schema ID, and then set the
                                                <uicontrol>Look Up Schema By</uicontrol> property to
                                            Schema ID.</p></entry>
                                </row>
                                <row>
                                    <entry>Schema ID</entry>
                                    <entry>Avro schema ID to look up in the Confluent Schema
                                        Registry.</entry>
                                </row>
                                <row>
                                    <entry>Key Deserializer <xref
                                            href="../Origins/KConsumer-DataFormats.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                placement="inline"/></xref></entry>
                                    <entry>Method used to deserialize the Kafka message key.<p>Set
                                            to Confluent when the Avro schema ID is embedded in each
                                            Kafka message. </p></entry>
                                </row>
                                <row>
                                    <entry>Value Deserializer</entry>
                                    <entry>Method used to deserialize the Kafka message value.<p>Set
                                            to Confluent when the Avro schema ID is embedded in each
                                            Kafka message.</p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>O-Binary </uicontrol>. Used by
                        message reading origins that have compression - Kinesis Consumer, Redis
                        Consumer. Non-compressed origins use rows - JMS, Kafka </draft-comment>
                </cmd>
            </step>
            <step id="O-Binary">
                <cmd>For binary data, on the <wintitle>Data Format</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_dpx_kdm_35">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Binary Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/Origin-FileCompression">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/Origin-FilePatternCompressed">
                                    <entry/>
                                </row>
                                <row id="O-row-Bin-MaxDataSize">
                                    <entry>Max Data Size (bytes)</entry>
                                    <entry>Maximum number of bytes in the message. Larger messages
                                        cannot be processed or written to error. </entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>O-Datagram</uicontrol> - Kafka
                        Consumer, WebSocket Server, WebSocket Client, COaP Server, HTTP Server, MQTT
                        Subscriber</draft-comment>
                </cmd>
            </step>
            <step id="O-Datagram">
                <cmd>For datagram data, on the <wintitle>Data Format</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_qrx_zqz_pw">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Datagram Properties</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Data Format</entry>
                                    <entry>Message type: <ul
                                            conref="#task_kzs_5vz_sq/ul-UDPDataFormat"
                                            id="ul_q1q_yp4_4x">
                                            <li/>
                                        </ul></entry>
                                </row>
                                <row>
                                    <entry
                                        conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/entry-TypesDBname"/>
                                    <entry><ph
                                            conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/ph-TypesDBinfo"
                                            /><p>For collectd data only.</p></entry>
                                </row>
                                <row>
                                    <entry
                                        conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/entry-HiRestitle"/>
                                    <entry><ph
                                            conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/ph-HiResinfo"
                                            /><p>For collectd data only.</p></entry>
                                </row>
                                <row>
                                    <entry
                                        conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/entry-Exludename"/>
                                    <entry><ph
                                            conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/ph-Excludeinfo"
                                            /><p>For collectd data only.</p></entry>
                                </row>
                                <row>
                                    <entry
                                        conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/entry-Authname"/>
                                    <entry><ph
                                            conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/ph-Authinfo"
                                            /><p>For collectd data only.</p></entry>
                                </row>
                                <row>
                                    <entry><ph conref="#task_kzs_5vz_sq/ph-NetFlowMode-prop"> </ph>
                                    </entry>
                                    <entry><ph conref="#task_kzs_5vz_sq/ph-NetFlowMode-desc"/><ul
                                            conref="#task_kzs_5vz_sq/ph-NetFlowMode-ul"
                                            id="ul_zgh_m4l_3bb">
                                            <li/>
                                        </ul><p>For NetFlow 9 data only.</p></entry>
                                </row>
                                <row>
                                    <entry><ph conref="#task_kzs_5vz_sq/ph-Netflow-CacheSize-prop"
                                        /></entry>
                                    <entry><ph conref="#task_kzs_5vz_sq/ph-NetflowCache-desc1"
                                                /><p><ph
                                                conref="#task_kzs_5vz_sq/ph-NetflowCache-desc2"
                                            /></p><p>For NetFlow 9 data only.</p></entry>
                                </row>
                                <row>
                                    <entry><ph
                                            conref="#task_kzs_5vz_sq/ph-NetFlow-CacheTimeout-prop"
                                        /></entry>
                                    <entry><ph
                                            conref="#task_kzs_5vz_sq/ph-Netflow-CacheTimeout-desc1"
                                                /><p><ph
                                                conref="#task_kzs_5vz_sq/ph-Netflow-CacheTimeout-desc2"
                                            /></p><p>For NetFlow 9 data only.</p></entry>
                                </row>
                                <row conref="ReusableTables.dita#concept_wfr_rnw_yq/MessagesCharset">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/IgnoreControlChars-row">
                                    <entry/>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>DELIMITED data</uicontrol> - Step
                        used by Delimited origins using compression – S3, Directory, Other Delimited
                        origins use steps. - Hadoop FS origin, JMS Consumer, Kafka
                        Consumer</draft-comment>
                </cmd>
            </step>
            <step id="DelimFILE">
                <cmd>For delimited data, on the <wintitle>Data Format</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_DelimitedData">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Delimited Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/Origin-FileCompression">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/Origin-FilePatternCompressed">
                                    <entry/>
                                </row>
                                <row id="O-row-DelimFormatType">
                                    <entry>Delimiter Format Type</entry>
                                    <entry>Delimiter format type. Use one of the following options:
                                            <ul
                                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/ul_delFileTypes"
                                            id="ul_s5z_b3z_3r">
                                            <li/>
                                        </ul></entry>
                                </row>
                                <row id="O-row-Del-HeaderLine">
                                    <entry>Header Line</entry>
                                    <entry>Indicates whether a file contains a header line, and
                                        whether to use the header line.</entry>
                                </row>
                                <row id="row-Del-ExtraColumns">
                                    <entry>Allow Extra Columns</entry>
                                    <entry>When processing data with a header line, allows
                                        processing records with more columns than exist in the
                                        header line.</entry>
                                </row>
                                <row id="row-Del-ExtraColPrefix">
                                    <entry>Extra Column Prefix</entry>
                                    <entry>Prefix to use for any additional columns. Extra columns
                                        are named using the prefix and sequential increasing
                                        integers as follows:
                                            <codeph>&lt;prefix>&lt;integer></codeph>. <p>For
                                            example, _extra_1. Default is _extra_.</p></entry>
                                </row>
                                <row id="O-row-Del-MaxRecLength">
                                    <entry>Max Record Length (chars)</entry>
                                    <entry>Maximum length of a record in characters. Longer records
                                        are not read. <p><ph
                                                conref="ReusablePhrases.dita#concept_vhs_5tz_xp/ph-MaxRecordSize"
                                            /></p></entry>
                                </row>
                                <row id="O-row-Del-DelChar">
                                    <entry>Delimiter Character</entry>
                                    <entry>Delimiter character for a custom delimiter format. Select
                                        one of the available options or use Other to enter a custom
                                            character.<p>You can enter a Unicode control character
                                            using the format \u<i>NNNN</i>, where ​<i>N</i> is a
                                            hexadecimal digit from the numbers 0-9 or the letters
                                            A-F. For example, enter \u0000 to use the null character
                                            as the delimiter or \u2028 to use a line separator as
                                            the delimiter.</p><p>Default is the pipe character ( |
                                            ).</p></entry>
                                </row>
                                <row id="O-row-Del-EscChar">
                                    <entry>Escape Character</entry>
                                    <entry>Escape character for a custom file type.</entry>
                                </row>
                                <row id="O-row-Del-QuoteChar">
                                    <entry>Quote Character</entry>
                                    <entry>Quote character for a custom file type.</entry>
                                </row>
                                <row id="O-row-Del-RootField">
                                    <entry>Root Field Type <xref
                                            href="../Data_Formats/DelimitedDataRootFieldTypes.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_exx_41q_ft"/></xref></entry>
                                    <entry>Root field type to use:<ul id="ul_izr_p1q_ft">
                                            <li>List-Map - Generates an indexed list of data.
                                                Enables you to use standard functions to process
                                                data. Use for new pipelines.</li>
                                            <li>List - Generates a record with an indexed list with
                                                a map for header and value. Requires the use of
                                                delimited data functions to process data. Use only
                                                to maintain pipelines created before 1.1.0.</li>
                                        </ul></entry>
                                </row>
                                <row id="O-row-Del-Lines2skip">
                                    <entry>Lines to Skip</entry>
                                    <entry>Lines to skip before reading data. </entry>
                                </row>
                                <row id="O-row-Del-ParseNulls">
                                    <entry>Parse NULLs</entry>
                                    <entry>Replaces the specified string constant with null
                                        values.</entry>
                                </row>
                                <row id="O-row-Del-NullConstant">
                                    <entry>NULL Constant</entry>
                                    <entry>String constant to replace with null values.</entry>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/Charset">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/IgnoreControlChars-row">
                                    <entry/>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><b>JSON-2props </b> - Kafka Consumer, Kinesis
                        all use them. HTTP Client, and probably others use the 2nd row. Non
                        compression origins use rows - JMS Consumer, Kafka Consumer</draft-comment>
                </cmd>
            </step>
            <step id="JSON-2props">
                <cmd>For JSON data, on the <wintitle>Data Format</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_JSONdata">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>JSON Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/Origin-FileCompression">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/Origin-FilePatternCompressed">
                                    <entry/>
                                </row>
                                <row id="ROW-JSONContent">
                                    <entry>JSON Content</entry>
                                    <entry>Type of JSON content. Use one of the following options: <p>
                                            <ul id="ul_atw_krl_5q">
                                                <li>Array of Objects </li>
                                                <li>Multiple Objects</li>
                                            </ul>
                                        </p></entry>
                                </row>
                                <row id="ROW-MaxObject">
                                    <entry>Maximum Object Length (chars)</entry>
                                    <entry>Maximum number of characters in a JSON object. <p>Longer
                                            objects are diverted to the pipeline for error handling.
                                                </p><p><ph
                                                conref="ReusablePhrases.dita#concept_vhs_5tz_xp/ph-MaxRecordSize"
                                            /></p></entry>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/Charset">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/IgnoreControlChars-row">
                                    <entry/>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>LOG data </uicontrol> - Used in JMS
                        Consumer, Kafka Consumer, Hadoop FS, probably Directory. Use the following
                        for origins that use the Log data format. UL in first row is also being
                        conrefed in Log Parser. </draft-comment>
                </cmd>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta">Also - For any changes to the bullets below this
                        first table, update Log Parser as well. Everything except the Log4j
                        table/bullet point is copied from here. </draft-comment>
                </cmd>
            </step>
            <step id="LogData_Log4j">
                <cmd>For log data, on the <wintitle>Data Format</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_ihc_3fs_sr">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Log Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/Origin-FileCompression">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/Origin-FilePatternCompressed">
                                    <entry/>
                                </row>
                                <row id="O-row-LogFormat">
                                    <entry>Log Format</entry>
                                    <entry>Format of the log files. Use one of the following
                                            options:<ul id="ul-LogFormatList">
                                            <li>Common Log Format</li>
                                            <li>Combined Log Format</li>
                                            <li>Apache Error Log Format</li>
                                            <li>Apache Access Log Custom Format</li>
                                            <li>Regular Expression</li>
                                            <li>Grok Pattern</li>
                                            <li>Log4j</li>
                                        </ul></entry>
                                </row>
                                <row id="O-row-MaxLine">
                                    <entry>Max Line Length</entry>
                                    <entry>Maximum length of a log line. The origin truncates longer
                                        lines. <p><ph
                                                conref="ReusablePhrases.dita#concept_vhs_5tz_xp/ph-MaxRecordSize"
                                            /></p></entry>
                                </row>
                                <row id="O-row-RetainOLine">
                                    <entry>Retain Original Line</entry>
                                    <entry>Determines how to treat the original log line. Select to
                                        include the original log line as a field in the resulting
                                            record.<p>By default, the original line is
                                            discarded.</p></entry>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/Charset">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/IgnoreControlChars-row">
                                    <entry/>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                    <ul id="ul-LogDetails">
                        <li>When you select <uicontrol>Apache Access Log Custom Format</uicontrol>,
                            use Apache log format strings to define the <uicontrol>Custom Log
                                Format</uicontrol>.</li>
                        <li>When you select <uicontrol>Regular Expression</uicontrol>, enter the
                            regular expression that describes the log format, and then map the
                            fields that you want to include to each regular expression group.</li>
                        <li>When you select <uicontrol>Grok Pattern</uicontrol>, you can use the
                                <uicontrol>Grok Pattern Definition</uicontrol> field to define
                            custom grok patterns. You can define a pattern on each line. <p>In the
                                    <uicontrol>Grok Pattern</uicontrol> field, enter the pattern to
                                use to parse the log. You can use a predefined grok patterns or
                                create a custom grok pattern using patterns defined in
                                    <uicontrol>Grok Pattern Definition</uicontrol>.</p><p>For more
                                information about defining grok patterns and supported grok
                                patterns, see <xref
                                    href="../Apx-GrokPatterns/GrokPatterns.dita#concept_vdk_xjb_wr"
                                />.</p></li>
                        <li>When you select <uicontrol>Log4j</uicontrol>, define the following properties:<p>
                                <table frame="all" rowsep="1" colsep="1" id="table_pzm_rbt_sr">
                                    <tgroup cols="2">
                                        <colspec colname="c1" colnum="1" colwidth="1*"/>
                                        <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                                        <thead>
                                            <row>
                                                <entry>Log4j Property</entry>
                                                <entry>Description</entry>
                                            </row>
                                        </thead>
                                        <tbody>
                                            <row>
                                                <entry>On Parse Error</entry>
                                                <entry>Determines how to handle information that
                                                  cannot be parsed:<ul id="ul_bvm_5bt_sr">
                                                  <li>Skip and Log Error - Skips reading the line
                                                  and logs a stage error.</li>
                                                  <li>Skip, No Error - Skips reading the line and
                                                  does not log an error.</li>
                                                  <li>Include as Stack Trace - Includes information
                                                  that cannot be parsed as a stack trace to the
                                                  previously-read log line. The information is added
                                                  to the message field for the last valid log
                                                  line.</li>
                                                  </ul></entry>
                                            </row>
                                            <row>
                                                <entry>Use Custom Log Format</entry>
                                                <entry>Allows you to define a custom log
                                                  format.</entry>
                                            </row>
                                            <row>
                                                <entry>Custom Format</entry>
                                                <entry>Use log4j variables to define a custom log
                                                  format. </entry>
                                            </row>
                                        </tbody>
                                    </tgroup>
                                </table>
                            </p></li>
                    </ul>
                </info>
            </step>
            <step>
                <cmd/>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>O-PROTO-Mess</uicontrol>. Step used
                        by message-reading stages except for Kafka. Kafka conrefs the first two rows
                        of table. <p>NOTE: When making changes, make sure to update the file version
                            and destination versions, as necessary.</p></draft-comment>
                </cmd>
            </step>
            <step id="O-PROTO-Mess">
                <cmd>For protobuf data, on the <wintitle>Data Format</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_s3c_mz4_45">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Protobuf Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/Origin-FileCompression">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/Origin-FilePatternCompressed">
                                    <entry/>
                                </row>
                                <row id="Mess-ProtoDescFile">
                                    <entry>Protobuf Descriptor File </entry>
                                    <entry>Descriptor file (.desc) to use. The descriptor file must
                                        be in the <ph
                                            conref="ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                        /> resources directory,
                                            <codeph>$SDC_RESOURCES</codeph>.<p>For information about
                                            generating the descriptor file, see <xref
                                                href="../Data_Formats/Protobuf-Prerequisites.dita"
                                            />. <ph
                                                conref="ReusablePhrases.dita#concept_vhs_5tz_xp/SDCDPM_SDCenvConfigs"
                                            /></p></entry>
                                </row>
                                <row id="Mess-MessType">
                                    <entry>Message Type</entry>
                                    <entry>The fully-qualified name for the message type to use when
                                        reading data.<p>Use the following format:
                                                <codeph>&lt;package name>.&lt;message
                                            type></codeph>. </p>Use a message type defined in the
                                        descriptor file.</entry>
                                </row>
                                <row id="Mess-DelimMessages">
                                    <entry>Delimited Messages</entry>
                                    <entry>Indicates if a message might include more than one
                                        protobuf message.</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>O-Proto-File</uicontrol>. Step used
                        by file-based proto. <p>NOTE: When making changes, make sure to update the
                            message version and destination versions, as
                        necessary.</p></draft-comment>
                </cmd>
            </step>
            <step id="O-PROTO-File">
                <cmd>For protobuf data, on the <wintitle>Data Format</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_f4r_4cp_45">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Protobuf Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/Origin-FileCompression">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/Origin-FilePatternCompressed">
                                    <entry/>
                                </row>
                                <row>
                                    <entry>Protobuf Descriptor File </entry>
                                    <entry>Descriptor file (.desc) to use. The descriptor file must
                                        be in the <ph
                                            conref="ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                        /> resources directory, <codeph>$SDC_RESOURCES</codeph>.
                                                <p><ph
                                                conref="ReusablePhrases.dita#concept_vhs_5tz_xp/SDCDPM_SDCenvConfigs"
                                            /> For information about generating the descriptor file,
                                            see <xref
                                                href="../Data_Formats/Protobuf-Prerequisites.dita"
                                            />.</p></entry>
                                </row>
                                <row>
                                    <entry>Message Type</entry>
                                    <entry>The fully-qualified name for the message type to use when
                                        reading data.<p>Use the following format:
                                                <codeph>&lt;package name>.&lt;message
                                            type></codeph>. </p>Use a message type defined in the
                                        descriptor file.</entry>
                                </row>
                                <row>
                                    <entry>Delimited Messages</entry>
                                    <entry>Indicates if a file might include more than one protobuf
                                        message.</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>O-SDCRecord</uicontrol> - Adding to
                        origins that use it. Non-compression origins use rows.</draft-comment>
                </cmd>
            </step>
            <step id="O-SDCRecord">
                <cmd>For SDC Record data, on the <wintitle>Data Format</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_rtb_csl_wx">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>SDC Record Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/Origin-FileCompression">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/Origin-FilePatternCompressed">
                                    <entry/>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="alisontaylor"><uicontrol>TEXT_Hadoop data</uicontrol> -
                        Hadoop FS and MapR FS origins (Include Custom Delimiter is not applicable) –
                        should this go in Local FS too? </draft-comment>
                </cmd>
            </step>
            <step id="Text_Hadoop">
                <cmd>For text data, on the <wintitle>Data Format</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_ney_vwc_fv">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Text Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Max Line Length</entry>
                                    <entry>Maximum number of characters allowed for a line. Longer
                                        lines are truncated.<p>Adds a boolean field to the record to
                                            indicate if it was truncated. The field name is
                                            Truncated. </p><p><ph
                                                conref="ReusablePhrases.dita#concept_vhs_5tz_xp/ph-MaxRecordSize"
                                            /></p></entry>
                                </row>
                                <row>
                                    <entry>Use Custom Delimiter <xref
                                            href="../Data_Formats/TextCDelim.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                placement="inline" id="image_f8g_d37_xv"
                                        /></xref></entry>
                                    <entry>Uses custom delimiters to define records instead of line
                                        breaks. </entry>
                                </row>
                                <row>
                                    <entry>Custom Delimiter</entry>
                                    <entry>One or more characters to use to define records. </entry>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/Charset">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/IgnoreControlChars-row">
                                    <entry/>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>TEXT data</uicontrol> - Amazon S3,
                        Directory, Kafka Consumer, Kinesis Consumer, JMS Consumer. HTTP Client.
                        WebSocket Client. Non compression origins using rows - JMS</draft-comment>
                </cmd>
            </step>
            <step id="Text">
                <cmd>For text data, on the <wintitle>Data Format</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_ngy_vcc_fv">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Text Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/Origin-FileCompression">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/Origin-FilePatternCompressed">
                                    <entry/>
                                </row>
                                <row id="O-row-MaxLineLength">
                                    <entry>Max Line Length</entry>
                                    <entry>Maximum number of characters allowed for a line. Longer
                                        lines are truncated.<p>Adds a boolean field to the record to
                                            indicate if it was truncated. The field name is
                                            Truncated. </p><p><ph
                                                conref="ReusablePhrases.dita#concept_vhs_5tz_xp/ph-MaxRecordSize"
                                            /></p></entry>
                                </row>
                                <row id="O-row-UseCDelim">
                                    <entry>Use Custom Delimiter <xref
                                            href="../Data_Formats/TextCDelim.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                placement="inline" id="image_f2g_d54_xv"
                                        /></xref></entry>
                                    <entry>Uses custom delimiters to define records instead of line
                                        breaks. </entry>
                                </row>
                                <row id="O-row-CustomDelim">
                                    <entry>Custom Delimiter</entry>
                                    <entry>One or more characters to use to define records. </entry>
                                </row>
                                <row id="O-row-IncludeCDelim">
                                    <entry>Include Custom Delimiter</entry>
                                    <entry>Includes delimiter characters in the record.</entry>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/Charset">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/IgnoreControlChars-row">
                                    <entry/>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>O-WholeFile</uicontrol> -
                        Directory</draft-comment>
                </cmd>
            </step>
            <step id="O-WholeFile">
                <cmd>For whole files on the <wintitle>Data Format</wintitle> tab, configure the
                    following property:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_ff1_v3h_xw">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Whole File Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Buffer Size (bytes)</entry>
                                    <entry>Size of the buffer to use to transfer data.</entry>
                                </row>
                                <row>
                                    <entry>Rate per Second <xref
                                            href="../Data_Formats/WholeFile-TransferRate.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                placement="inline"/></xref></entry>
                                    <entry>Transfer rate to use. <p>Enter a number to specify a rate
                                            in bytes per second. Use an expression to specify a rate
                                            that uses a different unit of measure per second, e.g.
                                            ${5 * MB}. Use -1 to opt out of this property. </p><p>By
                                            default, the origin does not use a transfer rate.
                                        </p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>O-WholeFile-Checksum</uicontrol> -
                        S3</draft-comment>
                </cmd>
            </step>
            <step id="O-WholeFile-Checksum">
                <cmd>For whole files, on the <wintitle>Data Format</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_x5m_5xf_zw">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Whole File Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Buffer Size (bytes)</entry>
                                    <entry>Size of the buffer to use to transfer data.</entry>
                                </row>
                                <row>
                                    <entry>Rate per Second <xref
                                            href="../Data_Formats/WholeFile-TransferRate.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                placement="inline" id="image_ljp_wy2_py"
                                        /></xref></entry>
                                    <entry>Transfer rate to use. <p>Enter a number to specify a rate
                                            in bytes per second. Use an expression to specify a rate
                                            that uses a different unit of measure per second, e.g.
                                            ${5 * MB}. Use -1 to opt out of this property. </p><p>By
                                            default, the origin does not use a transfer rate.
                                        </p></entry>
                                </row>
                                <row>
                                    <entry>Verify Checksum</entry>
                                    <entry>Verifies the checksum during the read. </entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>XML data </uicontrol>- step used by
                        message and file based origins: Kafka Consumer, Hadoop FS, Kinesis Consumer,
                        Directory, S3, HTTP Client, WebSocket Client, maybe more. Origins without
                        compression uses almost all the rows - JMS Consumer, Kafka Consumer, Rabbit
                        MQ Consumer, Redis Consumer</draft-comment>
                </cmd>
            </step>
            <step id="XMLprops">
                <cmd>For XML data, on the <wintitle>XML</wintitle> tab, configure the following
                    properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_pmz_mcj_45">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>XML Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/Origin-FileCompression">
                                    <entry/>
                                </row>
                                <row id="O-row-XMLDelimElement">
                                    <entry>Delimiter Element <xref
                                            href="../Data_Formats/XMLDF-XPath-Syntax.dita#concept_tmc_4bc_dy">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                placement="inline" id="image_xwx_xrm_js"
                                        /></xref></entry>
                                    <entry>
                                        <p>Delimiter to use to generate records. Omit a delimiter to
                                            treat the entire XML document as one record. Use one of
                                            the following:<ul id="XML-ul-delimiterelements">
                                                <li>An XML element directly under the root element.
                                                  <p>Use the XML element name without surrounding
                                                  angle brackets ( &lt; > ) . For example, msg
                                                  instead of &lt;msg>. </p></li>
                                                <li>A simplified XPath expression that specifies the
                                                  data to use.<p>Use a simplified XPath expression
                                                  to access data deeper in the XML document or data
                                                  that requires a more complex access
                                                  method.</p><p>For more information about valid
                                                  syntax, see <xref
                                                  href="../Data_Formats/XMLDF-XPath-Syntax.dita#concept_tmc_4bc_dy"
                                                  />.</p></li>
                                            </ul></p>
                                    </entry>
                                </row>
                                <row id="O-row-XMLFieldXpath">
                                    <entry>Include Field XPaths <xref
                                            href="../Data_Formats/XMLDF-FieldXPathExp.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                placement="inline"/></xref></entry>
                                    <entry>Includes the XPath to each parsed XML element and XML
                                        attribute in field attributes. Also includes each namespace
                                        in an xmlns record header attribute. <p>When not selected,
                                            this information is not included in the record. By
                                            default, the property is not selected.</p><p>
                                            <note><ph
                                                  conref="ReusablePhrases.dita#concept_vhs_5tz_xp/warn_FieldRecHeaderAtts"
                                                /></note>
                                        </p></entry>
                                </row>
                                <row id="O-row-XMLNamespaces">
                                    <entry>Namespaces </entry>
                                    <entry>Namespace prefix and URI to use when parsing the XML
                                        document. Define namespaces when the XML element being used
                                        includes a namespace prefix or when the XPath expression
                                        includes namespaces.<p>For information about using
                                            namespaces with an XML element, see <xref
                                                href="../Data_Formats/XMLDF-XMLelem-Namespace.dita#concept_ilc_r3g_2y"
                                            />.</p><p>For information about using namespaces with
                                            XPath expressions, see <xref
                                                href="../Data_Formats/XMLDF-XPath-Namespaces.dita#concept_mkk_3zj_dy"
                                            />.</p><p>Using <xref
                                                href="../Pipeline_Configuration/SimpleBulkEdit.dita#concept_alb_b3y_cbb"
                                                >simple or bulk edit mode</xref>, click the
                                                <uicontrol>Add</uicontrol> icon to add additional
                                            namespaces.</p></entry>
                                </row>
                                <row id="O-row-XMLOutputAtts">
                                    <entry>Output Field Attributes</entry>
                                    <entry>Includes XML attributes and namespace declarations in the
                                        record as field attributes. When not selected, XML
                                        attributes and namespace declarations are included in the
                                        record as fields.<note><ph
                                                conref="ReusablePhrases.dita#concept_vhs_5tz_xp/ph-FieldAtts-SDCRecord"
                                            /> For more information about working with field
                                            attributes, see <xref
                                                href="../Pipeline_Design/FieldAttributes.dita#concept_xfm_wtp_1z"
                                            />.</note><p>By default, the property is not
                                            selected.</p></entry>
                                </row>
                                <row id="O-row-XML-MaxRecLength">
                                    <entry>Max Record Length (chars) </entry>
                                    <entry>
                                        <p>The maximum number of characters in a record. Longer
                                            records are diverted to the pipeline for error handling. </p>
                                        <p><ph
                                                conref="ReusablePhrases.dita#concept_vhs_5tz_xp/ph-MaxRecordSize"
                                            /></p>
                                    </entry>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/Charset">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/IgnoreControlChars-row">
                                    <entry/>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>ORIGIN-MIXED REUSE - </uicontrol>The
                        following handful of steps are for origin/origin, origin/processor or
                        possibly origin/destination reuse. In alphabetical order.</draft-comment>
                </cmd>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta">S3 origins:</draft-comment>
                </cmd>
            </step>
            <step id="step-AmazonOrigins-Advanced">
                <cmd>On the <wintitle>Advanced</wintitle> tab, optionally configure proxy
                    information:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_ldy_2js_zw">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Advanced Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Connection Timeout</entry>
                                    <entry>Seconds to wait for a response before closing the
                                        connection. <p>Default is 10 seconds.</p></entry>
                                </row>
                                <row>
                                    <entry>Socket Timeout</entry>
                                    <entry>Seconds to wait for a response to a query.</entry>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/row-UseProxy">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/row-ProxyHost">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/row-ProxyPort">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/row-ProxyUser">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/row-ProxyPassword">
                                    <entry/>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta">Amazon S3 and Google Cloud Storage
                        origins:</draft-comment>
                </cmd>
            </step>
            <step id="Step-ObjectErrorHandling">
                <cmd>On the <uicontrol>Error Handling</uicontrol> tab, configure the following
                    properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_o42_2qv_ht">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Error Handling Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row id="row-ErrorHandlingOption">
                                    <entry>Error Handling Option</entry>
                                    <entry>The action taken when an error occurs while processing an
                                            object:<ul id="ul_hvm_gqv_ht">
                                            <li>None - Keeps the object in place.</li>
                                            <li>Archive - Copies or moves the object to another
                                                prefix or bucket.</li>
                                            <li>Delete - Deletes the object.</li>
                                        </ul><p>When archiving processed objects, best practice is
                                            to also archive objects that cannot be processed.
                                        </p></entry>
                                </row>
                                <row id="row-ArchivingOption">
                                    <entry>Archiving Option</entry>
                                    <entry>The action to take when archiving an object that cannot
                                        be processed. <p
                                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/S3origin-CopyMove1"
                                            /><p
                                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/S3origin-CopyMove2"
                                        /></entry>
                                </row>
                                <row id="row-ErrorPrefix">
                                    <entry>Error Prefix</entry>
                                    <entry>Prefix for the objects that cannot be processed.</entry>
                                </row>
                                <row id="row-ErrorBucket">
                                    <entry>Error Bucket</entry>
                                    <entry>Bucket for the objects that cannot be processed.</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>HTTPClient-Credentials</uicontrol> -
                        The following steps are used in HTTP Client origin, processor, and
                        destination Configuring topics:</draft-comment>
                </cmd>
            </step>
            <step id="HTTPClient-Credentials">
                <cmd>When using authentication, on the <wintitle>Credentials</wintitle> tab,
                    configure the following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_qfx_54h_jw">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Credentials Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Username</entry>
                                    <entry>User name for basic, digest, or universal authentication.
                                    </entry>
                                </row>
                                <row>
                                    <entry>Password</entry>
                                    <entry>Password for basic, digest, or universal authentication.
                                            <note
                                            conref="ReusablePhrases.dita#concept_vhs_5tz_xp/SDCDPM_Tip_Usernames"
                                        /></entry>
                                </row>
                                <row>
                                    <entry>Consumer Key</entry>
                                    <entry>Consumer key for OAuth 1.0 authentication.</entry>
                                </row>
                                <row>
                                    <entry>Consumer Secret</entry>
                                    <entry>Consumer secret for OAuth 1.0 authentication.</entry>
                                </row>
                                <row>
                                    <entry>Token</entry>
                                    <entry>Consumer token for OAuth 1.0 authentication.</entry>
                                </row>
                                <row>
                                    <entry>Token Secret</entry>
                                    <entry>Token secret for OAuth 1.0 authentication.</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>HTTPClient-Proxy</uicontrol> - - The
                        following steps are used in HTTP Client origin, processor, and destination
                        Configuring topics:</draft-comment>
                </cmd>
            </step>
            <step id="HTTPClient-Proxy">
                <cmd>To use an HTTP proxy, on the <wintitle>Proxy</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="HTTPProxy-table">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>HTTP Proxy Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Proxy URI</entry>
                                    <entry>Proxy URI.</entry>
                                </row>
                                <row>
                                    <entry>Username</entry>
                                    <entry>Proxy user name.</entry>
                                </row>
                                <row>
                                    <entry>Password</entry>
                                    <entry>Proxy password.<note
                                            conref="ReusablePhrases.dita#concept_vhs_5tz_xp/SDCDPM_Tip_Usernames"
                                        /></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>HTTPClient-SSLTLS</uicontrol> - The
                        following steps are used in HTTP Client origin, processor, and destination
                        Configuring topics. Also used in the MQTT Subscriber origin and MQTT
                        Publisher destination configuring topics.</draft-comment>
                </cmd>
            </step>
            <step id="HTTPClient-SSLTLS">
                <cmd>Optionally, on the <uicontrol>SSL/TLS</uicontrol> tab, configure truststore and
                    keystore details:</cmd>
                <info>
                    <draft-comment author="Loretta">delete when fully replaced with
                        TLS.</draft-comment>
                    <note conref="ReusablePhrases.dita#concept_vhs_5tz_xp/SDCDPM_Tip_Passwords"/>
                    <table frame="all" rowsep="1" colsep="1" id="SSLTLS_table">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>SSL/TLS Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Path to Truststore</entry>
                                    <entry>Absolute path to the truststore. </entry>
                                </row>
                                <row>
                                    <entry>Password</entry>
                                    <entry>Truststore password.</entry>
                                </row>
                                <row>
                                    <entry>Path to Keystore</entry>
                                    <entry>Absolute path to the keystore. </entry>
                                </row>
                                <row>
                                    <entry>Password</entry>
                                    <entry>Keystore password.</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta" id="TLS-Destinations">The following TLS-All step
                        is used by HTTP Server, HTTP to Kafka, SDC RPC to Kafka. Using Keystore+
                        rows are: TCP Server, SDC RPC origin. <p>Using Truststore+ rows are: HTTP
                            Client origin, processor &amp; destination, MQTT consumer &amp;
                            producer, SDC RPC destination, Websocket, write to another pipeline
                            </p><p>SunX509 is also a conref'ed ph</p></draft-comment>
                </cmd>
            </step>
            <step id="TLS-All">
                <cmd>To use SSL/TLS, click the <wintitle>TLS</wintitle> tab and configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_p5k_xqy_tz">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>TLS Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row id="TLS-EnableTLS-row">
                                    <entry>Use TLS</entry>
                                    <entry>
                                        <p>Enables the use of TLS. </p>
                                    </entry>
                                </row>
                                <row id="TLS-KeystoreFile-row">
                                    <entry>Keystore File <xref
                                            href="../Pipeline_Configuration/SSL-TLS-KeystoreTruststore.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                placement="inline" id="image_z24_nfv_zz"
                                        /></xref></entry>
                                    <entry>The path to the keystore file. Enter an absolute path to
                                        the file or a path relative to the <ph
                                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                        /> resources directory: <filepath>$SDC_RESOURCES</filepath>.
                                                <p><ph
                                                conref="ReusablePhrases.dita#concept_vhs_5tz_xp/SDCDPM_SDCenvConfigs"
                                            /></p><p>By default, no keystore is used. </p></entry>
                                </row>
                                <row id="TLS-KeystoreType-row">
                                    <entry>Keystore Type</entry>
                                    <entry>Type of keystore to use. Use one of the following
                                            types:<ul id="ul-keystoreTruststore-Types">
                                            <li>Java Keystore File (JKS)</li>
                                            <li>PKCS-12 (p12 file)</li>
                                        </ul><p>Default is Java Keystore File (JKS). </p></entry>
                                </row>
                                <row id="TLS-KeystorePassword-row">
                                    <entry>Keystore Password</entry>
                                    <entry>Password to the keystore file. A password is optional,
                                        but recommended.<p>
                                            <note
                                                conref="ReusablePhrases.dita#concept_vhs_5tz_xp/SDCDPM_Tip_Passwords"
                                            />
                                        </p></entry>
                                </row>
                                <row id="TLS-KeystoreKeyAlgo-row">
                                    <entry>Keystore Key Algorithm</entry>
                                    <entry>The algorithm used to manage the keystore. <p>Default is
                                                <ph>SunX509</ph>.</p></entry>
                                </row>
                                <row id="TLS-TruststoreFile-row">
                                    <entry>Truststore File <xref
                                            href="../Pipeline_Configuration/SSL-TLS-KeystoreTruststore.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                placement="inline"/></xref></entry>
                                    <entry>The path to the truststore file. Enter an absolute path
                                        to the file or a path relative to the <ph
                                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                        /> resources directory: <filepath>$SDC_RESOURCES</filepath>.
                                                <p><ph
                                                conref="ReusablePhrases.dita#concept_vhs_5tz_xp/SDCDPM_SDCenvConfigs"
                                            /></p><p>By default, no truststore is used. </p></entry>
                                </row>
                                <row id="TLS-TruststoreType-row">
                                    <entry>Truststore Type</entry>
                                    <entry>Type of truststore to use. Use one of the following types:<ul>
                                            <li>Java Keystore File (JKS)</li>
                                            <li>PKCS-12 (p12 file)</li>
                                        </ul><p>Default is Java Keystore File (JKS). </p></entry>
                                </row>
                                <row id="TLS-TruststorePassword-row">
                                    <entry>Truststore Password</entry>
                                    <entry>Password to the truststore file. A password is optional,
                                        but recommended.<p>
                                            <note
                                                conref="ReusablePhrases.dita#concept_vhs_5tz_xp/SDCDPM_Tip_Passwords"
                                            />
                                        </p></entry>
                                </row>
                                <row id="TLS-TruststoreKeyAlgo-row">
                                    <entry>Truststore Trust Algorithm</entry>
                                    <entry>The algorithm used to manage the truststore. <p>Default
                                            is <ph id="TLS-DefaultAlgorithm-ph"
                                        >SunX509</ph>.</p></entry>
                                </row>
                                <row id="TLS-DefaultProtocols-row">
                                    <entry>Use Default Protocols</entry>
                                    <entry>Determines the transport layer security (TLS) protocol to
                                        use. The default protocol is <ph
                                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/TLS-defaultProtocol-ph"
                                        />. To use a different protocol, clear this option.</entry>
                                </row>
                                <row id="TLS-TransportProtocols-row">
                                    <entry>Transport Protocols <xref
                                            href="../Pipeline_Configuration/SSL-TLS-TransferProtocols.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                placement="inline" id="image_h5t_wfv_zz"
                                        /></xref></entry>
                                    <entry>The TLS protocols to use. To use a protocol other than
                                        the default <ph
                                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/TLS-defaultProtocol-ph"
                                        />, click the <uicontrol>Add</uicontrol> icon and enter the
                                        protocol name. You can use <xref
                                            href="../Pipeline_Configuration/SimpleBulkEdit.dita#concept_alb_b3y_cbb"
                                            >simple or bulk edit mode</xref> to add
                                            protocols.<note>Older protocols are not as secure as <ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/TLS-defaultProtocol-ph"
                                            />.</note></entry>
                                </row>
                                <row id="TLS-DefaultCipherSuites-Row">
                                    <entry>Use Default Cipher Suites</entry>
                                    <entry>Determines the cipher suite to use when performing the
                                        SSL/TLS handshake. <p><ph
                                                conref="ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            /> provides a set of cipher suites that it can use by
                                            default. For a full list, see <xref
                                                href="../Pipeline_Configuration/SSL-TLS-CipherSuites.dita#concept_cwx_dyf_5z"
                                            />.</p></entry>
                                </row>
                                <row id="TLS-CipherSuites-row">
                                    <entry>Cipher Suites</entry>
                                    <entry>Cipher suites to use. To use a cipher suite that is not a
                                        part of the default set, click the
                                            <uicontrol>Add</uicontrol> icon and enter the name of
                                        the cipher suite. You can use <xref
                                            href="../Pipeline_Configuration/SimpleBulkEdit.dita#concept_alb_b3y_cbb"
                                            >simple or bulk edit mode</xref> to add cipher
                                            suites.<p>Enter the Java Secure Socket Extension (JSSE)
                                            name for the additional cipher suites that you want to
                                            use. </p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta">The following step is used by JDBC Multitable
                        Consumer and certain rows are used by SQL Server Change
                        Tracking.</draft-comment>
                </cmd>
            </step>
            <step id="Step-JDBCTablestab">
                <cmd>On the <wintitle>Tables</wintitle> tab, define one or more table
                    configurations. Click the <uicontrol>Add</uicontrol> icon to define another
                    table configuration.</cmd>
                <info>Configure the following properties for each table configuration:<table
                        frame="all" rowsep="1" colsep="1" id="table_cvl_qp5_qy">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Tables Property <xref
                                            href="../Origins/MultiTableJDBCConsumer-TableConfiguration.dita"
                                                ><image href="../Graphics/icon_moreInfo.png"
                                                scale="10" id="image_hvl_qp2_qy"/>
                                        </xref></entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row id="JDBC-SchemaName">
                                    <entry>Schema Name</entry>
                                    <entry>Name of the schema to use for this table
                                        configuration.</entry>
                                </row>
                                <row>
                                    <entry>Table Name Pattern <xref
                                            href="../Origins/MultiTableJDBCConsumer-TableName.dita"
                                                ><image href="../Graphics/icon_moreInfo.png"
                                                scale="10" id="image_evl_qp5_qy"/>
                                        </xref></entry>
                                    <entry id="entry-TableNamePatt">
                                        <p id="p-TableNamePatt-1">Pattern of the table names to read
                                            for this table configuration. Use the SQL LIKE syntax to
                                            define the pattern. </p>
                                        <p id="p-TableNamePat-2">Default is the percentage wildcard
                                            (%) which matches all tables in the schema.</p>
                                    </entry>
                                </row>
                                <row>
                                    <entry>Table Exclusion Pattern <xref
                                            href="../Origins/MultiTableJDBCConsumer-TableName.dita"
                                                ><image href="../Graphics/icon_moreInfo.png"
                                                scale="10" id="image_fvl_qp5_qy"/>
                                        </xref></entry>
                                    <entry id="entry-TableExclusionPatt">Pattern of the table names
                                        to exclude from being read for this table configuration. Use
                                        a Java-based regular expression, or regex, to define the
                                            pattern.<p>Leave empty if you do not need to exclude any
                                            tables.</p></entry>
                                </row>
                                <row>
                                    <entry>Override Offset Columns <xref
                                            href="../Origins/MultiTableJDBCConsumer-Offsets.dita"
                                                ><image href="../Graphics/icon_moreInfo.png"
                                                scale="10" id="image_gvl_qp5_qy"/>
                                        </xref></entry>
                                    <entry>Determines whether to use the primary key or other
                                        columns as the offset columns for this table configuration.
                                            <p>Select to override the primary key and define other
                                            offset columns. Clear to use the primary key as the
                                            offset column. </p></entry>
                                </row>
                                <row>
                                    <entry>Initial Offset <xref
                                            href="../Origins/MultiTableJDBCConsumer-Offsets.dita"
                                                ><image href="../Graphics/icon_moreInfo.png"
                                                scale="10" id="image_hvl_qp5_qy"/>
                                        </xref></entry>
                                    <entry id="entry-InitialOffset">Offset value to use for this
                                        table configuration when the pipeline starts. Enter the
                                        primary key name or offset column name and value. For
                                        Datetime columns, enter a Long value.<p>When you define
                                            multiple offset columns, you must define an initial
                                            offset value for each column, in the same order that the
                                            columns are defined.</p></entry>
                                </row>
                                <row>
                                    <entry>Offset Column Conditions <xref
                                            href="../Origins/MultiTableJDBCConsumer-Offsets.dita"
                                                ><image href="../Graphics/icon_moreInfo.png"
                                                scale="10" id="image_ivl_qp5_qy"/>
                                        </xref></entry>
                                    <entry>Additional conditions that the origin uses to determine
                                        where to start reading data for this table configuration.
                                        The origin adds the defined condition to the WHERE clause of
                                        the SQL query. <p>Use the expression language to define the
                                            conditions. For example, you can use the offset:column
                                            function to compare the value of an offset
                                        column.</p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table></info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta">The following steps are used in <uicontrol>JDBC
                            Consumer, JDBC Producer, JDBC Lookup, and JDBC Tee </uicontrol>config
                        topics. <uicontrol>Oracle CDC Client</uicontrol> is using some info from the
                        Credentials table, and the whole Legacy Drivers step.</draft-comment>
                </cmd>
            </step>
            <step id="JDBC-Credentials">
                <cmd>To enter JDBC credentials separately from the JDBC connection string, on the
                        <uicontrol>Credentials</uicontrol> tab, configure the following
                    properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_ybf_v1w_ht">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Credentials Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Username</entry>
                                    <entry>User name for the JDBC connection.</entry>
                                </row>
                                <row id="row-JDBCpassword">
                                    <entry>Password</entry>
                                    <entry>Password for the JDBC account.<note
                                            conref="ReusablePhrases.dita#concept_vhs_5tz_xp/SDCDPM_Tip_Usernames"
                                        /></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step id="JDBC-Legacy">
                <cmd>When using JDBC versions older than 4.0, on the <uicontrol>Legacy
                        Drivers</uicontrol> tab, optionally configure the following
                    properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_LogData">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Legacy Driver Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>JDBC Class Driver Name</entry>
                                    <entry>Class name for the JDBC driver. Required for JDBC
                                        versions older than version 4.0.</entry>
                                </row>
                                <row>
                                    <entry>Connection Health Test Query</entry>
                                    <entry>Optional query to test the health of a connection.
                                        Recommended only when the JDBC version is older than 4.0.
                                    </entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta">The following step is for JMS origin and rows
                        are used by the JMS Producer.</draft-comment>
                </cmd>
            </step>
            <step id="step-JMSProps">
                <cmd>On the <wintitle>JMS</wintitle> tab, configure the following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_qzr_nfk_dt">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>JMS Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row id="row-JMSInitialContextFactory">
                                    <entry>JMS Initial Context Factory</entry>
                                    <entry>JMS initial context factory. <p>For example, for Apache
                                            ActiveMQ you might use:
                                                <codeph>org.apache.activemq.jndi.ActiveMQInitialContextFactory</codeph>.
                                            </p><p>Or for Oracle Weblogic, you might use:
                                                <codeph>weblogic.jndi.WLInitialContextFactory</codeph>.</p></entry>
                                </row>
                                <row id="row-JNDIConnectionFactory">
                                    <entry>JNDI Connection Factory</entry>
                                    <entry>JNDI connection factory.<p>For example, for ActiveMQ, you
                                            might use: <codeph>ConnectionFactory</codeph>.</p><p>For
                                            Weblogic, you might use:
                                                <codeph>jms/ConnectionFactory</codeph>.</p></entry>
                                </row>
                                <row id="row-JMSProviderURL">
                                    <entry>JMS Provider URL</entry>
                                    <entry>URL for the JMS provider.<p>For example, for ActiveMQ,
                                            you might use:
                                                <codeph>tcp://localhost:&lt;portno></codeph>.</p><p>For
                                            Weblogic, you might use:
                                                <codeph>t3/localhost:&lt;portno></codeph>.</p></entry>
                                </row>
                                <row>
                                    <entry>JMS Destination Name</entry>
                                    <entry>JMS queue or topic name. <p>For example, for ActiveMQ,
                                            you might use:<codeph>&lt;queue
                                            name></codeph>.</p><p>For Weblogic, you might use:
                                                <codeph>jms/Queue</codeph>.</p></entry>
                                </row>
                                <row>
                                    <entry>JMS Message Selector</entry>
                                    <entry>Optional message selector to limit the messages read by
                                        the origin. By default, reads all messages. </entry>
                                </row>
                                <row>
                                    <entry>JMS Destination Type</entry>
                                    <entry>Optional destination type. </entry>
                                </row>
                                <row id="row-AdditionaProps">
                                    <entry>Additional JMS Configuration Properties</entry>
                                    <entry>
                                        <p>Additional JMS or JNDI configuration properties to use.
                                            To add properties, click <uicontrol>Add</uicontrol> and
                                            define the JMS or JNDI property name and value. </p>
                                        <p>When you add a configuration property, enter the exact
                                            property name and value. The stage does not validate the
                                            property names or values. </p>
                                    </entry>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/MaxBatchSize">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/BatchWaitTime">
                                    <entry/>
                                </row>
                                <row id="row-UseJMSCredentials">
                                    <entry>Use Credentials</entry>
                                    <entry>Enables using JMS credentials.</entry>
                                </row>
                                <row>
                                    <entry>Produce Single Record</entry>
                                    <entry>Generates a single record for records that include
                                        multiple objects. <p>When not selected, the origin generates
                                            multiple records when a record includes multiple
                                            objects.</p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta">The following step is for JMS Consumer origin
                        and JMS Producer.</draft-comment>
                </cmd>
            </step>
            <step id="step-JMSCredentials">
                <cmd>When using JMS credentials, on the <wintitle>Credentials</wintitle> tab,
                    configure the following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_gqq_3vy_dt">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Credentials Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Username</entry>
                                    <entry>JMS user name.</entry>
                                </row>
                                <row>
                                    <entry>Password</entry>
                                    <entry>JMS password.<note
                                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/SDCDPM_Tip_Usernames"
                                        /></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta">Rows in this table are used by Kafka Consumer
                        and SDC RPC to Kafka origins, also UDP to Kafka.</draft-comment>
                </cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_pzz_wtr_pw">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Misc Kafka Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row id="O-KafkaBrokerURI">
                                    <entry>Broker URI</entry>
                                    <entry>Connection string for the Kafka broker. Use the following
                                        format: <codeph>&lt;host>:&lt;port></codeph>. <p>To ensure a
                                            connection, enter a comma-separated list of additional
                                            broker URIs.</p></entry>
                                </row>
                                <row id="O-KafkaTopic">
                                    <entry>Topic</entry>
                                    <entry>Kafka topic to read.</entry>
                                </row>
                                <row id="O-KafkaConfig">
                                    <entry>Kafka Configuration <xref
                                            href="../Origins/KConsumer_AdditionalKProp.dita#concept_d5f_n2g_vq">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_mwv_b52_zq"/></xref>
                                    </entry>
                                    <entry id="entry-AddKafkaConfigs">
                                        <p id="p-KafkaConfig1">Additional Kafka configuration
                                            properties to use. Using <xref
                                                href="../Pipeline_Configuration/SimpleBulkEdit.dita#concept_alb_b3y_cbb"
                                                >simple or bulk edit mode</xref>, click the
                                                <uicontrol>Add</uicontrol> icon to add properties.
                                            Define the Kafka property name and value.</p>
                                        <p id="p-KafkaConfig2">Use the property names and values as
                                            expected by Kafka.</p>
                                        <p id="p-KafkaConfig3">For information about enabling secure
                                            connections to Kafka, see <xref
                                                href="../Origins/KConsumer-EnablingSecurity.dita"
                                            />.</p>
                                    </entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>RabbitMQ</uicontrol> - Entire step
                        used by Consumer, rows used by Producer</draft-comment>
                </cmd>
            </step>
            <step id="RabbitMQ">
                <cmd>On the <wintitle>RabbitMQ</wintitle> tab, configure the following
                    properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_sdq_nzn_2v">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>RabbitMQ Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row id="Rabbit-URI">
                                    <entry>URI</entry>
                                    <entry>RabbitMQ URI. <p>Typically uses the following format:
                                                <codeph>amqp:&lt;host>:&lt;port>/&lt;virtualhost></codeph>.</p></entry>
                                </row>
                                <row>
                                    <entry>Consumer Tag</entry>
                                    <entry>RabbitMQ consumer tag. Leave empty to use an
                                        automatically generated consumer tag. </entry>
                                </row>
                                <row>
                                    <entry>One Record per Message</entry>
                                    <entry>Generates a single record for each RabbitMQ message.
                                            <p>When not selected, the origin generates a record for
                                            each object in the message. </p></entry>
                                </row>
                                <row id="Rabbit-AddConfig">
                                    <entry>Additional Client Configuration</entry>
                                    <entry>Additional RabbitMQ client configuration properties to
                                        use. To add properties, click <uicontrol>Add</uicontrol> and
                                        define the RabbitMQ client property name and value. <p>Use
                                            the property names and values as expected by RabbitMQ.
                                        </p></entry>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/MaxBatchSize">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/BatchWaitTime">
                                    <entry/>
                                </row>
                                <row id="Rabbit-UseCreds">
                                    <entry>Use Credentials</entry>
                                    <entry>Enables the use of RabbitMQ credentials.</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step id="Rabbit-CredentialsStep">
                <cmd>On the <wintitle>Credentials</wintitle> tab, enter the RabbitMQ credentials to
                    use if you enabled credentials.</cmd>
                <info>
                    <note conref="ReusablePhrases.dita#concept_vhs_5tz_xp/SDCDPM_Tip_Usernames"/>
                </info>
            </step>
            <step id="Rabbit-QueueStep">
                <cmd>On the <wintitle>Queue</wintitle> tab, configure the following queue
                    properties:</cmd>
                <info>These properties directly correspond to RabbitMQ properties. For more
                    information, see the RabbitMQ documentation. <table frame="all" rowsep="1"
                        colsep="1" id="table_sxy_wg4_q5">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Queue Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Name</entry>
                                    <entry>Name of the queue to use or create.</entry>
                                </row>
                                <row>
                                    <entry>Durable</entry>
                                    <entry>Creates a durable queue when selected. <p>The stage
                                            creates a durable queue by default.</p></entry>
                                </row>
                                <row>
                                    <entry>Exclusive</entry>
                                    <entry>Creates an exclusive queue when selected. When exclusive,
                                        the queue allows only the <ph
                                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                        /> to use it.<p>The stage creates an exclusive queue by
                                            default.</p></entry>
                                </row>
                                <row>
                                    <entry>Auto-Delete</entry>
                                    <entry>Automatically deletes a queue after all consumers
                                        unsubscribe. <p>When used with an exclusive queue, the queue
                                            is automatically deleted when you stop the pipeline.
                                        </p></entry>
                                </row>
                                <row>
                                    <entry>Declaration Properties</entry>
                                    <entry>Additional queue declaration properties to use. </entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table></info>
            </step>
            <step id="Rabbit-ExchangeStep">
                <cmd>On the <wintitle>Exchange</wintitle> tab, optionally configure the following
                    binding properties for the bindings that you want to use. When no bindings are
                    configured, the default exchange is used.</cmd>
                <info>These properties directly correspond to RabbitMQ properties. For more
                    information, see the RabbitMQ documentation. <table frame="all" rowsep="1"
                        colsep="1" id="table_ufb_p34_q5">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Exchange Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Name</entry>
                                    <entry>Binding name.</entry>
                                </row>
                                <row>
                                    <entry>Type</entry>
                                    <entry>Binding type.</entry>
                                </row>
                                <row>
                                    <entry>Durable</entry>
                                    <entry>Creates a durable exchange.</entry>
                                </row>
                                <row>
                                    <entry>Auto-Delete</entry>
                                    <entry>Automatically deletes an exchange when all queues are
                                        finished using it.</entry>
                                </row>
                                <row>
                                    <entry>Routing Key</entry>
                                    <entry>Routing key. <p>Leave empty to default to the queue
                                            name.</p></entry>
                                </row>
                                <row>
                                    <entry>Declaration Properties</entry>
                                    <entry>Additional exchange properties to use.</entry>
                                </row>
                                <row>
                                    <entry>Binding Properties</entry>
                                    <entry>Additional binding properties to use. </entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table></info>
            </step>
            <step id="Rabbit-AdvancedStep">
                <cmd>Optionally configure advanced options on the <wintitle>Advanced</wintitle>
                    tab.</cmd>
                <info>These properties directly correspond to RabbitMQ properties. For more
                    information, see the RabbitMQ documentation. </info>
                <info>Generally, you should use the defaults for these properties:<table frame="all"
                        rowsep="1" colsep="1" id="table_ebg_144_q5">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Advanced Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Automatic Recovery Enabled</entry>
                                    <entry>Determines whether to attempt to reestablish a
                                        connection.</entry>
                                </row>
                                <row>
                                    <entry>Network Recovery Interval</entry>
                                    <entry>Milliseconds to wait before attempting to reestablish a
                                        network connection. <p>Default is 5000.</p></entry>
                                </row>
                                <row>
                                    <entry>Connection Timeout (ms)</entry>
                                    <entry>Milliseconds for the connection to establish. Use 0 to
                                        opt out of a connection timeout. <p>Default is
                                        0.</p></entry>
                                </row>
                                <row>
                                    <entry>Handshake Timeout (ms)</entry>
                                    <entry>Milliseconds for the handshake to complete.</entry>
                                </row>
                                <row>
                                    <entry>Shutdown Timeout (ms)</entry>
                                    <entry>Milliseconds for the shutdown to complete. </entry>
                                </row>
                                <row>
                                    <entry>Heartbeat Timeout (secs)</entry>
                                    <entry>Seconds to wait for a heartbeat to verify that RabbitMQ
                                        is up and the connection still available.<p>Use 0 to avoid
                                            requesting heartbeats. Default is 0.</p></entry>
                                </row>
                                <row>
                                    <entry>Maximum Frame Size (bytes)</entry>
                                    <entry>Maximum frame size in bytes. Use for performance tuning.
                                            <p>Setting a larger value can improve throughput.
                                            Setting a smaller value can improve latency.</p><p>Use 0
                                            for no limit. Default is 0.</p></entry>
                                </row>
                                <row>
                                    <entry>Maximum Channel Number</entry>
                                    <entry>Maximum number of channels allowed. </entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table></info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta">The following is used for SDC RPC origins. Last
                        two rows used by HTTP to Kafka origin</draft-comment>
                </cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_uq4_2rr_pw">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Misc SDC RPC Properties</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row id="row-RPCListPort">
                                    <entry>RPC Listening Port</entry>
                                    <entry id="entry-RPCListPort">Port number to listen to for data.
                                        Must match one of the port numbers associated with the SDC
                                        RPC destination that provides the data.</entry>
                                </row>
                                <row id="row-RPC-ID">
                                    <entry>RPC ID</entry>
                                    <entry id="entry-RPC-ID">User-defined ID. Must match the RPC ID
                                        defined in the SDC RPC destination.</entry>
                                </row>
                                <row id="O-RPC-TLSenabled">
                                    <entry>TLS Enabled <xref
                                            href="../RPC_Pipelines/EnablingEncryption.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                placement="inline" id="image_b2n_wrr_pw"
                                        /></xref></entry>
                                    <entry>Enables the secure transfer of data using TLS (SSL).
                                            <p>To use encryption, both the origin and SDC RPC
                                            destination must be enabled for TLS.</p></entry>
                                </row>
                                <row id="O-RPC-KeystoreFile">
                                    <entry>Keystore File</entry>
                                    <entry>Keystore file for SSL/TLS. <p>Must be stored in the <ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            /> resources directory,
                                                <filepath>$SDC_RESOURCES</filepath>. For more
                                            information about environment variables, see <xref
                                                href="../Configuration/DCEnvironmentConfig.dita#concept_rng_qym_qr"
                                            />.</p></entry>
                                </row>
                                <row id="O-RPC-KeystorePass">
                                    <entry>Keystore Password</entry>
                                    <entry>Password for the keystore file.<note
                                            conref="ReusablePhrases.dita#concept_vhs_5tz_xp/SDCDPM_Tip_Passwords"
                                        /></entry>
                                </row>
                                <row>
                                    <entry/>
                                    <entry/>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>O-UDPprops</uicontrol> - whole step
                        used by UDP Source. Rows in this table by UDP to Kafka. Data format
                        bulletted list is also used by Configuring the Kafka Consumer > Datagram
                        step and various origins that support Datagram. Some rows/entries used by
                        Configuring the TCP Server origin.</draft-comment>
                </cmd>
            </step>
            <step id="O-step-UDPprops">
                <cmd>On the <wintitle>UDP</wintitle> tab, configure the following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_ovk_ndv_1s">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3*"/>
                            <thead>
                                <row>
                                    <entry>UDP Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row id="row-UDPport">
                                    <entry>Port</entry>
                                    <entry>Port to listen to for data. Using <xref
                                            href="../Pipeline_Configuration/SimpleBulkEdit.dita#concept_alb_b3y_cbb"
                                            >simple or bulk edit mode</xref>, click the<uicontrol>
                                            Add</uicontrol> icon to list additional ports.<note>To
                                            listen to a port below 1024, <ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            /> must be run by a user with root privileges.
                                            Otherwise, the operating system does not allow <ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            /> to bind to the port.</note></entry>
                                </row>
                                <row id="row-EnableMultithreading">
                                    <entry>Use Native Transports (epoll)</entry>
                                    <entry id="entry_EnableMultithreading">Specifies whether to use
                                        multiple receiver threads for each port. Using multiple
                                        receiver threads can improve performance. <p>You can use
                                            multiple receiver threads using epoll, which can be
                                            available when <ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            /> runs on recent versions of 64-bit Linux. </p></entry>
                                </row>
                                <row id="row-ReceiverThreads">
                                    <entry>Number of Receiver Threads</entry>
                                    <entry id="entry_ReceiverThreads"><ph
                                            id="Multithread-ReceiverThreads-ph">Number of receiver
                                            threads to use for each port. For example, if you
                                            configure two threads per port and configure the origin
                                            to use three ports, the origin uses a total of six
                                            threads.</ph><p id="entry-ReceiverThreads">Use to
                                            increase the number of threads passing data to the
                                            origin when epoll is available on the <ph
                                                conref="ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            /> machine.</p><p>Default is 1.</p></entry>
                                </row>
                                <row id="row-UDPdataFormat">
                                    <entry>Data Format</entry>
                                    <entry>Data format passed by UDP:<ul id="ul-UDPDataFormat">
                                            <li>collectd</li>
                                            <li>NetFlow</li>
                                            <li>syslog </li>
                                            <li>Raw/separated data </li>
                                        </ul></entry>
                                </row>
                                <row id="row-MaxBatchSize">
                                    <entry>Max Batch Size (messages)</entry>
                                    <entry>Maximum number of messages to include in a batch and pass
                                        through the pipeline at one time. Honors values up to the
                                            <ph
                                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                        /> maximum batch size. <p>Default is 1000. The <ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            /> default is 1000.</p></entry>
                                </row>
                                <row id="row-BatchWaitTime">
                                    <entry>Batch Wait Time (ms) <xref
                                            href="../Origins/BatchSizeWaitTime.dita#concept_ypd_vgr_5q">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_mgp_2q3_br" placement="inline"
                                        /></xref></entry>
                                    <entry>Milliseconds to wait before sending a partial or empty
                                        batch.</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>O-UDP-collectd</uicontrol> - used by
                        UDP Source and entries and ph used in Kafka Consumer.</draft-comment>
                </cmd>
            </step>
            <step id="O-UDP-collectd">
                <cmd>On the <uicontrol>collectd</uicontrol> tab, define the following collectd
                    properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3*"/>
                            <thead>
                                <row>
                                    <entry>collectd Property</entry>
                                    <entry>Properties</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry id="entry-TypesDBname">TypesDB File Path</entry>
                                    <entry><ph id="ph-TypesDBinfo">Path to a user-provided types.db
                                            file. Overrides the default types.db file.</ph>
                                    </entry>
                                </row>
                                <row>
                                    <entry id="entry-HiRestitle">Convert Hi-Res Time &amp;
                                        Interval</entry>
                                    <entry><ph id="ph-HiResinfo">Converts the collectd high
                                            resolution time format interval and timestamp to UNIX
                                            time, in milliseconds.</ph></entry>
                                </row>
                                <row>
                                    <entry id="entry-Exludename">Exclude Interval</entry>
                                    <entry><ph id="ph-Excludeinfo">Excludes the interval field from
                                            output record.</ph></entry>
                                </row>
                                <row>
                                    <entry id="entry-Authname">Auth File</entry>
                                    <entry><ph id="ph-Authinfo">Path to an optional authentication
                                            file. Use an authentication file to accept signed and
                                            encrypted data.</ph>
                                    </entry>
                                </row>
                                <row id="row-Charset">
                                    <entry>Charset</entry>
                                    <entry>Character set of the data.</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta">O-Netflow9only used by UDP Source and probably
                        UDP to Kafka? </draft-comment>
                </cmd>
            </step>
            <step id="step-Netflow9only">
                <cmd>For NetFlow 9 data, on the <uicontrol>NetFlow 9</uicontrol> tab, configure the
                    following properties:</cmd>
                <info id="info-Netflow9only">When processing earlier versions of NetFlow data, these
                    properties are ignored. <table frame="all" rowsep="1" colsep="1"
                        id="table_ug3_pv1_hbb">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Netflow 9 Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry><ph id="ph-NetFlowMode-prop">Record Generation Mode <xref
                                                href="../Data_Formats/NetFlow9_Records.dita">
                                                <image href="../Graphics/icon_moreInfo.png"
                                                  scale="10" placement="inline"
                                                  id="image_jrf_wqm_3bb"/></xref></ph></entry>
                                    <entry><ph id="ph-NetFlowMode-desc">Determines the type of
                                            values to include in the record. Select one of the
                                            following options:</ph><ul id="ph-NetFlowMode-ul">
                                            <li>Raw Only</li>
                                            <li>Interpreted Only</li>
                                            <li>Both Raw and Interpreted</li>
                                        </ul></entry>
                                </row>
                                <row>
                                    <entry><ph id="ph-Netflow-CacheSize-prop">Max Templates in
                                            Cache</ph></entry>
                                    <entry><ph id="ph-NetflowCache-desc1">The maximum number of
                                            templates to store in the template cache. For more
                                            information about templates, see <xref
                                                href="../Data_Formats/NetFlow_CachingTemplates.dita#concept_ivr_j1l_3bb"
                                            />.</ph><p><ph id="ph-NetflowCache-desc2">Default is -1
                                                for an unlimited cache size.</ph></p></entry>
                                </row>
                                <row>
                                    <entry><ph id="ph-NetFlow-CacheTimeout-prop">Template Cache
                                            Timeout (ms)</ph></entry>
                                    <entry><ph id="ph-Netflow-CacheTimeout-desc1">The maximum number
                                            of milliseconds to cache an idle template. Templates
                                            unused for more than the specified time are evicted from
                                            the cache. For more information about templates, see
                                                <xref
                                                href="../Data_Formats/NetFlow_CachingTemplates.dita#concept_ivr_j1l_3bb"
                                            />.</ph><p><ph id="ph-Netflow-CacheTimeout-desc2"
                                                >Default is -1 for caching templates
                                                indefinitely.</ph></p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table></info>
            </step>
            <step id="PROCESSORS">
                <cmd>
                    <draft-comment author="Loretta"
                        ><uicontrol>PROCESSORS</uicontrol></draft-comment>
                </cmd>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta">Use this step for processors that have error
                        handling - should be almost all. </draft-comment>
                </cmd>
            </step>
            <step id="1stStep-ReqField-ErrorHandling">
                <cmd>In the Properties panel, on the <wintitle>General</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_blh_n2h_hr">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>General Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Name</entry>
                                    <entry>Stage name.</entry>
                                </row>
                                <row>
                                    <entry>Description</entry>
                                    <entry>Optional description.</entry>
                                </row>
                                <row>
                                    <entry>Required Fields <xref
                                            href="../Pipeline_Design/RequiredFields.dita#concept_dnj_bkm_vq">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_h4p_p5v_yq"/></xref></entry>
                                    <entry>Fields that must include data for the record to be passed
                                        into the stage. <note outputclass="" type="tip">You might
                                            include fields that the stage uses.</note><p>Records
                                            that do not include all required fields are processed
                                            based on the error handling configured for the
                                            pipeline.</p></entry>
                                </row>
                                <row>
                                    <entry>Preconditions <xref
                                            href="../Pipeline_Design/Preconditions.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_f3b_khp_fs"/></xref></entry>
                                    <entry>Conditions that must evaluate to TRUE to allow a record
                                        to enter the stage for processing. Click
                                            <uicontrol>Add</uicontrol> to create additional
                                        preconditions. <p>Records that do not meet all preconditions
                                            are processed based on the error handling configured for
                                            the stage.</p></entry>
                                </row>
                                <row>
                                    <entry>On Record Error <xref
                                            href="../Pipeline_Design/ErrorHandling-Stage.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_w55_4yn_lw"/></xref></entry>
                                    <entry>Error record handling for the stage: <ul
                                            id="ul_swp_lfh_hr">
                                            <li>Discard - Discards the record.</li>
                                            <li>Send to Error - Sends the record to the pipeline for
                                                error handling.</li>
                                            <li>Stop Pipeline - Stops the pipeline. Not valid for
                                                cluster pipelines.</li>
                                        </ul></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="alisontaylor">Use this step for processors that include a
                        Stage Library field </draft-comment>
                </cmd>
            </step>
            <step id="1stStep-ReqField-StageLib">
                <cmd>In the Properties panel, on the <wintitle>General</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_blh_n5h_ur">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>General Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Name</entry>
                                    <entry>Stage name.</entry>
                                </row>
                                <row>
                                    <entry>Description</entry>
                                    <entry>Optional description.</entry>
                                </row>
                                <row>
                                    <entry>Stage Library</entry>
                                    <entry>Library version that you want to use. </entry>
                                </row>
                                <row>
                                    <entry>Required Fields <xref
                                            href="../Pipeline_Design/RequiredFields.dita#concept_dnj_bkm_vq">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_h2p_p1v_yq"/></xref></entry>
                                    <entry>Fields that must include data for the record to be passed
                                        into the stage. <note outputclass="" type="tip">You might
                                            include fields that the stage uses.</note><p>Records
                                            that do not include all required fields are processed
                                            based on the error handling configured for the
                                            pipeline.</p></entry>
                                </row>
                                <row>
                                    <entry>Preconditions <xref
                                            href="../Pipeline_Design/Preconditions.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_f6b_kjp_fs"/></xref></entry>
                                    <entry>Conditions that must evaluate to TRUE to allow a record
                                        to enter the stage for processing. Click
                                            <uicontrol>Add</uicontrol> to create additional
                                        preconditions. <p>Records that do not meet all preconditions
                                            are processed based on the error handling configured for
                                            the stage.</p></entry>
                                </row>
                                <row>
                                    <entry>On Record Error <xref
                                            href="../Pipeline_Design/ErrorHandling-Stage.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_w32_8yn_lw"/></xref></entry>
                                    <entry>Error record handling for the stage: <ul
                                            id="ul_swr_ofh_hr">
                                            <li>Discard - Discards the record.</li>
                                            <li>Send to Error - Sends the record to the pipeline for
                                                error handling.</li>
                                            <li>Stop Pipeline - Stops the pipeline. Not valid for
                                                cluster pipelines.</li>
                                        </ul></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta">Use the rows in this step for processors with
                        event generation and error handling - currently the three scripting
                        processors. Xref to the local Event Generation topic!</draft-comment>
                </cmd>
            </step>
            <step>
                <cmd>In the Properties panel, on the <wintitle>General</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_enz_31h_cy">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>General Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row id="P-row-1stS-Name">
                                    <entry>Name</entry>
                                    <entry>Stage name.</entry>
                                </row>
                                <row id="P-row-1stS-Desc">
                                    <entry>Description</entry>
                                    <entry>Optional description.</entry>
                                </row>
                                <row>
                                    <entry>Produce Events</entry>
                                    <entry id="P-entry-1stS-Events">Generates event records when
                                        events occur. Use for event handling. <xref
                                            href="../Event_Handling/EventFramework-Overview.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_plp_tp2_px"/></xref></entry>
                                </row>
                                <row id="P-row-1stS-ReqFields">
                                    <entry>Required Fields <xref
                                            href="../Pipeline_Design/RequiredFields.dita#concept_dnj_bkm_vq">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_fnz_31h_cy"/></xref></entry>
                                    <entry>Fields that must include data for the record to be passed
                                        into the stage. <note outputclass="" type="tip">You might
                                            include fields that the stage uses.</note><p>Records
                                            that do not include all required fields are processed
                                            based on the error handling configured for the
                                            pipeline.</p></entry>
                                </row>
                                <row id="P-row-1stS-PreCond">
                                    <entry>Preconditions <xref
                                            href="../Pipeline_Design/Preconditions.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_gnz_31h_cy"/></xref></entry>
                                    <entry>Conditions that must evaluate to TRUE to allow a record
                                        to enter the stage for processing. Click
                                            <uicontrol>Add</uicontrol> to create additional
                                        preconditions. <p>Records that do not meet all preconditions
                                            are processed based on the error handling configured for
                                            the stage.</p></entry>
                                </row>
                                <row id="P-row-1stS-Error">
                                    <entry>On Record Error <xref
                                            href="../Pipeline_Design/ErrorHandling-Stage.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_hnz_31h_cy"/></xref></entry>
                                    <entry>Error record handling for the stage: <ul
                                            id="ul_inz_31h_cy">
                                            <li>Discard - Discards the record.</li>
                                            <li>Send to Error - Sends the record to the pipeline for
                                                error handling.</li>
                                            <li>Stop Pipeline - Stops the pipeline. Not valid for
                                                cluster pipelines.</li>
                                        </ul></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta">Use the following step for processors without
                        record handling - Field Remover, Record Deduplicator.</draft-comment>
                </cmd>
            </step>
            <step id="1stStep-ReqField-noEHandling">
                <cmd>In the Properties panel, on the <wintitle>General</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_wjk_pfh_hr">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>General Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Name</entry>
                                    <entry>Stage name.</entry>
                                </row>
                                <row>
                                    <entry>Description</entry>
                                    <entry>Optional description.</entry>
                                </row>
                                <row>
                                    <entry>Required Fields <xref
                                            href="../Pipeline_Design/RequiredFields.dita#concept_dnj_bkm_vq">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_nkk_pfh_hr"/></xref></entry>
                                    <entry>Fields that must include data for the record to be passed
                                        into the stage. <note outputclass="" type="tip">You might
                                            include fields that the stage uses.</note><p>Records
                                            that do not include all required fields are processed
                                            based on the error handling configured for the
                                            pipeline.</p></entry>
                                </row>
                                <row>
                                    <entry>Preconditions <xref
                                            href="../Pipeline_Design/Preconditions.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_pbl_thp_fs"/></xref></entry>
                                    <entry>Conditions that must evaluate to TRUE to allow a record
                                        to enter the stage for processing. Click
                                            <uicontrol>Add</uicontrol> to create additional
                                        preconditions. <p>Records that do not meet all preconditions
                                            are processed based on the error handling configured for
                                            the stage.</p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step id="DEST_INFO">
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>DESTINATION -
                        General</uicontrol></draft-comment>
                </cmd>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>1stStep-NoStageLib</uicontrol> - Use
                        for destinations without stage libs, e.g. Local FS. </draft-comment>
                </cmd>
            </step>
            <step id="1stStep-NoStageLib">
                <cmd>In the Properties panel, on the <wintitle>General</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_fvw_4kg_j5">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>General Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Name</entry>
                                    <entry>Stage name.</entry>
                                </row>
                                <row>
                                    <entry>Description</entry>
                                    <entry>Optional description.</entry>
                                </row>
                                <row>
                                    <entry>Required Fields <xref
                                            href="../Pipeline_Design/RequiredFields.dita#concept_dnj_bkm_vq">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_lww_4kg_j5"/></xref></entry>
                                    <entry>Fields that must include data for the record to be passed
                                        into the stage. <note outputclass="" type="tip">You might
                                            include fields that the stage uses.</note><p>Records
                                            that do not include all required fields are processed
                                            based on the error handling configured for the
                                            pipeline.</p></entry>
                                </row>
                                <row>
                                    <entry>Preconditions <xref
                                            href="../Pipeline_Design/Preconditions.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_sww_4kg_j5"/></xref></entry>
                                    <entry>Conditions that must evaluate to TRUE to allow a record
                                        to enter the stage for processing. Click
                                            <uicontrol>Add</uicontrol> to create additional
                                        preconditions. <p>Records that do not meet all preconditions
                                            are processed based on the error handling configured for
                                            the stage.</p></entry>
                                </row>
                                <row>
                                    <entry>On Record Error <xref
                                            href="../Pipeline_Design/ErrorHandling-Stage.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_dpb_pyn_lw"/></xref></entry>
                                    <entry>Error record handling for the stage: <ul
                                            id="ul_fxw_4kg_j5">
                                            <li>Discard - Discards the record.</li>
                                            <li>Send to Error - Sends the record to the pipeline for
                                                error handling.</li>
                                            <li>Stop Pipeline - Stops the pipeline.</li>
                                        </ul></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"
                            ><uicontrol>1stStep-StageLib-ReqField-EHandling</uicontrol> - Use this
                        step for destinations with stage library, req fields, and error handling. –
                        Some rows are used by Hive Metastore. Also using these for executors that
                        don't have event handling.</draft-comment>
                </cmd>
            </step>
            <step id="1stStep-StageLib-ReqField-EHandling">
                <cmd>In the Properties panel, on the <wintitle>General</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_tvy_43h_hr">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>General Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row id="row-Name">
                                    <entry>Name</entry>
                                    <entry>Stage name.</entry>
                                </row>
                                <row id="row-Desc">
                                    <entry>Description</entry>
                                    <entry>Optional description.</entry>
                                </row>
                                <row id="row-StageLib">
                                    <entry>Stage Library</entry>
                                    <entry>Library version that you want to use. </entry>
                                </row>
                                <row>
                                    <entry>Required Fields <xref
                                            href="../Pipeline_Design/RequiredFields.dita#concept_dnj_bkm_vq">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_hwy_43h_hr"/></xref></entry>
                                    <entry>Fields that must include data for the record to be passed
                                        into the stage. <note outputclass="" type="tip">You might
                                            include fields that the stage uses.</note><p>Records
                                            that do not include all required fields are processed
                                            based on the error handling configured for the
                                            pipeline.</p></entry>
                                </row>
                                <row>
                                    <entry>Preconditions <xref
                                            href="../Pipeline_Design/Preconditions.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_rfz_thp_fs"/></xref></entry>
                                    <entry>Conditions that must evaluate to TRUE to allow a record
                                        to enter the stage for processing. Click
                                            <uicontrol>Add</uicontrol> to create additional
                                        preconditions. <p>Records that do not meet all preconditions
                                            are processed based on the error handling configured for
                                            the stage.</p></entry>
                                </row>
                                <row id="row-RecordError">
                                    <entry>On Record Error <xref
                                            href="../Pipeline_Design/ErrorHandling-Stage.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_lqj_pyn_lw"/></xref></entry>
                                    <entry>Error record handling for the stage: <ul
                                            id="ul_fqw_4fy_kt">
                                            <li>Discard - Discards the record.</li>
                                            <li>Send to Error - Sends the record to the pipeline for
                                                error handling.</li>
                                            <li>Stop Pipeline - Stops the pipeline.</li>
                                        </ul></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>1stStep-D-E-EventHandling</uicontrol>
                        - Used by MapR Jobs executor, HDFS Metadata executor, Hadoop FS
                        destination.</draft-comment>
                </cmd>
            </step>
            <step id="1stStep-D-E-EventHandling">
                <cmd>In the Properties panel, on the <wintitle>General</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_wsl_2rx_mx">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>General Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Name</entry>
                                    <entry>Stage name.</entry>
                                </row>
                                <row>
                                    <entry>Description</entry>
                                    <entry>Optional description.</entry>
                                </row>
                                <row>
                                    <entry>Stage Library</entry>
                                    <entry>Library version that you want to use. </entry>
                                </row>
                                <row>
                                    <entry>Produce Events <xref
                                            href="../Event_Handling/StageEvents.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_wrq_mrx_mx"/></xref></entry>
                                    <entry>Generates event records when events occur. Use for event
                                        handling. <xref
                                            href="../Event_Handling/EventFramework-Overview.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_l15_ft2_px"/></xref></entry>
                                </row>
                                <row>
                                    <entry>Required Fields <xref
                                            href="../Pipeline_Design/RequiredFields.dita#concept_dnj_bkm_vq">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_xsl_2rx_mx"/></xref></entry>
                                    <entry>Fields that must include data for the record to be passed
                                        into the stage. <note outputclass="" type="tip">You might
                                            include fields that the stage uses.</note><p>Records
                                            that do not include all required fields are processed
                                            based on the error handling configured for the
                                            pipeline.</p></entry>
                                </row>
                                <row>
                                    <entry>Preconditions <xref
                                            href="../Pipeline_Design/Preconditions.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_ysl_2rx_mx"/></xref></entry>
                                    <entry>Conditions that must evaluate to TRUE to allow a record
                                        to enter the stage for processing. Click
                                            <uicontrol>Add</uicontrol> to create additional
                                        preconditions. <p>Records that do not meet all preconditions
                                            are processed based on the error handling configured for
                                            the stage.</p></entry>
                                </row>
                                <row>
                                    <entry>On Record Error <xref
                                            href="../Pipeline_Design/ErrorHandling-Stage.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_zsl_2rx_mx"/></xref></entry>
                                    <entry>Error record handling for the stage: <ul
                                            id="ul_atl_2rx_mx">
                                            <li>Discard - Discards the record.</li>
                                            <li>Send to Error - Sends the record to the pipeline for
                                                error handling.</li>
                                            <li>Stop Pipeline - Stops the pipeline.</li>
                                        </ul></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>D-AVRO-Event</uicontrol> - Used in
                        Flume. Flume doesn't have other compression types, so it's not getting that
                        note. </draft-comment>
                </cmd>
            </step>
            <step id="D-AVRO-Event">
                <cmd>For Avro data, on the <wintitle>Data Format</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_o1j_7nd_9t">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Avro Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Avro Schema Location</entry>
                                    <entry>Location of the Avro schema definition to use when
                                        writing data:<ul id="ul_mr3_sdg_lx">
                                            <li>In Pipeline Configuration - Use the schema that you
                                                provide in the stage configuration. </li>
                                            <li>In Record Header - Use the schema in the avroSchema
                                                record header attribute. Use only when the
                                                avroSchema attribute is defined for all records.
                                                  <xref
                                                  href="../Pipeline_Design/RecordBasedWrites-overview.dita">
                                                  <image href="../Graphics/icon_moreInfo.png"
                                                  scale="10" id="image_alj_pws_nx"/></xref></li>
                                            <li>Confluent Schema Registry - Retrieve the schema from
                                                the Confluent Schema Registry. </li>
                                        </ul></entry>
                                </row>
                                <row>
                                    <entry>Avro Schema</entry>
                                    <entry>Avro schema definition used to write the data. <p>You can
                                            optionally use the runtime:loadResource function to use
                                            a schema definition stored in a runtime resource file.
                                        </p></entry>
                                </row>
                                <row>
                                    <entry>Register Schema</entry>
                                    <entry>Select to register a new Avro schema with the Confluent
                                        Schema Registry.</entry>
                                </row>
                                <row>
                                    <entry>Schema Registry URLs</entry>
                                    <entry>Confluent Schema Registry URLs used to look up the schema
                                        or to register a new schema. To add a URL, click
                                            <uicontrol>Add</uicontrol>. Use the following format to
                                        enter the
                                        URL:<codeblock>http://&lt;host name>:&lt;port number></codeblock></entry>
                                </row>
                                <row>
                                    <entry>Look Up Schema By</entry>
                                    <entry>Method used to look up the schema in the Confluent Schema
                                            Registry:<ul id="ul_t62_m3g_lx">
                                            <li>Subject - Look up the specified Avro schema
                                                subject.</li>
                                            <li>Schema ID - Look up the specified Avro schema ID.
                                            </li>
                                        </ul></entry>
                                </row>
                                <row>
                                    <entry>Schema Subject</entry>
                                    <entry>Avro schema subject to look up or to register in the
                                        Confluent Schema Registry.<p>If the specified subject to
                                            look up has multiple schema versions, the origin uses
                                            the latest schema version for that subject. To use an
                                            older version, find the corresponding schema ID, and
                                            then set the <uicontrol>Look Up Schema By</uicontrol>
                                            property to Schema ID.</p></entry>
                                </row>
                                <row>
                                    <entry>Schema ID</entry>
                                    <entry>Avro schema ID to look up in the Confluent Schema
                                        Registry.</entry>
                                </row>
                                <row>
                                    <entry>Include Schema</entry>
                                    <entry>Includes the schema in each event. <note>Omitting the
                                            schema definition can improve performance, but requires
                                            the appropriate schema management to avoid losing track
                                            of the schema associated with the data.</note></entry>
                                </row>
                                <row>
                                    <entry>Avro Compression Codec</entry>
                                    <entry>The Avro compression type to use. <p>When using Avro
                                            compression, do not enable other compression available
                                            in the destination. </p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>D-AVRO-File</uicontrol> - used in
                        Amazon S3 and other file-based destinations that write avro.</draft-comment>
                </cmd>
            </step>
            <step id="D-AVRO-File">
                <cmd>For Avro data, on the <wintitle>Data Format</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_o7j_3nd_9t">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Avro Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Avro Schema Location</entry>
                                    <entry>Location of the Avro schema definition to use when
                                        writing data:<ul id="ul_mr6_sdg_lx">
                                            <li>In Pipeline Configuration - Use the schema that you
                                                provide in the stage configuration. </li>
                                            <li>In Record Header - Use the schema in the avroSchema
                                                record header attribute. Use only when the
                                                avroSchema attribute is defined for all records.
                                                  <xref
                                                  href="../Pipeline_Design/RecordBasedWrites-overview.dita">
                                                  <image href="../Graphics/icon_moreInfo.png"
                                                  scale="10" id="image_yjw_4ws_nx"/></xref></li>
                                            <li>Confluent Schema Registry - Retrieve the schema from
                                                the Confluent Schema Registry. </li>
                                        </ul><p>The destination includes the schema definition in
                                            each generated file. </p></entry>
                                </row>
                                <row>
                                    <entry>Avro Schema</entry>
                                    <entry>Avro schema definition used to write the data. <p>You can
                                            optionally use the runtime:loadResource function to use
                                            a schema definition stored in a runtime resource file.
                                        </p></entry>
                                </row>
                                <row>
                                    <entry>Register Schema</entry>
                                    <entry>Select to register a new Avro schema with the Confluent
                                        Schema Registry.</entry>
                                </row>
                                <row>
                                    <entry>Schema Registry URLs</entry>
                                    <entry>Confluent Schema Registry URLs used to look up the schema
                                        or to register a new schema. To add a URL, click
                                            <uicontrol>Add</uicontrol>. Use the following format to
                                        enter the
                                        URL:<codeblock>http://&lt;host name>:&lt;port number></codeblock></entry>
                                </row>
                                <row>
                                    <entry>Look Up Schema By</entry>
                                    <entry>Method used to look up the schema in the Confluent Schema
                                            Registry:<ul id="ul_t38_m6g_lx">
                                            <li>Subject - Look up the specified Avro schema
                                                subject.</li>
                                            <li>Schema ID - Look up the specified Avro schema ID.
                                            </li>
                                        </ul></entry>
                                </row>
                                <row>
                                    <entry>Schema Subject</entry>
                                    <entry>Avro schema subject to look up or to register in the
                                        Confluent Schema Registry.<p>If the specified subject to
                                            look up has multiple schema versions, the origin uses
                                            the latest schema version for that subject. To use an
                                            older version, find the corresponding schema ID, and
                                            then set the <uicontrol>Look Up Schema By</uicontrol>
                                            property to Schema ID.</p></entry>
                                </row>
                                <row>
                                    <entry>Schema ID</entry>
                                    <entry>Avro schema ID to look up in the Confluent Schema
                                        Registry.</entry>
                                </row>
                                <row>
                                    <entry>Avro Compression Codec</entry>
                                    <entry>The Avro compression type to use. <p>When using Avro
                                            compression, do not enable other compression available
                                            in the destination. </p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>D-AVRO-Mess</uicontrol> - Used in all
                        message destinations except for Kafka Producer - ie, Kinesis Producer, MapR
                        Streams, Rabbit MQ Producer.</draft-comment>
                </cmd>
            </step>
            <step id="D-AVRO-Mess">
                <cmd>For Avro data, on the <wintitle>Data Format</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_o2j_4nd_3t">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Avro Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Avro Schema Location</entry>
                                    <entry>Location of the Avro schema definition to use when
                                        writing data:<ul id="ul_mr5_sdg_lx">
                                            <li>In Pipeline Configuration - Use the schema that you
                                                provide in the stage configuration. </li>
                                            <li>In Record Header - Use the schema in the avroSchema
                                                record header attribute. Use only when the
                                                avroSchema attribute is defined for all records.
                                                  <xref
                                                  href="../Pipeline_Design/RecordBasedWrites-overview.dita">
                                                  <image href="../Graphics/icon_moreInfo.png"
                                                  scale="10" id="image_dsv_nws_nx"/></xref></li>
                                            <li>Confluent Schema Registry - Retrieve the schema from
                                                the Confluent Schema Registry. </li>
                                        </ul></entry>
                                </row>
                                <row>
                                    <entry>Avro Schema</entry>
                                    <entry>Avro schema definition used to write the data. <p>You can
                                            optionally use the runtime:loadResource function to use
                                            a schema definition stored in a runtime resource file.
                                        </p></entry>
                                </row>
                                <row>
                                    <entry>Register Schema</entry>
                                    <entry>Select to register a new Avro schema with the Confluent
                                        Schema Registry.</entry>
                                </row>
                                <row>
                                    <entry>Schema Registry URLs</entry>
                                    <entry>Confluent Schema Registry URLs used to look up the schema
                                        or to register a new schema. To add a URL, click
                                            <uicontrol>Add</uicontrol>. Use the following format to
                                        enter the
                                        URL:<codeblock>http://&lt;host name>:&lt;port number></codeblock></entry>
                                </row>
                                <row>
                                    <entry>Look Up Schema By</entry>
                                    <entry>Method used to look up the schema in the Confluent Schema
                                            Registry:<ul id="ul_t54_m2g_lx">
                                            <li>Subject - Look up the specified Avro schema
                                                subject.</li>
                                            <li>Schema ID - Look up the specified Avro schema ID.
                                            </li>
                                        </ul></entry>
                                </row>
                                <row>
                                    <entry>Schema Subject</entry>
                                    <entry>Avro schema subject to look up or to register in the
                                        Confluent Schema Registry.<p>If the specified subject to
                                            look up has multiple schema versions, the origin uses
                                            the latest schema version for that subject. To use an
                                            older version, find the corresponding schema ID, and
                                            then set the <uicontrol>Look Up Schema By</uicontrol>
                                            property to Schema ID.</p></entry>
                                </row>
                                <row>
                                    <entry>Schema ID</entry>
                                    <entry>Avro schema ID to look up in the Confluent Schema
                                        Registry.</entry>
                                </row>
                                <row>
                                    <entry>Include Schema</entry>
                                    <entry>Includes the schema in each message. <note>Omitting the
                                            schema definition can improve performance, but requires
                                            the appropriate schema management to avoid losing track
                                            of the schema associated with the data.</note></entry>
                                </row>
                                <row>
                                    <entry>Avro Compression Codec</entry>
                                    <entry>The Avro compression type to use. <p>When using Avro
                                            compression, do not enable other compression available
                                            in the destination. </p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="alisontaylor"><uicontrol>D-AVRO-KAFKA</uicontrol> used in
                        Kafka Producer</draft-comment>
                </cmd>
            </step>
            <step id="D-AVRO-KAFKA">
                <cmd>For Avro data, on the <wintitle>Data Format</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_o9j_2nd_7t">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Avro Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Avro Schema Location</entry>
                                    <entry>Location of the Avro schema definition to use when
                                        writing data:<ul id="ul_mr9_sdg_lx">
                                            <li>In Pipeline Configuration - Use the schema that you
                                                provide in the stage configuration. </li>
                                            <li>In Record Header - Use the schema in the avroSchema
                                                record header attribute. Use only when the
                                                avroSchema attribute is defined for all records.
                                                  <xref
                                                  href="../Pipeline_Design/RecordBasedWrites-overview.dita">
                                                  <image href="../Graphics/icon_moreInfo.png"
                                                  scale="10" id="image_fvd_sis_nw"/></xref></li>
                                            <li>Confluent Schema Registry - Retrieve the schema from
                                                the Confluent Schema Registry. </li>
                                        </ul></entry>
                                </row>
                                <row>
                                    <entry>Avro Schema</entry>
                                    <entry>Avro schema definition used to write the data. <p>You can
                                            optionally use the runtime:loadResource function to use
                                            a schema definition stored in a runtime resource file.
                                        </p></entry>
                                </row>
                                <row>
                                    <entry>Register Schema</entry>
                                    <entry>Select to register a new Avro schema with the Confluent
                                        Schema Registry.</entry>
                                </row>
                                <row>
                                    <entry>Schema Registry URLs </entry>
                                    <entry>Confluent Schema Registry URLs used to look up the schema
                                        or to register a new schema. To add a URL, click
                                            <uicontrol>Add</uicontrol>. Use the following format to
                                        enter the
                                        URL:<codeblock>http://&lt;host name>:&lt;port number></codeblock></entry>
                                </row>
                                <row>
                                    <entry>Look Up Schema By</entry>
                                    <entry>Method used to look up the schema in the Confluent Schema
                                            Registry:<ul id="ul_t93_m5g_lx">
                                            <li>Subject - Look up the specified Avro schema
                                                subject.</li>
                                            <li>Schema ID - Look up the specified Avro schema ID.
                                            </li>
                                        </ul></entry>
                                </row>
                                <row>
                                    <entry>Schema Subject</entry>
                                    <entry>Avro schema subject to look up or to register in the
                                        Confluent Schema Registry.<p>If the specified subject to
                                            look up has multiple schema versions, the origin uses
                                            the latest schema version for that subject. To use an
                                            older version, find the corresponding schema ID, and
                                            then set the <uicontrol>Look Up Schema By</uicontrol>
                                            property to Schema ID.</p></entry>
                                </row>
                                <row>
                                    <entry>Schema ID</entry>
                                    <entry>Avro schema ID to look up in the Confluent Schema
                                        Registry.</entry>
                                </row>
                                <row>
                                    <entry>Include Schema</entry>
                                    <entry>Includes the schema in each message. <note>If you
                                            configured Kafka Producer to embed the Avro schema ID in
                                            each message that it writes, clear this
                                        property.</note></entry>
                                </row>
                                <row>
                                    <entry>Avro Compression Codec</entry>
                                    <entry>The Avro compression type to use. <p>When using Avro
                                            compression, do not enable other compression available
                                            in the destination. </p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>D-Binary</uicontrol> - Used by Kafka
                        Consumer and Amazon S3 dest.</draft-comment>
                </cmd>
            </step>
            <step id="D-Binary">
                <cmd>For binary data, on the <wintitle>Data Format</wintitle> tab, configure the
                    following property:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_xct_mbm_gt">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Binary Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Binary Field Path</entry>
                                    <entry>Field that contains the binary data.</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>DelimProps:</uicontrol> Use for the
                        appropriate delimited destinations - currently Amazon S3, Flume, Hadoop FS,
                        Kafka Producer, and Kinesis Firehose:</draft-comment>
                </cmd>
            </step>
            <step id="DelimProps">
                <cmd>For delimited data, on the <wintitle>Data Format</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_wb3_2kg_br">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Delimited Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Delimiter Format</entry>
                                    <entry>Format for delimited data:<ul
                                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/ul_delFileTypes"
                                            id="ul_k3j_vvf_jr">
                                            <li/>
                                        </ul></entry>
                                </row>
                                <row>
                                    <entry>Header Line</entry>
                                    <entry>Indicates whether to create a header line.</entry>
                                </row>
                                <row>
                                    <entry>Replace New Line Characters</entry>
                                    <entry>Replaces new line characters with the configured
                                            string.<p>Recommended when writing data as a single line
                                            of text.</p></entry>
                                </row>
                                <row>
                                    <entry>New Line Character Replacement</entry>
                                    <entry>String to replace each new line character. For example,
                                        enter a space to replace each new line character with a
                                        space. <p>Leave empty to remove the new line
                                        characters.</p></entry>
                                </row>
                                <row>
                                    <entry>Delimiter Character</entry>
                                    <entry>Delimiter character for a custom delimiter format. Select
                                        one of the available options or use Other to enter a custom
                                        character. <p>You can enter a Unicode control character
                                            using the format \u<i>NNNN</i>, where ​<i>N</i> is a
                                            hexadecimal digit from the numbers 0-9 or the letters
                                            A-F. For example, enter \u0000 to use the null character
                                            as the delimiter or \u2028 to use a line separator as
                                            the delimiter.</p><p>Default is the pipe character ( |
                                            ).</p></entry>
                                </row>
                                <row>
                                    <entry>Escape Character </entry>
                                    <entry>Escape character for a custom delimiter format. Select
                                        one of the available options or use Other to enter a custom
                                        character. <p>Default is the backslash character ( \
                                        ).</p></entry>
                                </row>
                                <row>
                                    <entry>Quote Character</entry>
                                    <entry>Quote character for a custom delimiter format. Select one
                                        of the available options or use Other to enter a custom
                                        character. <p>Default is the quotation mark character ( "
                                            ).</p></entry>
                                </row>
                                <row conref="ReusableTables.dita#concept_wfr_rnw_yq/D-CHARSET-other">
                                    <entry/>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>JSONProps</uicontrol> - Used for
                        Hadoop FS, Kafka Producer, Amazon S3, Kinesis Producer, and Kinesis
                        Firehose.</draft-comment>
                </cmd>
            </step>
            <step id="JSONProps">
                <cmd>For JSON data, on the <wintitle>Data Format</wintitle> tab, configure the
                    following property:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_lgq_53c_wr">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>JSON Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>JSON Content</entry>
                                    <entry>Determines how JSON data is written:<ul
                                            id="ul_mss_w3c_wr">
                                            <li>JSON Array of Objects - Each file includes a single
                                                array. In the array, each element is a JSON
                                                representation of each record.</li>
                                            <li>Multiple JSON Objects - Each file includes multiple
                                                JSON objects. Each object is a JSON representation
                                                of a record.</li>
                                        </ul></entry>
                                </row>
                                <row conref="ReusableTables.dita#concept_wfr_rnw_yq/D-CHARSET-other">
                                    <entry/>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><b>D-PROTO-props</b> - used by all protobuf
                        destinations EXCEPT for Google Pub/Sub Publisher which as one additional
                        property....that table is below.<p>NOTE: when making changes, make sure to
                            make any related ones to BOTH origin tables.</p></draft-comment>
                </cmd>
            </step>
            <step id="D-PROTO-props">
                <cmd>For protobuf data, on the <wintitle>Data Format</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_vmt_tdp_45">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Protobuf Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Protobuf Descriptor File </entry>
                                    <entry>Descriptor file (.desc) to use. The descriptor file must
                                        be in the <ph
                                            conref="ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                        /> resources directory, <codeph>$SDC_RESOURCES</codeph>.
                                                <p><ph
                                                conref="ReusablePhrases.dita#concept_vhs_5tz_xp/SDCDPM_SDCenvConfigs"
                                            /> For information about generating the descriptor file,
                                            see <xref
                                                href="../Data_Formats/Protobuf-Prerequisites.dita"
                                            />.</p></entry>
                                </row>
                                <row>
                                    <entry>Message Type</entry>
                                    <entry>The fully-qualified name for the message type to use when
                                        writing data.<p>Use the following format:
                                                <codeph>&lt;package name>.&lt;message
                                            type></codeph>. </p>Use a message type defined in the
                                        descriptor file.</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="alisontaylor"><b>D-PROTO-props-PubSub</b> used by the
                        Google Pub/Sub Publisher destination only, it has one additional
                        property</draft-comment>
                </cmd>
            </step>
            <step id="D-PROTO-props-PubSub">
                <cmd>For protobuf data, on the <wintitle>Data Format</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_vmt_jds_38">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Protobuf Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Protobuf Descriptor File </entry>
                                    <entry>Descriptor file (.desc) to use. The descriptor file must
                                        be in the <ph
                                            conref="ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                        /> resources directory, <codeph>$SDC_RESOURCES</codeph>.
                                                <p><ph
                                                conref="ReusablePhrases.dita#concept_vhs_5tz_xp/SDCDPM_SDCenvConfigs"
                                            /> For information about generating the descriptor file,
                                            see <xref
                                                href="../Data_Formats/Protobuf-Prerequisites.dita"
                                            />.</p></entry>
                                </row>
                                <row>
                                    <entry>Message Type</entry>
                                    <entry>The fully-qualified name for the message type to use when
                                        writing data.<p>Use the following format:
                                                <codeph>&lt;package name>.&lt;message
                                            type></codeph>. </p>Use a message type defined in the
                                        descriptor file.</entry>
                                </row>
                                <row>
                                    <entry>Write Delimiter</entry>
                                    <entry>Writes a delimiter after each message. </entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>TextProps</uicontrol> - Using for
                        Hadoop FS, Kafka Producer, and Amazon S3.</draft-comment>
                </cmd>
            </step>
            <step id="TextProps">
                <cmd>For text data, on the <wintitle>Data Format</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_egv_3df_jr">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Text Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Text Field Path</entry>
                                    <entry>Field that contains the text data to be written. All data
                                        must be incorporated into the specified field. </entry>
                                </row>
                                <row>
                                    <entry>Record Separator</entry>
                                    <entry>Characters to use to separate records. Use any valid Java
                                        string literal. For example, when writing to Windows, you
                                        might use \r\n to separate records. <p>By default, the
                                            destination uses \n.</p></entry>
                                </row>
                                <row>
                                    <entry>Insert Record Separator if No Text</entry>
                                    <entry>When a record does not include the text field, inserts
                                        the configured record separator string to create an empty
                                            line.<p>When not selected, records without the text
                                            field are discarded.</p></entry>
                                </row>
                                <row conref="ReusableTables.dita#concept_wfr_rnw_yq/D-CHARSET-other">
                                    <entry/>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd/>
                <info>
                    <draft-comment author="Loretta"><b>D-WHOLEFile-props</b> - Whole table used by
                        Hadoop FS, Local FS, and MapR FS. Rows used by S3 and Azure Data Lake
                        Store</draft-comment>
                </info>
            </step>
            <step id="D-WholeFile_props">
                <cmd>For whole files, on the <wintitle>Data Format</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_amd_xml_zw">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Whole File Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row id="row-FileNameExp">
                                    <entry>File Name Expression</entry>
                                    <entry>
                                        <p>Expression to use for the file names. </p>
                                        <p>For tips on how to name files based on input file names,
                                            see <xref
                                                href="../Data_Formats/WholeFile-Writing.dita#concept_a2s_4jw_1x"
                                            />.</p>
                                    </entry>
                                </row>
                                <row id="row-PermExp">
                                    <entry>Permissions Expression <xref
                                            href="../Data_Formats/WholeFile-AccessPermissions.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_zwb_ccw_1x"/></xref></entry>
                                    <entry>Expression that defines the access permissions for output
                                        files. Expressions should evaluate to a symbolic or
                                        numeric/octal representation of the permissions you want to
                                        use. <p>By default, with no specified expression, files use
                                            the default permissions of the destination system.
                                            </p><p>To use the original source file access
                                            permissions, use the following expression:
                                            <codeblock>${record:value('/fileInfo/permissions')}</codeblock></p></entry>
                                </row>
                                <row id="row-FileExists">
                                    <entry>File Exists</entry>
                                    <entry>Action to take when a file of the same name already
                                        exists in the output directory. Use one of the following
                                            options:<ul id="ul_a4s_l5l_zw">
                                            <li>Send to Error - Handles the record based on stage
                                                error record handling. </li>
                                            <li>Overwrite - Overwrites the existing file.</li>
                                        </ul></entry>
                                </row>
                                <row id="row-IncludeChecksum">
                                    <entry>Include Checksum in Events <xref
                                            href="../Data_Formats/WholeFile-IncludingChecksumEvent.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_fyz_ywp_vx"/></xref></entry>
                                    <entry>Includes checksum information in whole file event
                                        records. <p>Use only when the destination generates event
                                            records. </p></entry>
                                </row>
                                <row id="row-ChecksumAlgo">
                                    <entry>Checksum Algorithm</entry>
                                    <entry>Algorithm to generate the checksum.</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta">The following step is used by message-based
                        destinations that write XML data - JMS Producer and Kafka Producer for
                        now:</draft-comment>
                </cmd>
            </step>
            <step id="step-WriteXML-messages">
                <cmd>For XML data, on the <wintitle>XML</wintitle> tab, configure the following
                    properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_sst_c5l_r1b">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>XML Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Pretty Format</entry>
                                    <entry>Adds indentation to make the resulting XML document
                                        easier to read. Increases the record size
                                        accordingly.</entry>
                                </row>
                                <row>
                                    <entry>Validate Schema</entry>
                                    <entry>Validates <ph
                                            conref="ReusablePhrases.dita#concept_vhs_5tz_xp/ph-WriteXML-ValidateSchema"/><p>
                                            <note type="important">Regardless of whether you
                                                validate the XML schema, the destination requires
                                                the record in a specific format. For more
                                                information, see <xref
                                                  href="../Data_Formats/WritingXML-Requirement.dita#concept_cmn_hml_r1b"
                                                />.</note>
                                        </p></entry>
                                </row>
                                <row>
                                    <entry>XML Schema</entry>
                                    <entry>The XML schema to use to validate records.</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>DESTINATION -
                        Specific</uicontrol></draft-comment>
                </cmd>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>FS-OutputFiles</uicontrol> - Hadoop
                        FS, Local FS and MapR FS conref rows/partial rows in the table. Data Lake
                        Store also steals rows and entries from here. </draft-comment>
                </cmd>
            </step>
            <step id="FS-OutputFiles">
                <cmd>On the <wintitle>Output Files</wintitle> tab, configure the following
                    options:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_lpp_dd1_s5">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Output Files Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Idle Timeout (secs)</entry>
                                    <entry id="entry-IdleTimeout">Maximum time that an output file
                                        can remain idle. After no records are written to a file for
                                        this amount of time, the destination closes the file. Enter
                                        a time in seconds or use the <codeph>MINUTES</codeph> or
                                            <codeph>HOURS</codeph> constant in an expression to
                                        define the time increment.<p>Use -1 to set no limit. Default
                                            is 1 hour, defined as follows: <codeph>${1 *
                                                HOURS}</codeph>. </p><p>Not available when using the
                                            whole file data format. </p></entry>
                                </row>
                                <row id="row-FileType">
                                    <entry>File Type</entry>
                                    <entry>Output file type:<ul id="ul_pzp_dd1_s5">
                                            <li>Text files</li>
                                            <li>Sequence files</li>
                                            <li>Whole files - Select when using the whole file data
                                                format.</li>
                                        </ul></entry>
                                </row>
                                <row id="row-DataFormat">
                                    <entry>Data Format</entry>
                                    <entry id="entry-HDFSdataformat">Format of data to be written.
                                        Use one of the following options:<ul id="ul_vzp_dd1_s5">
                                            <li>Avro</li>
                                            <li>Binary</li>
                                            <li>Delimited</li>
                                            <li>JSON</li>
                                            <li>Protobuf</li>
                                            <li>Text</li>
                                            <li>Whole File <xref
                                                  href="../Data_Formats/WholeFile.dita">
                                                  <image href="../Graphics/icon_moreInfo.png"
                                                  scale="10" placement="inline"
                                                  id="image_ylz_vsm_zw"/></xref></li>
                                        </ul></entry>
                                </row>
                                <row id="row-FilePrefix">
                                    <entry>File Prefix</entry>
                                    <entry>Prefix to use for output files. Use when writing to a
                                        directory that receives files from other sources.<p>Uses the
                                            prefix sdc-${sdc:id()} by default. The prefix evaluates
                                            to sdc-&lt;<ph
                                                conref="ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            /> ID>. </p><p>The <ph
                                                conref="ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            /> ID is stored in the following file:
                                                <filepath>$SDC_DATA/sdc.id</filepath>. <ph
                                                conref="ReusablePhrases.dita#concept_vhs_5tz_xp/SDCDPM_SDCenvConfigs"
                                            /></p></entry>
                                </row>
                                <row id="row-FileSuffix">
                                    <entry>File Suffix</entry>
                                    <entry>Suffix to use for output files, such as txt or json. When
                                        used, the destination adds a period and the configured
                                        suffix as follows: &lt;filename>.&lt;suffix>.<p>You can
                                            include periods within the suffix, but do not start the
                                            suffix with a period. Forward slashes are not
                                            allowed.</p><p>Not available for the whole file data
                                            format. </p></entry>
                                </row>
                                <row id="row-DirectoryinHeader">
                                    <entry>Directory in Header <xref
                                            href="../Pipeline_Design/RecordBasedWrites-overview.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_ydt_vvg_1w"/></xref>
                                    </entry>
                                    <entry>Indicates that the target directory is defined in record
                                        headers. Use only when the targetDirectory header attribute
                                        is defined for all records. </entry>
                                </row>
                                <row id="row-DirectoryTemplate">
                                    <entry>Directory Template <xref
                                            href="../Destinations/HadoopFS-DirectoryTemplates.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_c1q_dd1_s5"/></xref></entry>
                                    <entry id="entry-DirectoryTemplate">Template for creating output
                                        directories. You can use constants, field values, and
                                        datetime variables. <p>Output directories are created based
                                            on the smallest datetime variable in the
                                        template.</p></entry>
                                </row>
                                <row id="row-DataTimeZone">
                                    <entry>Data Time Zone</entry>
                                    <entry>Time zone for the destination system. Used to resolve
                                        datetimes in the directory template and evaluate where
                                        records are written.</entry>
                                </row>
                                <row id="row-TimeBasis">
                                    <entry>Time Basis <xref
                                            href="../Destinations/HadoopFS-TimeBasis.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_f1q_dd1_s5"/></xref></entry>
                                    <entry id="entry-TimeBasis">Time basis to use for creating
                                        output directories and writing records to the directories.
                                        Use one of the following expressions:<ul id="ul_h1q_dd1_s5">
                                            <li>${time:now()} - Uses the processing time as the time
                                                basis. </li>
                                            <li>${record:value(&lt;date field path>)} - Uses the
                                                time associated with the record as the time
                                                basis.</li>
                                        </ul></entry>
                                </row>
                                <row id="row-MaxRecords">
                                    <entry>Max Records in a File</entry>
                                    <entry>Maximum number of records to be written to an output
                                        file. Additional records are written to a new file. <p>Use 0
                                            to opt out of this property. </p><p>Not available when
                                            using the whole file data format. </p></entry>
                                </row>
                                <row id="row-MaxFileSize">
                                    <entry>Max File Size (MB)</entry>
                                    <entry>Maximum size of an output file. Additional records are
                                        written to a new file. <p>Use 0 to opt out of this property.
                                            </p><p>Not available when using the whole file data
                                            format. </p></entry>
                                </row>
                                <row id="row-CCodec">
                                    <entry>Compression Codec</entry>
                                    <entry>Compression type for output files:<ul id="ul_o1q_dd1_s5">
                                            <li>None </li>
                                            <li>gzip</li>
                                            <li>bzip2</li>
                                            <li>Snappy</li>
                                            <li>LZ4</li>
                                            <li>Other</li>
                                        </ul><p>
                                            <note>Do not use with Avro data. To compress Avro data,
                                                use the Avro Compression Codec property on the Data
                                                Formats tab.</note>
                                        </p></entry>
                                </row>
                                <row id="row-CCodecClass">
                                    <entry>Compression Codec Class</entry>
                                    <entry>Full class name of the other compression codec that you
                                        want to use. </entry>
                                </row>
                                <row id="row-SequenceFileKey">
                                    <entry>Sequence File Key</entry>
                                    <entry>Record key for creating sequence files. Use one of the
                                        following options:<ul id="ul_v1q_dd1_s5">
                                            <li>${record:value(&lt;field path>)}</li>
                                            <li>${uuid()}</li>
                                        </ul></entry>
                                </row>
                                <row id="row-CompressionType">
                                    <entry>Compression Type</entry>
                                    <entry>Compression type for sequence files when using a
                                        compression codec:<ul id="ul_bbq_dd1_s5">
                                            <li>Block Compression</li>
                                            <li>Record Compression</li>
                                        </ul></entry>
                                </row>
                                <row id="row-RollAttri">
                                    <entry>Use Roll Attribute <xref
                                            href="../Pipeline_Design/RecordBasedWrites-overview.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                            /></xref>
                                    </entry>
                                    <entry>Checks the record header for the roll header attribute
                                        and closes the current file when the roll attribute exists.
                                            <p>Can be used with Max Records in a File and Max File
                                            Size to close files.</p></entry>
                                </row>
                                <row id="row-RollAttrName">
                                    <entry>Roll Attribute Name</entry>
                                    <entry>Name of the roll header attribute.<p>Default is
                                        roll.</p></entry>
                                </row>
                                <row id="row-ValidateHDFSPerms">
                                    <entry>Validate HDFS Permissions</entry>
                                    <entry id="D-entry-ValidatePermissions">When you start the
                                        pipeline, the destination tries writing to the configured
                                        directory template to validate permissions. The pipeline
                                        does not start if validation fails.<note>Do not use this
                                            option when the directory template uses expressions to
                                            represent the entire directory.</note></entry>
                                </row>
                                <row>
                                    <entry>Skip File Recovery</entry>
                                    <entry id="entry-SkipRecovery">Determines whether the
                                        destination performs file recovery after an unexpected stop
                                        of the pipeline. </entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta">The following step is used by the Event Hub
                        destination, rows by the origin.</draft-comment>
                </cmd>
            </step>
            <step id="AEhub-EventHubConfig">
                <cmd>On the <wintitle>Event Hub</wintitle> tab, configure the following
                    properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_q3q_g2f_bbb">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Event Hub Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row id="AEhub-Namespace">
                                    <entry>Namespace Name</entry>
                                    <entry>The name of the namespace that contains the event hub
                                        that you want to use.</entry>
                                </row>
                                <row id="AEhub-EventHubName">
                                    <entry>Event Hub Name</entry>
                                    <entry>The event hub name.</entry>
                                </row>
                                <row id="AEhub-KeyName">
                                    <entry>Shared Access Policy Name</entry>
                                    <entry>The policy name associated with the namespace. <p>To
                                            retrieve the policy name, when logged into the Azure
                                            portal, navigate to your namespace and event hub, and
                                            then click Shared Access Policies for a list of
                                            policies. </p><p>When appropriate, you can use the
                                            default shared access key policy,
                                            RootManageSharedAccessKey.</p></entry>
                                </row>
                                <row id="AEhub-Key">
                                    <entry>Connection String Key </entry>
                                    <entry>One of the connection string keys associated with the
                                        specified shared access policy. <p>To retrieve a connection
                                            string key, after accessing the list of shared access
                                            policies, click the policy name, and then copy the
                                            Connection String - Primary Key value. </p><p>The value
                                            typically begins with "Endpoint".</p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
                <info>For more information about Microsoft Azure Event Hub, see the <xref
                        href="https://docs.microsoft.com/en-us/azure/event-hubs/" format="html"
                        scope="external">Event Hub documentation</xref>.</info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta">Elasticsearch properties - all used by
                        Elasticsearch destination. Most used by Elasticsearch origin. All but
                        Default Operation and Unsupported Operation Handling used by Configuring a
                        Pipeline > Error handling. Default Operation and Unsupported Operation
                        Handling descriptions used by Kudu, JDBC Producer and JDBC
                        Tee</draft-comment>
                </cmd>
            </step>
            <step id="ELASTICprops-Step">
                <cmd>On the <wintitle>Elasticsearch</wintitle> tab, configure the following
                    properties:</cmd>
                <info id="ElasticProps-Info">
                    <table frame="all" rowsep="1" colsep="1" id="table_ht4_x5v_4r">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Elasticsearch Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row id="row_ElasticCluster">
                                    <entry>Cluster HTTP URI</entry>
                                    <entry>HTTP URI used to connect to the cluster. Use the
                                        following format:
                                        <codeblock>&lt;host>:&lt;port></codeblock></entry>
                                </row>
                                <row id="row_ElasticHTTP">
                                    <entry>Additional HTTP Params</entry>
                                    <entry>Additional HTTP parameters that you want to send as query
                                        string parameters to Elasticsearch. Enter the exact
                                        parameter name and value expected by Elasticsearch.</entry>
                                </row>
                                <row id="row_AdditionalNodes">
                                    <entry>Detect Additional Nodes in Cluster</entry>
                                    <entry>
                                        <p>Detects additional nodes in the cluster based on the
                                            configured Cluster URI. </p>
                                        <p>Selecting this property is the equivalent to setting the
                                            client.transport.sniff Elasticsearch property to true. </p>
                                        <p>Use only when the Data Collector shares the same network
                                            as the Elasticsearch cluster. Do not use for Elastic
                                            Cloud or Docker clusters. </p>
                                    </entry>
                                </row>
                                <row id="row_ElasticSecurity">
                                    <entry>Use Security</entry>
                                    <entry>Specifies whether security is enabled on the
                                        Elasticsearch cluster. </entry>
                                </row>
                                <row id="row_ElasticTime">
                                    <entry>Time Basis <xref
                                            href="../Destinations/Elastic-TimeBasis.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_w4w_q3p_ht"/>
                                        </xref></entry>
                                    <entry>
                                        <p>Time basis to use for writing to time-based indexes. Use
                                            one of the following expressions:<ul id="ul_wbn_qdt_r5">
                                                <li><codeph>${time:now()}</codeph> - Uses the
                                                  processing time as the time basis. The processing
                                                  time is the time associated with the <ph
                                                  conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                                  /> running the pipeline. </li>
                                                <li>An expression that calls a field and resolves to
                                                  a datetime value, such as
                                                  <codeph>${record:value(&lt;date field
                                                  path>)}</codeph>. Uses the datetime result as the
                                                  time basis. </li>
                                            </ul></p>
                                        <p>When the Index property does not include datetime
                                            variables, you can ignore this property. </p>
                                        <p>Default is <codeph>${time:now()}</codeph>.</p>
                                    </entry>
                                </row>
                                <row id="row_ElasticZone">
                                    <entry>Data Time Zone</entry>
                                    <entry>Time zone for the destination system. Used to resolve
                                        datetimes in time-based indexes. </entry>
                                </row>
                                <row id="row_ElasticIndex">
                                    <entry>Index</entry>
                                    <entry>Index for the generated documents. Enter an index name or
                                        an expression that evaluates to the index name. <p>For
                                            example, if you enter <codeph>customer</codeph> as the
                                            index, the destination writes the document within the
                                                <codeph>customer</codeph> index. </p><p>If you use
                                            datetime variables in the expression, make sure to
                                            configure the time basis appropriately. For details
                                            about datetime variables, see <xref
                                                href="../Expression_Language/DateTimeVariables.dita#concept_gh4_qd2_sv"
                                            />.</p></entry>
                                </row>
                                <row id="row_ElasticMapping">
                                    <entry>Mapping</entry>
                                    <entry>Mapping type for the generated documents. Enter the
                                        mapping type, an expression that evaluates to the mapping
                                        type, or a field that includes the mapping type. <p>For
                                            example, if you enter <codeph>user</codeph> as the
                                            mapping type, the destination writes the document with a
                                                <codeph>user</codeph> mapping type.</p></entry>
                                </row>
                                <row id="row_ElasticDocID">
                                    <entry>Document ID <xref
                                            href="../Destinations/Elastic-DocID.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_r1f_gfg_z3"/>
                                        </xref></entry>
                                    <entry>Expression that evaluates to the ID for the generated
                                        documents. When you do not specify an ID, Elasticsearch
                                        creates an ID for each document.<p>By default, the
                                            destination allows Elasticsearch to create the
                                        ID.</p></entry>
                                </row>
                                <row>
                                    <entry>Parent ID</entry>
                                    <entry>Optional parent ID for the generated documents. Enter a
                                        parent ID or an expression that evaluates to the parent ID.
                                            <p>Use to establish a parent-child relationship between
                                            documents in the same index.</p></entry>
                                </row>
                                <row>
                                    <entry>Routing</entry>
                                    <entry>Optional custom routing value for the generated
                                        documents. Enter a routing value or an expression that
                                        evaluates to the routing value.<p>Elasticsearch routes a
                                            document to a particular shard in an index based on the
                                            routing value defined for the document. You can define a
                                            custom value for each document. If you don’t define a
                                            custom routing value, Elasticsearch uses the parent ID
                                            (if defined) or the document ID as the routing
                                            value.</p></entry>
                                </row>
                                <row id="row_ElasticCharset">
                                    <entry>Data Charset</entry>
                                    <entry>
                                        <p>Character encoding of the data to be processed. </p>
                                    </entry>
                                </row>
                                <row>
                                    <entry>Default Operation <xref
                                            href="../Destinations/Elastic-DefineOperation.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_r1f_ggg_z5"/>
                                        </xref></entry>
                                    <entry id="entry_DefaultOperation">Default CRUD operation to
                                        perform if the sdc.operation.type record header attribute is
                                        not set. </entry>
                                </row>
                                <row>
                                    <entry>Unsupported Operation Handling</entry>
                                    <entry id="entry_UnsupportedOperation">Action to take when the
                                        CRUD operation type defined in the sdc.operation.type record
                                        header attribute is not supported:<ul id="ul_sgk_pg3_1y">
                                            <li>Discard - Discards the record.</li>
                                            <li>Send to Error - Sends the record to the pipeline for
                                                error handling.</li>
                                            <li>Use Default Operation - Writes the record to the
                                                destination system using the default operation.</li>
                                        </ul></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step id="ElasticSHIELD-step">
                <cmd>If you enabled security, on the <wintitle>Security</wintitle> tab, configure
                    the following properties:</cmd>
                <info id="ElasticSHIELD-Info">
                    <table frame="all" rowsep="1" colsep="1" id="table_jsk_fhg_z5">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Security Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Security Username/Password</entry>
                                    <entry>Elasticsearch username and password. <p>Enter the
                                            username and password using the following
                                            syntax:<codeblock>&lt;username>:&lt;password></codeblock></p><note
                                            conref="ReusablePhrases.dita#concept_vhs_5tz_xp/SDCDPM_Tip_Usernames"
                                        /></entry>
                                </row>
                                <row>
                                    <entry>SSL Truststore Path</entry>
                                    <entry>
                                        <p>Location of the truststore file. </p>
                                        <p>Configuring this property is the equivalent to
                                            configuring the shield.ssl.truststore.path Elasticsearch
                                            property. </p>
                                        <p>Not necessary for Elastic Cloud clusters. </p>
                                    </entry>
                                </row>
                                <row>
                                    <entry>SSL Truststore Password</entry>
                                    <entry>
                                        <p>Password for the truststore file. </p>
                                        <p>Configuring this property is the equivalent to
                                            configuring the shield.ssl.truststore.password
                                            Elasticsearch property. </p>
                                        <p>Not necessary for Elastic Cloud clusters. </p>
                                    </entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta">the following are used by Google Pub Sub and
                        rows by Pipeline > Write to Error</draft-comment>
                </cmd>
            </step>
            <step id="step-GooglePubSub">
                <cmd>On the <uicontrol>Pub/Sub</uicontrol> tab, configure the following
                    property:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_ypc_bzq_v1b">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Pub/Sub Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row id="row-PubSub-TopicID">
                                    <entry>Topic ID</entry>
                                    <entry>Google Pub/Sub topic ID to write messages to.</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step id="step-GooglePubSub-Creds">
                <cmd>On the <uicontrol>Credentials</uicontrol> tab, configure the following
                    properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_ffd_w3p_v1b">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Credentials Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row id="row-PubSub-ProjectID">
                                    <entry>Project ID</entry>
                                    <entry>Google Pub/Sub project ID to connect to.</entry>
                                </row>
                                <row id="row-PubSub-CredProvider">
                                    <entry>Credentials Provider <xref
                                            href="../Destinations/PubSubPublisher-Credentials.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                placement="inline" id="image_d3x_vhk_lz"
                                        /></xref></entry>
                                    <entry>Credentials provider to use to connect to Google
                                            Pub/Sub:<ul id="ul_tmm_2rp_v1b">
                                            <li>Default credentials provider</li>
                                            <li>Service account credentials file (JSON) </li>
                                        </ul></entry>
                                </row>
                                <row id="roe-PubSub-CredFilePath">
                                    <entry>Credentials File Path (JSON)</entry>
                                    <entry>When using a Google Cloud service account credentials
                                        file, path to the file that the destination uses to connect
                                        to Google Pub/Sub. The credentials file must be a JSON
                                            file.<p>Enter a path relative to the <ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            /> resources directory, <codeph>$SDC_RESOURCES</codeph>,
                                            or enter an absolute path. </p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta">the following are used by Google Cloud Storage
                        and rows by Pipeline > Write to Error</draft-comment>
                </cmd>
            </step>
            <step id="step-GoogleCloudStorage">
                <cmd>On the <wintitle>GCS</wintitle> tab, configure the following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_fbj_bkw_rt">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>GCS Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/S3Bucket">
                                    <entry/>
                                </row>
                                <row id="row-GCS-CPrefix">
                                    <entry>Common Prefix</entry>
                                    <entry>Common prefix that determines where objects are written.
                                    </entry>
                                </row>
                                <row id="row-GCS-PartPrefix">
                                    <entry>Partition Prefix <xref
                                            href="../Destinations/GCS-PartitionPrefix.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_v34_wcr_yv"/>
                                        </xref></entry>
                                    <entry>Optional partition prefix to specify the partition to
                                        use. <p>Use a specific partition prefix or define an
                                            expression that evaluates to a partition prefix.
                                            </p><p>When using datetime variables in the expression,
                                            be sure to configure the time basis for the
                                        stage.</p></entry>
                                </row>
                                <row id="row-GCS-DataTimeZone">
                                    <entry>Data Time Zone</entry>
                                    <entry>
                                        <p>Time zone for the destination system. Used to resolve
                                            datetimes in a time-based partition prefix. </p>
                                    </entry>
                                </row>
                                <row id="row-GCS-TimeBasis">
                                    <entry>Time Basis <xref
                                            href="../Destinations/GCS-TimeBasis.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_d73_kf8_wv"/>
                                        </xref></entry>
                                    <entry>
                                        <p>Time basis to use for writing to a time-based bucket or
                                            partition prefix. Use one of the following
                                                expressions:<ul id="ul_wbn_qdt_r53">
                                                <li><codeph>${time:now()}</codeph> - Uses the
                                                  processing time as the time basis in conjunction
                                                  with the specified Data Time Zone.</li>
                                                <li>An expression that calls a field and resolves to
                                                  a datetime value, such as
                                                  <codeph>${record:value(&lt;date field
                                                  path>)}</codeph>. Uses the time associated with
                                                  the record as the time basis, adjusted for the
                                                  specified Data Time Zone.</li>
                                            </ul></p>
                                        <p>When the Partition Prefix property has no time component,
                                            you can ignore this property. </p>
                                        <p>Default is <codeph>${time:now()}</codeph>.</p>
                                    </entry>
                                </row>
                                <row id="row-GCS-ObjNamePrefix">
                                    <entry>Object Name Prefix <xref
                                            href="../Destinations/GCS-ObjectNames.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_d34_kf2_wv"/>
                                        </xref></entry>
                                    <entry>Defines a prefix for object names written by the
                                        destination. By default, object names start with "sdc" as
                                        follows: <codeph>sdc-&lt;UUID></codeph>. <p>Not required for
                                            the whole file data format. </p></entry>
                                </row>
                                <row>
                                    <entry>Object Name Suffix <xref
                                            href="../Destinations/GCS-ObjectNames.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_swx_rvt_5bb"/>
                                        </xref></entry>
                                    <entry>Suffix to use for object names, such as txt or json. When
                                        used, the destination adds a period and the configured
                                        suffix as follows: &lt;object name>.&lt;suffix>.<p>You can
                                            include periods within the suffix, but do not start the
                                            suffix with a period. Forward slashes are not
                                            allowed.</p><p>Not available for the whole file data
                                            format. </p></entry>
                                </row>
                                <row>
                                    <entry>Compress with Gzip</entry>
                                    <entry>Compresses files with gzip before writing to Google Cloud
                                        Storage. <p>Not available for the whole file data
                                            format.</p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step id="Step-GCS-Credentials">
                <cmd>On the <uicontrol>Credentials</uicontrol> tab, configure the following
                    properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_zfm_ypx_q1b">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Credentials Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row id="row-GCS-ProjectID">
                                    <entry>Project ID</entry>
                                    <entry>Project ID to connect to.</entry>
                                </row>
                                <row id="row-GCS-CredProvider">
                                    <entry>Credentials Provider <xref
                                            href="../Destinations/GCS-Credentials.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_pls_tp4_hj"/></xref></entry>
                                    <entry>Credentials provider to use to connect:<ul
                                            id="ul_tmm_2rp_v3b">
                                            <li>Default credentials provider</li>
                                            <li>Service account credentials file (JSON) </li>
                                        </ul></entry>
                                </row>
                                <row id="row-GCS-CredFilePath">
                                    <entry>Credentials File Path (JSON) </entry>
                                    <entry>When using a Google Cloud service account credentials
                                        file, path to the file that the origin uses to connect to
                                        Google Cloud Storage. The credentials file must be a JSON
                                            file.<p>Enter a path relative to the <ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            /> resources directory, <codeph>$SDC_RESOURCES</codeph>,
                                            or enter an absolute path. </p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>FS-LateRecords</uicontrol> - for
                        Hadoop FS and Local FS</draft-comment>
                </cmd>
            </step>
            <step id="FS-LateRecords">
                <cmd>On the <wintitle>Late Records</wintitle> tab, configure the following
                    properties:</cmd>
                <info>
                    <note type="tip">These properties are relevant for a time basis based on the
                        time of a record. If you use processing time as the time basis, set the late
                        record time limit to one second.</note>
                    <table frame="all" rowsep="1" colsep="1" id="table_wv3_xzd_br">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Late Records Property <xref
                                            href="../Destinations/HadoopFS-LateRecordHandling.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_skv_3kw_45"/>
                                        </xref>
                                    </entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Late Record Time Limit (secs)</entry>
                                    <entry>Time limit for output directories to accept data. <p>You
                                            can enter a time in seconds, or use the expression to
                                            enter a time in hours. You can also use MINUTES in the
                                            default expression to define the time in minutes.
                                        </p></entry>
                                </row>
                                <row>
                                    <entry>Late Record Handling</entry>
                                    <entry>Determines how to handle late records:<ul
                                            id="ul_gx4_c12_br">
                                            <li>Send to error - Sends the record to the stage for
                                                error handling. </li>
                                            <li>Send to late records file - Sends the record to a
                                                late records file.</li>
                                        </ul></entry>
                                </row>
                                <row>
                                    <entry>Late Record Directory Template <xref
                                            href="../Destinations/HadoopFS-DirectoryTemplates.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_blv_3kw_45"/></xref></entry>
                                    <entry>Template for creating late record directories. You can
                                        use constants, field values, and datetime variables.
                                            <p>Output directories are created based on the smallest
                                            datetime variable in the template.</p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>KafkaConfig</uicontrol> - used for
                        the Kafka Producer - Kafka tab properties. Rows in the table are used for
                        Configuring a Pipeline, error handling > Write to Kafka, and DPM chapter >
                        Aggregated Statistics > Configuring a Pipeline to Aggregate Statistics. --
                        Message per Batch also used by RabbitMQ Producer. Some rows also used by
                        MapR Streams Producer.</draft-comment>
                </cmd>
            </step>
            <step id="KafkaConfig">
                <cmd>On the <wintitle>Kafka</wintitle> tab, configure the following
                    properties:</cmd>
                <info>
                    <p>
                        <table frame="all" rowsep="1" colsep="1" id="KafkaTableProperties">
                            <tgroup cols="2">
                                <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                                <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                                <thead>
                                    <row>
                                        <entry>Kafka Properties</entry>
                                        <entry>Description</entry>
                                    </row>
                                </thead>
                                <tbody>
                                    <row id="KPBrokerURI">
                                        <entry>Broker URI</entry>
                                        <entry>Connection string for the Kafka broker. Use the
                                            following format:
                                                <codeph>&lt;host>:&lt;port></codeph>.<p>To ensure a
                                                connection, enter a comma-separated list of
                                                additional broker URI.</p></entry>
                                    </row>
                                    <row id="KPRuntimeTopic">
                                        <entry>Runtime Topic Resolution <xref
                                                href="../Destinations/KProducer-RuntimeResolution.dita"
                                                  ><image href="../Graphics/icon_moreInfo.png"
                                                  scale="10" id="image_ekt_x5g_cs"/>
                                            </xref></entry>
                                        <entry id="KPRuntimeTopicEntry">Evaluates an expression at
                                            runtime to determine the topic to use for each
                                            record.</entry>
                                    </row>
                                    <row id="KPTopic">
                                        <entry>Topic</entry>
                                        <entry>Topic to use. <p>Not available when using runtime
                                                topic resolution.</p></entry>
                                    </row>
                                    <row id="KPTopicEx">
                                        <entry>Topic Expression</entry>
                                        <entry>Expression used to determine where each record is
                                            written when using runtime topic resolution. Use an
                                            expression that evaluates to a topic name. </entry>
                                    </row>
                                    <row id="KPTopicWList">
                                        <entry>Topic White List</entry>
                                        <entry>List of valid topic names to write to when using
                                            runtime topic resolution. Use to avoid writing to
                                            invalid topics. Records that resolve to invalid topic
                                            names are passed to the stage for error handling. <p>Use
                                                an asterisk (*) to allow writing to any topic name.
                                                By default, all topic names are valid.</p></entry>
                                    </row>
                                    <row id="KPPartStrategy">
                                        <entry>Partition Strategy <xref
                                                href="../Destinations/KProducer-PartitionStrategy.dita#concept_qpm_xp4_4r">
                                                <image href="../Graphics/icon_moreInfo.png"
                                                  scale="10" id="image_l3j_vpb_h1b"/></xref></entry>
                                        <entry>Strategy to use to write to partitions:<ul
                                                id="ul_tq2_yr3_br">
                                                <li>Round Robin - Takes turns writing to different
                                                  partitions.</li>
                                                <li>Random - Writes to partitions randomly.</li>
                                                <li>Expression - Uses an expression to write data to
                                                  different partitions. Writes records to the
                                                  partitions specified by the results of the
                                                  expression.</li>
                                                <li>Default - Uses an expression to extract a
                                                  partition key from the record. Writes records to
                                                  partitions based on a hash of the partition key.
                                                </li>
                                            </ul></entry>
                                    </row>
                                    <row id="KPPartExpr">
                                        <entry>Partition Expression <xref
                                                href="../Destinations/KProducer-PartitionStrategy.dita#concept_qpm_xp4_4r">
                                                <image href="../Graphics/icon_moreInfo.png"
                                                  scale="10" id="image_as2_sc1_ft"/></xref></entry>
                                        <entry id="KPPartExpr_entry">Expression to use with the
                                            default or expression partition strategy. <p>When using
                                                the default partition strategy, specify an
                                                expression that returns the partition key from the
                                                record. The expression must evaluate to a string
                                                value. </p><p>When using the expression partition
                                                strategy, specify an expression that evaluates to
                                                the partition where you want each record written.
                                                Partition numbers start with 0. The expression must
                                                evaluate to a numeric value.</p><p
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/EEditor"
                                            /></entry>
                                    </row>
                                    <row id="KPOneMessPBatch">
                                        <entry>One Message per Batch</entry>
                                        <entry>For each batch, writes the records to each partition
                                            as a single message. </entry>
                                    </row>
                                    <row id="KPKConfigs">
                                        <entry>Kafka Configuration</entry>
                                        <entry>Additional Kafka properties to use. Using <xref
                                                href="../Pipeline_Configuration/SimpleBulkEdit.dita#concept_alb_b3y_cbb"
                                                >simple or bulk edit mode</xref>, click the
                                                <uicontrol>Add</uicontrol> icon and define the Kafka
                                            property name and value. <p>Use the property names and
                                                values as expected by Kafka. Do not use the
                                                broker.list property.</p><p>For information about
                                                enabling secure connections to Kafka, see <xref
                                                  href="../Destinations/KProducer-EnablingSecurity.dita#concept_znr_b3c_rw"
                                                />.</p></entry>
                                    </row>
                                    <row>
                                        <entry>Key Serializer <xref
                                                href="../Destinations/KProducer-DataFormat.dita">
                                                <image href="../Graphics/icon_moreInfo.png"
                                                  scale="10" id="image_as8_sc7_ft"/></xref></entry>
                                        <entry>Method used to serialize the Kafka message key when
                                            the configured data format is Avro. <p>Set to Confluent
                                                to embed the Avro schema ID in each message that
                                                Kafka Producer writes. </p></entry>
                                    </row>
                                    <row>
                                        <entry>Value Serializer</entry>
                                        <entry>Method used to serialize the Kafka message value when
                                            the configured data format is Avro. <p>Set to Confluent
                                                to embed the Avro schema ID in each message that
                                                Kafka Producer writes.</p></entry>
                                    </row>
                                </tbody>
                            </tgroup>
                        </table>
                    </p>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta">MapRStreams-props - used by MapR Streams
                        Producer and Info tag used by Configuring a Pipeline > Write to MapR
                        Streams. </draft-comment>
                </cmd>
            </step>
            <step id="MAPRStreams-Step">
                <cmd>On the <wintitle>MapR Streams Producer</wintitle> tab, configure the following
                    properties:</cmd>
                <info id="MapRStreams-Info">
                    <table frame="all" rowsep="1" colsep="1" id="table_izf_lxn_2v">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>MapR Streams Producer Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/KPRuntimeTopic">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/KPTopic">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/KPTopicEx">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/KPTopicWList">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/KPPartStrategy">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/KPPartExpr">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/KPOneMessPBatch">
                                    <entry/>
                                </row>
                                <row>
                                    <entry>MapR Streams Configuration <xref
                                            href="../Destinations/MapRStreamsProd-AddProps.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_dgb_pns_2v"/></xref>
                                    </entry>
                                    <entry>Additional configuration properties to use. Using <xref
                                            href="../Pipeline_Configuration/SimpleBulkEdit.dita#concept_alb_b3y_cbb"
                                            >simple or bulk edit mode</xref>, click the
                                            <uicontrol>Add</uicontrol> icon and define the MapR
                                        Streams property name and value.<p>Use the property names
                                            and values as expected by MapR.</p><p>You can use MapR
                                            Streams properties and the set of Kafka properties
                                            supported by MapR Streams. </p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>RPCdest</uicontrol> - Used for Config
                        RPC Dest and pipeline error handling > Write to pipeline.</draft-comment>
                </cmd>
            </step>
            <step id="RPCdest">
                <cmd>On the <wintitle>RPC</wintitle> tab, configure the following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="sdcrpcDest-table">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>RPC Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row id="row-RPCconnect">
                                    <entry>SDC RPC Connection <xref
                                            href="../Destinations/RPCdest-Connections.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                placement="inline" id="image_bb2_k4b_ft"
                                        /></xref></entry>
                                    <entry>Connection information for the destination pipeline to
                                        continue processing data. Use the following format:
                                            <codeph>&lt;host>:&lt;port></codeph>. <p>Use a single
                                            RPC connection for each destination pipeline. Using
                                                <xref
                                                href="../Pipeline_Configuration/SimpleBulkEdit.dita#concept_alb_b3y_cbb"
                                                >simple or bulk edit mode</xref>, add additional
                                            connections as needed.</p><p>Use the port number when
                                            you configure the SDC RPC origin that receives the
                                            data.</p></entry>
                                </row>
                                <row id="row-RPCID">
                                    <entry>SDC RPC ID</entry>
                                    <entry>User-defined ID to allow the destination to pass data to
                                        an SDC RPC origin. Use this ID in all SDC RPC origins to
                                        process data from the destination.</entry>
                                </row>
                                <row id="row-verifyHost">
                                    <entry>Verify Host in Server Certificate</entry>
                                    <entry id="entry-verifyHost">Verifies the host in the SDC RPC
                                        origin keystore file.</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>RPCdestAdv </uicontrol>- Used in RPC
                        destination &amp; pipeline error config - write to pipeline.</draft-comment>
                </cmd>
            </step>
            <step id="RPCadv">
                <cmd>On the <wintitle>Advanced</wintitle> tab, configure the following
                    properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_mhd_nd1_ft">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Advanced Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row id="row-RetriesBatch">
                                    <entry>Retries Per Batch</entry>
                                    <entry>Number of times the destination tries to write a batch to
                                        the SDC RPC origin. <p id="p-RetriesBatch">When the
                                            destination cannot write the batch within the configured
                                            number of retries, it fails the batch.</p><p
                                            id="p-RetriesBatchDefault">Default is 3.</p></entry>
                                </row>
                                <row id="row-BackOffPeriod">
                                    <entry>Back Off Period</entry>
                                    <entry>Milliseconds to wait before retrying writing a batch to
                                        the SDC RPC origin.<p id="p-BackOffExample">The value that
                                            you enter increases exponentially after each retry,
                                            until it reaches the maximum wait time of 5 minutes. For
                                            example, if you set the back off period to 10, the
                                            destination attempts the first retry after waiting 10
                                            milliseconds, attempts the second retry after waiting
                                            100 milliseconds, and attempts the third retry after
                                            waiting 1,000 milliseconds. </p><p>Set to 0 to retry
                                            immediately.</p><p id="p-BackOffDefault">Default is
                                            0.</p></entry>
                                </row>
                                <row id="row-ConTimeout">
                                    <entry>Connection Timeout (ms)</entry>
                                    <entry>Milliseconds to establish a connection to the SDC RPC
                                        origin. <p>The destination retries the connection based on
                                            the Retries Per Batch property.</p><p>Default is 5000
                                            milliseconds.</p></entry>
                                </row>
                                <row id="row-ReadTimeout">
                                    <entry>Read Timeout (ms)</entry>
                                    <entry>Milliseconds to wait for the SDC RPC origin to read data
                                        from a batch. <p id="p-ReadTimeout">The destination retries
                                            the write based on the Retries Per Batch property.</p><p
                                            id="p-ReadTimeoutDefault">Default is 2000
                                            milliseconds.</p></entry>
                                </row>
                                <row id="row-UseCompression">
                                    <entry>Use Compression <xref
                                            href="../Destinations/RPC-Compression.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                placement="inline"/></xref></entry>
                                    <entry>Enables the destination to use compression to pass data
                                        to the SDC RPC origin. Enabled by default. </entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="alisontaylor">The following steps are used in "Step 1.
                        Install the StreamSets Custom Service Descriptor" in the Installation
                        chapter and the Upgrade chapter.</draft-comment>
                </cmd>
            </step>
            <step id="CSDInstallDownload">
                <cmd>Use the following URL to download the CSD from the StreamSets website: <xref
                        href="https://streamsets.com/opensource" format="html" scope="external"
                    />.</cmd>
            </step>
            <step id="CSDInstallPath">
                <cmd>Copy the <ph
                        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                    /> CSD file to the<uicontrol> Local Descriptor Repository Path</uicontrol>. By
                    default, the path is <codeph>/opt/cloudera/csd</codeph>.</cmd>
                <info>To verify the path to use, in Cloudera Manager, click <menucascade>
                        <uicontrol>Administration</uicontrol>
                        <uicontrol>Settings</uicontrol>
                    </menucascade>. In the navigation panel, select the <uicontrol>Custom Service
                        Descriptors</uicontrol> category. Place the CSD file in the path configured
                    for <uicontrol>Local Descriptor Repository Path</uicontrol>. </info>
            </step>
            <step id="CSDInstallFileOwnership">
                <cmd>Set the file ownership to <codeph>cloudera-scm:cloudera-scm</codeph> with
                    permission <uicontrol>644</uicontrol>. </cmd>
                <info>For example:
                    <codeblock>chown cloudera-scm:cloudera-scm /opt/cloudera/csd/STREAMSETS*.jar
chmod 644 /opt/cloudera/csd/STREAMSETS*.jar</codeblock></info>
            </step>
            <step id="CSDInstallRestart">
                <cmd>Use one of the following commands to restart Cloudera Manager Server:</cmd>
                <info><p>For Ubuntu 14.04, CentOS 6, or Red Hat Enterprise Linux 6:
                        <codeblock>service cloudera-scm-server restart</codeblock></p>For Ubuntu
                    16.04, CentOS 7, or Red Hat Enterprise Linux 7:
                    <codeblock>systemctl restart cloudera-scm-server</codeblock></info>
            </step>
            <step id="CSDInstallRestartService">
                <cmd>In Cloudera Manager, to restart the Cloudera Management Service, click <menucascade>
                        <uicontrol>Home</uicontrol>
                        <uicontrol>Status</uicontrol>
                    </menucascade>. To the right of Cloudera Management Service, click the
                        <uicontrol>Menu</uicontrol> icon and select
                    <uicontrol>Restart</uicontrol>.</cmd>
            </step>
            <step>
                <cmd>
                    <draft-comment author="alisontaylor">The following steps are used in "Step 2.
                        Manually Install the Parcel and Checksum Files (Optional)" used in the
                        Installation and Upgrade chapters.</draft-comment>
                </cmd>
            </step>
            <step id="ParcelDownload">
                <cmd>Download the StreamSets parcel and related checksum file for the Cloudera
                    Manager Server operating system from the following location:</cmd>
                <info><xref href="https://archives.streamsets.com/index.html" format="html"
                        scope="external"/></info>
            </step>
            <step id="ParcelRepoPath">
                <cmd>Copy the StreamSets parcel and checksum files to the <uicontrol>Cloudera
                        Manager Local Parcel Repository Path</uicontrol>. </cmd>
                <info>By default, the path is <codeph>/opt/cloudera/parcel-repo</codeph>.</info>
                <info>To verify the path to use, click <menucascade>
                        <uicontrol>Administration</uicontrol>
                        <uicontrol>Settings</uicontrol>
                    </menucascade>. In the navigation panel, select the
                        <uicontrol>Parcels</uicontrol> category. Place the StreamSets parcel file in
                    the path configured for <uicontrol>Local Parcel Repository Path</uicontrol>.
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="alisontaylor">The following steps are used for the
                        Enabling SSL topics for the MongoDB origin and destination, and for the
                        Configuring Secure Connections to LDAP topic in Configuration
                        chapter</draft-comment>
                </cmd>
            </step>
            <step id="MongoDBEnableSSL_step1">
                <cmd>In the <uicontrol>Advanced</uicontrol> tab for the stage, select the
                        <uicontrol>SSL Enabled</uicontrol> property.</cmd>
            </step>
            <step id="MongoDBEnableSSL_step2">
                <cmd>Define the following options in the SDC_JAVA_OPTS environment variable in the
                    Data Collector environment configuration file located in the
                        <codeph>$SDC_DIST/libexec</codeph> directory:</cmd>
                <choices id="choices_vwy_pg2_ww">
                    <choice><codeph>javax.net.ssl.trustStore</codeph> - path to truststore
                        file</choice>
                    <choice><codeph>javax.net.ssl.trustStorePassword</codeph> - truststore
                        password</choice>
                </choices>
                <info>
                    <p conref="ReusablePhrases.dita#concept_vhs_5tz_xp/EnvFileLocation"/>
                    <p>For example, define the options as
                        follows:<codeblock>export SDC_JAVA_OPTS="<b class="+ topic/ph hi-d/b ">-Djavax.net.ssl.trustStore=&lt;path to truststore file> -Djavax.net.ssl.trustStorePassword=&lt;password></b> 
-Xmx1024m  -Xms1024m -server ${SDC_JAVA_OPTS}"                   </codeblock></p>
                    <p>Or to secure the password, save the password in a text file and then define
                        the truststore password option as follows:
                            <codeph>-Djavax.net.ssl.trustStorePassword=$(cat
                            passwordfile.txt)</codeph></p>
                </info>
            </step>
            <step id="MongoDBEnableSSL_step3">
                <cmd>Restart <ph
                        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                    /> to enable the changes.</cmd>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta">Cluster doc shared steps – </draft-comment>
                    <draft-comment author="Loretta">First checkpoint for Kafka and Mesos YARN
                        Streaming, Info steps conrefed by other versions.</draft-comment>
                </cmd>
            </step>
            <step id="Cluster-CheckpointStorage">
                <cmd>To enable checkpoint metadata storage, grant the user defined in the user
                    environment variable write permission on
                    <filepath>/user/$SDC_USER</filepath>.</cmd>
                <info id="Cluster-info-SDCUSER">The user environment variable defines the system
                    user used to run Data Collector as a service. The file that defines the user
                    environment variable depends on your operating system. For more information, see
                        <xref
                        href="../Configuration/DCUserGroupServiceStart.dita#concept_htz_t1s_3v"/>. </info>
                <info id="Cluster-info-SDCUser-Example">For example, say the user environment
                    variable is defined as <filepath>sdc</filepath> and the cluster does not use
                    Kerberos. Then you might use the following commands to create the directory and
                    configure the necessary write
                    permissions:<codeblock id="Cluster-Code-SDCUserCode">$sudo -u hdfs hadoop fs -mkdir /user/sdc
$sudo -u hdfs hadoop fs -chown sdc /user/sdc</codeblock></info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta">Mesos version of above, removed Kerberos from
                        example.</draft-comment>
                </cmd>
            </step>
            <step id="Cluster-CheckpointStorage-Mesos">
                <cmd>To enable checkpoint metadata storage, grant the user defined in the user
                    environment variable write permission on
                    <filepath>/user/$SDC_USER</filepath>.</cmd>
                <info conref="#task_kzs_5vz_sq/Cluster-info-SDCUSER"/>
                <info>For example, say $SDC_USER is defined as <filepath>sdc</filepath>. Then you
                    might use the following commands to create the directory and configure the
                    necessary write
                    permissions:<codeblock conref="#task_kzs_5vz_sq/Cluster-Code-SDCUserCode"/></info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta">Spark Submit Script step - whole step used by
                        Kafka Yarn Streaming, Kafka Mesos and Mapr Streaming use parts of
                        it.</draft-comment>
                </cmd>
            </step>
            <step id="cluster-SparkSubmitScript">
                <cmd id="cmd-SparkSubmitScript">If necessary, specify the location of the
                    spark-submit script.</cmd>
                <info id="info-SparkSubmit-Default"><ph
                        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                    /> assumes that the spark-submit script used to submit job requests to Spark
                    Streaming is located in the following directory:
                    <codeblock>/usr/bin/spark-submit</codeblock></info>
                <info id="info-SparkSubmit-envVariable">If the script is not in this directory, use
                    the SPARK_SUBMIT_YARN_COMMAND environment variable to define the location of the
                    script.</info>
                <info id="info-SparkSubmit-Location">The location of the script may differ depending
                    on the Spark version and distribution that you use.</info>
                <info id="info-SparkSubmit-example"><ph id="ph-SparkSubmit-example">For example,
                        when using CDH Spark 2.1, the spark-submit script is in the following
                        directory by default: /usr/bin/spark2-submit. Then, you might use the
                        following command to define the location of the
                    script:</ph><codeblock>export SPARK_SUBMIT_YARN_COMMAND<ph id="ph-SparkSubmit-Ex-command">=/usr/bin/spark2-submit</ph></codeblock></info>
                <info id="info-SparkSubmit-Restart">
                    <note id="in">If you change the location of the spark-submit script, you must
                        restart <ph conref="ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> to
                        capture the change.</note>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta">MapR Cluster Batch and HDFS (cluster
                        batch)</draft-comment>
                </cmd>
            </step>
            <step id="ClusterBatch-SDCUser">
                <cmd>Grant the user defined in the user environment variable write permission on
                        <filepath>/user/$SDC_USER</filepath>.</cmd>
                <info conref="#task_kzs_5vz_sq/Cluster-info-SDCUSER"/>
                <info conref="#task_kzs_5vz_sq/Cluster-info-SDCUser-Example"/>
            </step>
            <step>
                <cmd/>
            </step>
            <step id="Cluster-ConfigKerberos">
                <cmd>If YARN is configured to use Kerberos authentication, configure <ph
                        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                    /> to use Kerberos authentication. </cmd>
                <info>When you configure Kerberos authentication for <ph
                        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                    />, you enable <ph
                        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                    /> to use Kerberos and define the principal and keytab. <note type="important"
                        id="Cluster-KerbNote">For cluster pipelines, enter an absolute path to the
                        keytab when configuring <ph
                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                        />. Standalone pipelines do not require an absolute path.</note></info>
                <info>Once enabled, <ph
                        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                    /> automatically uses the Kerberos principal and keytab to connect to any YARN
                    cluster that uses Kerberos. <ph
                        conref="ReusablePhrases.dita#concept_vhs_5tz_xp/SDCDPM-Kerberos"/></info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta">Just the bullets in this step used I
                        think:</draft-comment>
                </cmd>
            </step>
            <step>
                <cmd>To enable <ph
                        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                    /> to submit YARN jobs, perform one of the following tasks:</cmd>
                <info>
                    <ul id="Cluster-SubmitYarnTasks">
                        <li>On YARN, set the min.user.id to a value equal to or lower than the user
                            ID associated with the <ph
                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                            /> user ID, typically named "sdc".</li>
                        <li>On YARN, add the <ph
                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                            /> user name, typically "sdc", to the allowed.system.users
                            property.</li>
                    </ul>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="alisontaylor">The following two steps are reused in the
                        Tarball and RPM upgrade section - Update the Configuration
                        Files</draft-comment>
                </cmd>
            </step>
            <step id="MainConfigFile">
                <cmd>Compare the previous and new versions of the <codeph>sdc.properties</codeph>
                    file, and update the new file as needed with the same customized property
                    values.</cmd>
            </step>
            <step id="RemainingConfigFiles">
                <cmd>Compare the previous and new versions of the remaining files, and update the
                    new files as needed with the same customized property values:</cmd>
                <info>
                    <ul>
                        <li>The appropriate realm.properties file, based on the authentication type
                            that you use.</li>
                        <li>credential stores properties file</li>
                        <li><codeph>email-password.txt</codeph></li>
                        <li>keystore files</li>
                        <li>LDAP files</li>
                        <li>log4j properties file</li>
                        <li>security policy file</li>
                        <li>Vault properties file<p>As of version 2.7.0.0, most of the Vault
                                configuration properties have been moved to the new credential
                                stores properties file. The properties use the same name, with an
                                added "credentialStore.vault.config" prefix. If you are upgrading
                                from a version earlier than 2.7.0.0, copy any values that you
                                customized in the previous Vault properties file into the same
                                property names in the credential stores properties file.</p></li>
                    </ul>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="alisontaylor">Use in Starting Pipelines with Parameters
                        in the Pipeline Configuration chapter and the Pipeline Maintenance
                        chapter</draft-comment>
                </cmd>
            </step>
            <step id="step_StartPipelineParams1">
                <cmd>From the pipeline canvas, click the <uicontrol>More</uicontrol> icon, and then
                    click <uicontrol>Start with Parameters</uicontrol>.</cmd>
                <info>If <uicontrol>Start with Parameters</uicontrol> is not enabled, the pipeline
                    is not valid.<p>The <uicontrol>Start with Parameters</uicontrol> dialog box
                        lists all parameters defined for the pipeline and their default
                    values.</p></info>
            </step>
            <step id="step_StartPipelineParams2">
                <cmd>Override any default values with the values you want to use for this pipeline
                    run.</cmd>
            </step>
            <step id="step_StartPipelineParams3">
                <cmd>Click <uicontrol>Start</uicontrol>.</cmd>
            </step>
        </steps>
    </taskbody>
</task>
