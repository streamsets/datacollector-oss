<?xml version="1.0" encoding="UTF-8"?>
<!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_jzm_kf4_zx">
 <title>Azure Data Lake Store</title>
 <conbody>
  <p><indexterm>Azure Data Lake Store
                    destination<indexterm>overview</indexterm></indexterm><indexterm>destinations<indexterm>Azure
                    Data Lake Store</indexterm></indexterm>The Azure Data Lake Store destination
            writes data to the Microsoft Azure Data Lake Store. You can use the Azure Data Lake
            Store destination in standalone and cluster batch pipelines. The destination supports
            connecting to Azure Data Lake Store using Azure AD Service Identity authentication.
            Azure AD User Identity authentication is not supported at this time.</p>
        <p>Before you use the destination, you must perform some prerequisite tasks.</p>
        <p>When you configure the Azure Data Lake Store destination, you specify connection
            information such as the Application ID and fully qualified domain name (FQDN) for the
            account. </p>
        <p>You can define a directory template and time basis to determine the output directories
            that the destination creates and the files where records are written. You can also
            define a file prefix and suffix, the data time zone, and properties that define when the
            destination closes a file. </p>
        <p>Alternatively, you can write records to the specified directory, use the defined Avro
            schema, and roll files based on record header attributes. For more information, see
                <xref href="../Pipeline_Design/RecordBasedWrites-overview.dita#concept_lmn_gdc_1w"
            />.</p>
        <p>The destination can also generate events for an event stream. For more information about
            the event framework, see <xref
                href="../Event_Handling/EventFramework-Overview.dita#concept_cph_5h4_lx"/>.</p>
 </conbody>
</concept>
