<?xml version="1.0" encoding="UTF-8"?>
<!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_t34_2ds_bx">
 <title>Map Jobs into a Topology</title>
 <shortdesc>In <ph conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
        />, you can monitor and view the details of a single pipeline. However, you typically run
        multiple intermediary pipelines, all of which work together to create a complete
        dataflow.</shortdesc>
 <conbody>
        <p><draft-comment author="alisontaylor">text copied from same topic in Control Hub User
                Guide. Make the same updates in both places. </draft-comment>As a data architect,
            you create a topology in <ph
                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/DPM-short"/> to
            map multiple related jobs into a single view. A topology provides interactive end-to-end
            views of data as it traverses multiple pipelines. You can add any number of jobs to a
            topology.</p>
        <p>To continue our Customer 360 example, after the WebLog Collection pipeline reads web
            server log files and writes the data to Kafka, another pipeline consumes the Kafka data,
            processes it, and streams the data to HDFS. Additional pipelines read from Twitter
            social feeds and from an enterprise data warehouse and also write the data to HDFS. In
                <ph conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/DPM-short"
            />, you can create a topology that includes jobs for all the pipelines, as follows: </p>
        <p><image href="../Graphics/DPM_MapPipelinesTopology.png" scale="60" id="image_q5h_1dc_fbb"
            /></p>
        <p>From the topology, you can select each job and then drill into the configuration details
            of each pipeline. For example, if we select the Social Feeds Dataflows job in the
            topology canvas above, we can the three stages included in the pipeline in the detail
            pane on the right.</p>
    </conbody>
</concept>
