<?xml version="1.0" encoding="UTF-8"?>
<!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_bl5_dl4_1r">
 <title>How does this really work?</title>
 <shortdesc>Let's walk through it...</shortdesc>
 <conbody>
  <p>After you install and launch <ph
    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>, you use the
    <ph conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> UI to log
   in and create your first pipeline. </p>
  <p>What do you want it to do? Let's say you want to read XML files from a directory and remove the
   newline characters before moving it into HDFS. To do this, you start with a Directory origin
   stage and configure it to point to the source file directory. (You can also have the stage
   archive processed files and write files that were not fully processed to a separate directory for
   review.)</p>
  <p>To remove the newline characters, connect Directory to an Expression Evaluator processor and
   configure it to remove the newline character from the last field in the record.</p>
  <p>To make the data available to HDFS, you connect the Expression Evaluator to a Hadoop FS
   destination stage. You configure the stage to write the data as a JSON object (though you can use
   other data formats as well). </p>
  <p>You preview data to see how source data moves through the pipeline and notice that some fields
   have missing data. So you add a Value Replacer to replace null values in those fields. </p>
  <p>Now that the data flow is done, you configure the pipeline error record handling to write error
   records to a file, you create a data drift alert to let you know when field names change, and you
   configure an email alert to let you know when the pipeline generates more than 100 error records.
   Then, you start the pipeline and <ph
    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> goes to work. </p>
  <p>The <ph conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> goes
   into Monitor mode and displays summary and error statistics immediately. To get a closer look at
   the activity, you take a snapshot of the pipeline so you can examine how a set of data passed
   through the pipeline. You see some unexpected data in the pipeline, so you create a data rule for
   a link between two stages to gather information about similar data and set an alert to notify you
   when the numbers get too high.</p>
  <p>And what about those error records being written to file? They're saved with error details, so
   you can create an error pipeline to reprocess that data. Et voila!</p>
  <p>StreamSets <ph conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
   /> is a powerful tool, but we're making it as simple as possible to use. So give it a try, click
   the Help icon for information, and contact us if you need a hand. </p>
 </conbody>
</concept>
