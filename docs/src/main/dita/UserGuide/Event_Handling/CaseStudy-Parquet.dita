<?xml version="1.0" encoding="UTF-8"?>
<!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_jkm_rnz_kx">
    <title>Case Study: Parquet Conversion</title>
    <conbody>
        <p><indexterm>dataflow trigger case study<indexterm>HDFS avro to
                parquet</indexterm></indexterm><indexterm>MapReduce executor<indexterm>case
                    study</indexterm></indexterm>Say you want to store data on HDFS using the
            columnar format, Parquet. But <ph
                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>
            doesn't have a Parquet data format. How do you do it? </p>
        <p>The event framework was created for exactly this purpose. The simple addition of an event
            stream to the pipeline enables the automatic conversion of Avro files to Parquet. </p>
        <p>You can use the Spark executor to trigger a Spark application or the MapReduce executor
            to trigger a MapReduce job. This case study uses the MapReduce executor.</p>
        <p>It's just a few simple steps:</p>
        <p>
            <ol id="ol_oh1_4gm_lx">
                <li>Create the pipeline you want to use.<p>Use the origin and processors that you
                        need, like any other pipeline. And then, configure Hadoop FS to write Avro
                        data to HDFS.</p><p>The Hadoop FS destination generates events each time it
                        closes a file. This is perfect because we want to convert files to Parquet
                        only after they are fully written. </p><p>
                        <note>To avoid running unnecessary numbers of MapReduce jobs, configure the
                            destination to create files as large as the destination system can
                            comfortably handle.</note>
                    </p><p><image href="../Graphics/Event-ParquetBasicPipe.png"
                            id="image_x2b_yvm_lx" scale="75"/></p></li>
                <li>Configure Hadoop FS to generate events. <p>On the <wintitle>General</wintitle>
                        tab of the destination, select the <uicontrol>Produce Events</uicontrol>
                        property.</p><p>With this property selected, the event output stream becomes
                        available and Hadoop FS generates an event record each time it closes an
                        output file. The event record includes the file path for the closed file.
                            </p><p><image href="../Graphics/Event-ParquetHDFS.png"
                            id="image_ibh_mvm_lx" scale="70"/></p></li>
                <li>Connect the Hadoop FS event output stream to a MapReduce executor. <p>Now, each
                        time the MapReduce executor receives an event, it triggers the jobs that you
                        configure it to run.</p><p><image href="../Graphics/Event-ParquetPipe.png"
                            id="image_wjn_k5m_lx" scale="65"/></p></li>
                <li>Configure the MapReduce executor to run a job that converts the completed Avro
                    file to Parquet.<p>In the MapReduce executor, configure the MapReduce
                        configuration details and select the Avro to Parquet job type. Then, on the
                        Avro to Parquet tab, configure the details for the job. </p><p><image
                            href="../Graphics/Event-Parquet-MapReduce.png" id="image_rzm_frb_tx"
                            scale="60"/></p><p>The <uicontrol>Input Avro File</uicontrol> property
                        default, <codeph>${record:value('/filepath')}</codeph>, runs the job on the
                        file specified in the Hadoop FS file closure event record. </p></li>
            </ol>
        </p>
        <p>With this event stream added to the pipeline, each time the Hadoop FS destination closes
            a file, it generates an event. When the MapReduce executor receives the event, it kicks
            off a MapReduce job that converts the Avro file to Parquet. Simple!</p>
    </conbody>
</concept>
