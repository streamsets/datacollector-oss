<?xml version="1.0" encoding="UTF-8"?>
<!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_ocb_nnl_px">
 <title>Case Study: Event Storage </title>
 <conbody>
  <p><indexterm>dataflow trigger case study<indexterm>event storage</indexterm></indexterm>Store
            event records to preserve an audit trail of the events that occur. You can store event
            records from any event-generating stage. For this case study, say you want to keep a log
            of the files written to HDFS by the following pipeline: </p>
        <p><image href="../Graphics/Event-Storage.png" id="image_csj_gwl_px" scale="60"/></p>
        <p>To do this, you simply:<ol id="ol_xgc_3wl_px">
                <li>Configure the Hadoop FS destination to generate events.<p>On the
                            <wintitle>General</wintitle> tab, select the <uicontrol>Produce
                            Events</uicontrol> property </p><p>Now the event output stream becomes
                        available, and the destination generates an event each time it closes a
                        file. For this destination, each event record includes fields for the file
                        name, file path, and size of the closed file. </p><p><image
                            href="../Graphics/Event-Storage-HDFS.png" id="image_qbc_1hj_yx"
                            scale="60"/></p></li>
                <li>You can write the event records to any destination, but let's assume you want to
                    write them to HDFS as well:<p><image href="../Graphics/Event-Storage-HDFS-2.png"
                            id="image_orv_fyl_px" scale="55"/></p><p>You could be done right there,
                        but you want to include the time of the event in the record, so you know
                        exactly when the Hadoop FS destination closed a file. </p></li>
                <li> All event records include the event creation time in the
                    sdc.event.creation_timestamp record header attribute, so you can add an
                    Expression Evaluator to the pipeline and use the following expression to include
                    the creation time in the
                        record:<codeblock>${record:attribute('sdc.event.creation_timestamp')}</codeblock><p>The
                        resulting pipeline looks like this: </p><p><image
                            href="../Graphics/Event-Storage-EEval.png" id="image_sqc_3bq_tx"
                            scale="60"/></p><p>Note that event creation time is expressed as an
                        epoch or Unix timestamp, such as 1477698601031. And record header attributes
                        provide data as strings.</p><p>
                        <note type="tip">You can use time functions to convert timestamps to
                            different data types. For more information, see <xref
                                href="../Expression_Language/Functions.dita#concept_lhz_pyp_1r"
                            />.</note>
                    </p></li>
            </ol></p>
 </conbody>
</concept>
