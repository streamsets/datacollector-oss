<?xml version="1.0" encoding="UTF-8"?>
<!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_rxg_shn_lx">
 <title>Executors</title>
 <conbody>
        <p>
            <indexterm>event framework<indexterm>executors</indexterm></indexterm>Executors perform
            tasks when they receive event records. You can use the following executor stages for
            event handling:<dl>
                <dlentry>
                    <dt>Amazon S3 executor</dt>
                    <dd>Creates new Amazon S3 objects for the specified content or adds tags to
                        existing Amazon S3 objects upon receiving an event. </dd>
                    <dd><ph
                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/ph-S3usage"
                        /></dd>
                </dlentry>
                <dlentry>
                    <dt>Email executor</dt>
                    <dd>Sends a custom email to the configured recipients upon receiving an event.
                        You can optionally configure a condition that determines when to send the
                        email. </dd>
                    <dd>You can use the executor in any logical way, such as sending an email each
                        time the Azure Data Lake Store destination completes streaming a whole file.
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>Hive Query executor</dt>
                    <dd>Executes user-defined Hive or Impala queries for each event.</dd>
                    <dd><ph
                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/HiveMetadata-Use"
                        /></dd>
                    <dd>For example, you might use the Hive Query executor as part of the <xref
                            href="../Hive_Drift_Solution/HiveDrift-Overview.dita#concept_phk_bdf_2w"
                            >Drift Synchronization Solution for Hive</xref> if you read data with
                        Impala. Impala requires you to run the Invalidate Metadata command when the
                        table structure or data changes. </dd>
                    <dd>Instead of trying to time this action manually, you can use the Hive Query
                        executor to submit the command automatically each time the Hive Metastore
                        destination changes the structure of a table and each time the Hadoop FS
                        destination closes a file. </dd>
                </dlentry>
                <dlentry>
                    <dt>HDFS File Metadata executor</dt>
                    <dd>Changes file metadata, creates an empty file, or removes a file or directory
                        from HDFS or a local file system upon receiving an event. <p>When changing
                            file metadata, the executor can rename and move files in addition to
                            specifying the owner and group, and updating permissions and ACLs for
                            files. When creating an empty file, the executor can specify the owner
                            and group and set permissions and ACLs for the file. When removing files
                            and directories, the executor performs the task recursively.</p></dd>
                    <dd><ph
                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/FileMetadata-Use"
                        />
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>Pipeline Finisher executor</dt>
                    <dd>Stops the pipeline when it receives an event, transitioning the pipeline to
                        a Finished state. Allows the pipeline to complete all expected processing
                        before stopping.</dd>
                    <dd><ph
                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/PipeFinish-Use"
                        /> This enables you to achieve "batch" processing - stopping the pipeline
                        when all available data is processed rather than leaving the pipeline to sit
                        idle indefinitely. </dd>
                    <dd>For example, you might use the Pipeline Finisher executor with the JDBC
                        Multitable Consumer to stop the pipeline when it processes all queried data
                        in the specified tables.</dd>
                </dlentry>
                <dlentry>
                    <dt>JDBC Query executor</dt>
                    <dd>Connects to a database using JDBC and runs the specified SQL query. </dd>
                    <dd>Use to run a SQL query on a database after an event occurs.</dd>
                </dlentry>
                <dlentry>
                    <dt>MapR FS File Metadata executor</dt>
                    <dd>Changes file metadata, creates an empty file, or removes a file or directory
                        in MapR FS upon receiving an event. When changing file metadata, the
                        executor can rename and move files in addition to specifying the owner and
                        group, and updating permissions and ACLs for files. When creating an empty
                        file, the executor can specify the owner and group and set permissions and
                        ACLs for the file. When removing files and directories, the executor
                        performs the task recursively.</dd>
                    <dd><ph
                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/MapRMetadata-Use"
                        />
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>MapReduce executor</dt>
                    <dd>Connects to HDFS or MapR FS and starts a MapReduce job for each event. </dd>
                    <dd><ph
                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/MapReduce-Use"
                        /> For example, you might use the MapReduce executor with the Hadoop FS
                        destination to convert Avro files to Parquet when Hadoop FS closes a file. </dd>
                </dlentry>
                <dlentry>
                    <dt>Shell executor</dt>
                    <dd>Executes a user-defined shell script for each event. </dd>
                </dlentry>
                <dlentry>
                    <dt>Spark executor</dt>
                    <dd>Connects to Spark on YARN or Databricks and starts a Spark application for
                        each event.</dd>
                    <dd><ph
                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/Spark-Use"
                        /> For example, you might use the Spark executor with the Hadoop FS
                        destination to convert Avro files to Parquet when Hadoop FS closes a file. </dd>
                </dlentry>
            </dl></p>
 </conbody>
</concept>
