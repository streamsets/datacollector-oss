<?xml version="1.0" encoding="UTF-8"?>
<!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_d1q_xl4_lx">
    <title>Case Study: Output File Management</title>
    <conbody>
        <p><indexterm>dataflow trigger case study<indexterm>output file
                management</indexterm></indexterm><indexterm>Hive Metadata executor<indexterm>case
                    study</indexterm></indexterm>By default, the Hadoop FS destination creates a
            complex set of directories for output files and late record files, keeping files open
            for writing based on stage configuration. That's great, but once the files are complete,
            you'd like the files moved to a different location. And while you're at it, it would be
            nice to set the permissions for the written files. </p>
        <p>So what do you do? </p>
        <p>Add an event stream to the pipeline to manage output files when the Hadoop FS destination
            is done writing them. Then, use the HDFS File Metadata executor in the event
                stream.<note>If you prefer, instead of using the Hadoop FS destination with the HDFS
                File Metadata executor, you can use the Local FS destination with the HDFS File
                Metadata executor or the MapR FS destination with the MapR FS File Metadata
                executor.</note></p>
        <p>Here's a pipeline that reads from a database using JDBC, performs some processing, and
            writes to HDFS:</p>
        <p><image href="../Graphics/Event-Move-BasicPipe.png" id="image_y1d_zm4_lx" scale="70"/></p>
        <ol id="ol_fvh_5m4_lx">
            <li>To add an event stream, first configure Hadoop FS to generate events:<p>On the
                        <wintitle>General</wintitle> tab of the Hadoop FS destination, select the
                        <uicontrol>Produce Events</uicontrol> property. </p><p>Now, the event output
                    stream becomes available, and Hadoop FS generates an event record each time it
                    closes an output file. The Hadoop FS event record includes fields for the file
                    name, path, and size.</p><p><image href="../Graphics/Event-Move-HDFS.png"
                        id="image_n5q_5p4_lx" scale="65"/></p></li>
            <li>Connect the Hadoop FS event output stream to a HDFS File Metadata executor.<p>Now,
                    each time the HDFS File Metadata executor receives an event, it triggers the
                    tasks that you configure it to run.</p><p><image
                        href="../Graphics/Event-Move-HDFSMetadata.png" id="image_x4w_kq4_lx"
                        scale="75"/></p></li>
            <li>Configure the HDFS File Metadata executor to move the files to the directory that
                you want and set the permissions for the file.<p>In the HDFS File Metadata executor,
                    configure the HDFS configuration details on the <wintitle>HDFS</wintitle> tab.
                    Then, on the <wintitle>Tasks</wintitle> tab, select <uicontrol>Change Metadata
                        on Existing File</uicontrol> configure the changes that you want to make.
                    </p><p>In this case, you want to move files to
                        <filepath>/new/location</filepath>, and set the file permissions to 0440 to
                    allow the user and group read access to the files:</p><p><image
                        href="../Graphics/Event-Move-FileMetadata-props.png" id="image_uk1_l1c_tx"
                        scale="60"/></p></li>
        </ol>
        <p>With this event stream added to the pipeline, each time the Hadoop FS destination closes
            a file, it generates an event record. When the HDFS File Metadata executor receives the
            event record, it moves the file and sets the file permissions. No muss, no fuss.</p>
    </conbody>
</concept>
