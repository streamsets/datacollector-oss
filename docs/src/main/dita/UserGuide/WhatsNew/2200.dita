<?xml version="1.0" encoding="UTF-8"?>
<!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_oyv_zfk_fy">
 <title>What's New in 2.2.0.0</title>
 <conbody>
  <p><ph conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> version
            2.2.0.0 includes the following new features and enhancements:<dl>
                <dlentry>
                    <dt>Event Framework</dt>
                    <dd>The Data Collector event framework enables the pipeline to trigger tasks in
                        external systems based on actions that occur in the pipeline, such as
                        running a MapReduce job after the pipeline writes a file to HDFS. You can
                        also use the event framework to store event information, such as when an
                        origin starts or completes reading a file. </dd>
                    <dd>For details, see the <xref
                            href="../Event_Handling/EventFramework-Overview.dita#concept_cph_5h4_lx"
                            >Event Framework chapter</xref>. </dd>
                    <dd>The event framework includes the following new features and enhancements:<ul
                            id="ul_ojf_xgk_fy">
                            <li><xref href="../Executors/Executors-overview.dita#concept_stt_2lk_fx"
                                    >New executor stages</xref>. A new type of stage that performs
                                tasks in external systems upon receiving an event. This release
                                includes the following executors:<ul id="ul_bn4_ygk_fy">
                                    <li><xref
                                            href="../Executors/HDFSMetadata.dita#concept_wgj_slk_fx"
                                            >HDFS File Metadata executor</xref> - Changes file
                                        metadata such as the name, location, permissions, and ACLs. </li>
                                    <li><xref href="../Executors/HiveQuery.dita#concept_kjw_llk_fx"
                                            >Hive Query executor</xref> - Runs a Hive or Impala
                                        query. </li>
                                    <li><xref href="../Executors/JDBCQuery.dita#concept_j3r_gcv_sx"
                                            >JDBC Query executor</xref> - Runs a SQL query. </li>
                                    <li><xref href="../Executors/MapReduce.dita#concept_bj2_zlk_fx"
                                            >MapReduce executor</xref> - Runs a custom MapReduce job
                                        or an Avro to Parquet MapReduce job. </li>
                                </ul></li>
                            <li>Event generation. The following stages now generate events that you
                                can use in a pipeline: <ul id="ul_sxd_bhk_fy">
                                    <li><xref
                                            href="../Origins/Directory-EventGeneration.dita#concept_ttg_vgn_qx"
                                            >Directory</xref> and <xref
                                            href="../Origins/FileTail-EventGeneration.dita#concept_gwn_c32_px"
                                            >File Tail</xref> origins - Generate events when they
                                        start and complete reading a file.</li>
                                    <li><xref
                                            href="../Destinations/AmazonS3-EventGeneration.dita#concept_aqq_tt2_px"
                                            >Amazon S3 destination</xref> - Generates events when it
                                        completes writing to an object or streaming a whole file. </li>
                                    <li><xref
                                            href="../Destinations/HadoopFS-EventGeneration.dita#concept_bvb_rxj_px"
                                            >Hadoop FS</xref>, <xref
                                            href="../Destinations/LocalFS-EventGeneration.dita#concept_in1_fcm_px"
                                            >Local FS</xref>, and <xref
                                            href="../Destinations/MapRFS-EventGeneration.dita#concept_bqd_3qb_rx"
                                            >MapR FS</xref> destinations - Generate events when they
                                        close an output file or complete streaming a whole file. </li>
                                    <li><xref
                                            href="../Processors/Groovy-EventGeneration.dita#concept_qcz_ssq_1y"
                                            >Groovy Evaluator</xref>, <xref
                                            href="../Processors/JavaScript-EventGeneration.dita#concept_mkv_wgh_cy"
                                            >JavaScript Evaluator</xref>, and <xref
                                            href="../Processors/Jython-EventGeneration.dita#concept_zhd_chh_cy"
                                            >Jython Evaluator</xref> processors - Can run scripts
                                        that generate events. </li>
                                    <li><xref
                                            href="../Executors/HDFSFileMeta-EventGeneration.dita#concept_vhl_mfj_rx"
                                            >HDFS File Metadata executor</xref> - Generates events
                                        when it changes file metadata. </li>
                                    <li><xref
                                            href="../Executors/MapReduce-EventGeneration.dita#concept_e1s_sm5_sx"
                                            >MapReduce executor</xref> - Generates events when it
                                        starts a MapReduce job.</li>
                                </ul></li>
                            <li><xref href="../Pipeline_Design/DevStages.dita#concept_czx_ktn_ht"
                                    >Dev stages</xref>. You can use the following stages to develop
                                and test event handling: <ul id="ul_ysc_2hk_fy">
                                    <li>Dev Data Generator enhancement - You can now configure the
                                        Dev Data Generator to generate event records as well as data
                                        records. You can also specify the number of records in a
                                        batch. </li>
                                    <li>To Event - Generates event records using the incoming record
                                        as the body of the event record.</li>
                                </ul></li>
                        </ul></dd>
                </dlentry>
                <dlentry>
                    <dt>Installation</dt>
                    <dd>
                        <ul id="ul_q3x_pvm_3y">
                            <li><xref href="../Installation/Requirements.dita#concept_vzg_n2p_kq"
                                    >Installation requirements</xref>:<ul id="ul_zh5_qvm_3y">
                                    <li>Java requirement - Oracle Java 7 is supported but now
                                        deprecated. Oracle announced the end of public updates for
                                        Java 7 in April 2015. StreamSets recommends migrating to
                                        Java 8, as Java 7 support will be removed in a future Data
                                        Collector release. </li>
                                    <li>File descriptors requirement - Data Collector now requires a
                                        minimum of 32,768 open file descriptors. </li>
                                </ul></li>
                        </ul>
                        <ul id="ul_jtf_ghk_fy">
                            <li><xref
                                    href="../Installation/CoreInstall_Overview.dita#concept_vvw_p3m_s5"
                                    >Core installation</xref> includes the basic stage library only
                                - The core RPM and tarball installations now include the basic stage
                                library only, to allow Data Collector to use less disk space.
                                Install additional stage libraries using the Package Manager for
                                tarball installations or the command line for RPM and tarball
                                installations. <p>Previously, the core installation also included
                                    the Groovy, Jython, and statistics stage libraries.</p></li>
                        </ul>
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>Configuration</dt>
                    <dd>
                        <ul id="ul_v5w_mhk_fy">
                            <li><xref
                                    href="../Installation/AvailableStageLibraries.dita#concept_evs_xkm_s5"
                                    >New stage libraries</xref>. Data Collector now supports the
                                following stage libraries: <ul id="ul_oxv_nhk_fy">
                                    <li>Apache Kudu version 1.0.x - Earlier Kudu versions are no
                                        longer supported. </li>
                                    <li>Cloudera CDH version 5.9 distribution of Apache Hadoop. </li>
                                    <li>Cloudera version 5.9 distribution of Apache Kafka 2.0. </li>
                                    <li>Elasticsearch version 5.0.x. </li>
                                    <li>Google Cloud Bigtable. </li>
                                    <li>Hortonworks HDP version 2.5 distribution of Apache Hadoop. </li>
                                    <li>MySQL Binary Log. </li>
                                    <li>Salesforce. </li>
                                </ul></li>
                            <li><xref
                                    href="../Configuration/LDAP-Configuring.dita#concept_x2j_5ts_g5"
                                    >LDAP authentication</xref> - If you use LDAP authentication,
                                you can now configure Data Collector to connect to multiple LDAP
                                servers. You can also configure Data Collector to support an LDAP
                                deployment where members are defined by uid or by full DN. </li>
                            <li><xref
                                    href="../Configuration/JavaConfig_GarbageCollector.dita#concept_kqh_lj3_vx"
                                    >Java garbage collector</xref> - Data Collector now uses the
                                Concurrent Mark Sweep (CMS) garbage collector by default. You can
                                configure Data Collector to use a different garbage collector by
                                modifying Java configuration options in the Data Collector
                                environment configuration file.</li>
                            <li>Environment variables for <xref
                                    href="../Configuration/JavaConfigOptions.dita#concept_vrx_4fg_qr"
                                    >Java configuration options</xref>. Data Collector now uses
                                three environment variables to define Java configuration options:
                                    <ul id="ul_wym_thk_fy">
                                    <li>SDC_JAVA_OPTS - Includes configuration options for all Java
                                        versions. SDC_JAVA7_OPTS - Includes configuration options
                                        used only when Data Collector is running Java 7.</li>
                                    <li>SDC_JAVA8_OPTS - Includes configuration options used only
                                        when Data Collector is running Java 8. </li>
                                </ul></li>
                            <li><xref
                                    href="../Getting_Started/ConfiguringConsoleSettings.dita#task_r3q_fnx_pr"
                                    >New time zone property</xref> - You can configure the Data
                                Collector UI to use UTC, the browser time zone, or the Data
                                Collector time zone. The time zone property affects how dates and
                                times display in the UI. The default is the browser time zone.</li>
                        </ul>
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>Origins</dt>
                    <dd>
                        <ul id="ul_kq1_whk_fy">
                            <li>New <xref href="../Origins/MySQLBinaryLog.dita#concept_kqg_1yh_xx"
                                    >MySQL Binary Log origin</xref> - Reads MySQL binary logs to
                                generate records with change data capture information. </li>
                            <li>New <xref href="../Origins/Salesforce.dita#concept_odf_vr3_rx"
                                    >Salesforce origin </xref>- Reads data from Salesforce. The
                                origin can execute a SOQL query to read existing data from
                                Salesforce. The origin can also subscribe to the Force.com Streaming
                                API to receive notifications for changes to Salesforce data. </li>
                            <li><xref
                                    href="../Origins/Directory-Subdirectories.dita#concept_qpt_rg3_cy"
                                    >Directory origin</xref> enhancement - You can configure the
                                Directory origin to read files from all subdirectories when using
                                the last-modified timestamp for the read order. </li>
                            <li><xref
                                    href="../Origins/JDBCConsumer-Configuring.dita#task_ryz_tkr_bs"
                                    >JDBC Query Consumer</xref> and <xref
                                    href="../Origins/OracleCDC-Configuring.dita#task_ehh_mjj_tw"
                                    >Oracle CDC Client</xref> origin enhancement - You can now
                                configure the transaction isolation level that the JDBC Query
                                Consumer and Oracle CDC Client origins use to connect to the
                                database. Previously, the origins used the default transaction
                                isolation level configured for the database.</li>
                        </ul>
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>Processors</dt>
                    <dd>
                        <ul id="ul_wfg_13k_fy">
                            <li>New <xref href="../Processors/Spark.dita#concept_cpx_1lm_zx">Spark
                                    Evaluator processor</xref> - Processes data based on a Spark
                                application that you develop. Use the Spark Evaluator processor to
                                develop a Spark application that performs custom processing within a
                                pipeline. </li>
                            <li><xref
                                    href="../Processors/FieldFlattener-FlatField.dita#concept_vpx_zc1_xx"
                                    >Field Flattener processor</xref> enhancements - In addition to
                                flattening the entire record, you can also now use the Field
                                Flattener processor to flatten specific list or map fields in the
                                record. </li>
                            <li><xref
                                    href="../Processors/FieldTypeConverter-ChangeScale.dita#concept_sym_c4g_xx"
                                    >Field Type Converter processor</xref> enhancements - You can
                                now use the Field Type Converter processor to change the scale of a
                                decimal field. Or, if you convert a field with another data type to
                                the Decimal data type, you can configure the scale to use in the
                                conversion. </li>
                            <li><xref href="../Processors/ListPivoter.dita#concept_ekg_313_qw">Field
                                    Pivoter processor</xref> enhancements - The List Pivoter
                                processor has been renamed to the Field Pivoter processor. You can
                                now use the processor to pivot data in a list, map, or list-map
                                field. You can also use the processor to save the field name of the
                                first-level item in the pivoted field. </li>
                            <li><xref
                                    href="../Processors/JDBCLookup-Configuring.dita#task_kbr_2cy_hw"
                                    >JDBC Lookup</xref> and <xref
                                    href="../Processors/JDBCTee-Configuring.dita#task_qpj_ncy_hw"
                                    >JDBC Tee</xref> processor enhancement - You can now configure
                                the transaction isolation level that the JDBC Lookup and JDBC Tee
                                processors use to connect to the database. Previously, the origins
                                used the default transaction isolation level configured for the
                                database. </li>
                            <li>Scripting processor enhancements - The <xref
                                    href="../Processors/Groovy-EventGeneration.dita#concept_qcz_ssq_1y"
                                    >Groovy Evaluator</xref>, <xref
                                    href="../Processors/JavaScript-EventGeneration.dita#concept_mkv_wgh_cy"
                                    >JavaScript Evaluator</xref>, and <xref
                                    href="../Processors/Jython-EventGeneration.dita#concept_zhd_chh_cy"
                                    >Jython Evaluator</xref> processors can generate event records
                                and work with record header attributes. The sample scripts now
                                include examples of both and a new tip for generating unique record
                                IDs. </li>
                            <li><xref
                                    href="../Processors/XMLFlattener-Configuring.dita#task_pmb_l55_sv"
                                    >XML Flattener processor</xref> enhancement - You can now
                                configure the XML Flattener processor to write the flattened data to
                                a new output field. Previously, the processor wrote the flattened
                                data to the same field. </li>
                            <li><xref href="../Processors/XMLParser.dita#concept_dtt_q5q_k5">XML
                                    Parser processor</xref> enhancement. You can now generate
                                records from XML documents using simplified XPath expressions. This
                                enables reading records from deeper within XML documents. </li>
                        </ul>
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>Destination</dt>
                    <dd>
                        <ul id="ul_v5j_l3k_fy">
                            <li>New <xref
                                    href="../Destinations/DataLakeStore.dita#concept_jzm_kf4_zx"
                                    >Azure Data Lake Store destination</xref> - Writes data to
                                Microsoft Azure Data Lake Store. </li>
                            <li>New <xref href="../Destinations/Bigtable.dita#concept_pl5_tmq_tx"
                                    >Google Bigtable destination</xref> - Writes data to Google
                                Cloud Bigtable. </li>
                            <li>New <xref href="../Destinations/Salesforce.dita#concept_rlb_rt3_rx"
                                    >Salesforce destination</xref> - Writes data to Salesforce. New
                                Wave Analytics destination. Writes data to Salesforce Wave
                                Analytics. The destination creates a dataset with external data. </li>
                            <li><xref
                                    href="../Destinations/AmazonS3-Configuring.dita#task_pxb_j3r_rt"
                                    >Amazon S3 destination</xref> change - The AWS KMS Key ID
                                property has been renamed AWS KMS Key ARN. Data Collector upgrades
                                existing pipelines seamlessly. </li>
                            <li>File suffix enhancement. You can now configure a file suffix, such
                                as txt or json, for output files generated by <xref
                                    href="../Destinations/HadoopFS-destination.dita#concept_awl_4km_zq"
                                    >Hadoop FS</xref>, <xref
                                    href="../Destinations/LocalFS.dita#concept_zvc_bv5_1r">Local
                                    FS</xref>, <xref
                                    href="../Destinations/MapRFS.dita#concept_spv_xlc_fv">MapR
                                    FS</xref>, and the <xref
                                    href="../Destinations/AmazonS3.dita#concept_avx_bnq_rt">Amazon
                                    S3</xref> destinations. </li>
                            <li><xref href="../Destinations/JDBCProducer.dita#concept_kvs_3hh_ht"
                                    >JDBC Producer</xref> destination enhancement - You can now
                                configure the transaction isolation level that the JDBC Producer
                                destination uses to connect to the database. Previously, the
                                destination used the default transaction isolation level configured
                                for the database. </li>
                            <li><xref
                                    href="../Destinations/Kudu-DefineOperation.dita#concept_dvg_vvj_wx"
                                    >Kudu destination</xref> enhancement - You can now configure the
                                destination to perform one of the following write operations:
                                insert, update, delete, or upsert.</li>
                        </ul>
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>Data Formats</dt>
                    <dd>
                        <ul id="ul_r3d_dkk_fy">
                            <li><xref href="../Data_Formats/XMLDFormat.dita#concept_lty_42b_dy"
                                    >XML processing</xref> enhancement - You can now generate
                                records from XML documents using <xref
                                    href="../Data_Formats/XMLDF-Xpath.dita#concept_zw2_mfk_dy"
                                    >simplified XPath expressions</xref> with origins that process
                                XML data and the XML Parser processor. This enables reading records
                                from deeper within XML documents. </li>
                            <li>Consolidated data format properties - You now configure the data
                                format and related properties on a new Data Format tab. Previously,
                                data formats had individual configuration tabs, e.g., Avro,
                                Delimited, Log. <p>Related properties, such as Charset, Compression
                                    Format, and Ignore Control Characters now appear on the Data
                                    Format tab as well. </p></li>
                            <li><xref
                                    href="../Data_Formats/WholeFile-IncludingChecksumEvent.dita#concept_ojv_sr4_vx"
                                    >Checksum generation for whole files</xref> - Destinations that
                                stream whole files can now generate checksums for the files so you
                                can confirm the accurate transmission of the file.</li>
                        </ul>
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>Pipeline Maintenance</dt>
                    <dd>
                        <ul id="ul_h24_3kk_fy">
                            <li>Add labels to pipelines from the Home page - You can now add labels
                                to multiple pipelines from the Data Collector Home page. Use labels
                                to group similar pipelines. For example, you might want to group
                                pipelines by database schema or by the test or production
                                environment. </li>
                            <li>Reset the origin for multiple pipelines from the Home page - You can
                                now reset the origin for multiple pipelines at the same time from
                                the Data Collector Home page.</li>
                        </ul>
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>Rules and Alerts</dt>
                    <dd>
                        <ul id="ul_sfk_lkk_fy">
                            <li><xref href="../Alerts/MetricGauge.dita#concept_ky2_g4f_qv">Metric
                                    rules and alerts</xref> enhancements - The gauge metric type can
                                now provide alerts based on the number of input, output, or error
                                records for the last processed batch.</li>
                        </ul>
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>Expression Language Functions</dt>
                    <dd>
                        <ul id="ul_bwr_nkk_fy">
                            <li>New <xref
                                    href="../Expression_Language/FileFunctions.dita#concept_kxj_nyl_5x"
                                    >file functions </xref>- You can use the following new file
                                functions to work with file paths:<ul id="ul_xdq_4kk_fy">
                                    <li>file:fileExtension(&lt;filepath>) - Returns the file
                                        extension from a path. </li>
                                    <li>file:fileName(&lt;filepath>) - Returns a file name from a
                                        path. </li>
                                    <li>file:parentPath(&lt;filepath>) - Returns the parent path of
                                        the specified file or directory. </li>
                                    <li>file:pathElement(&lt;filepath>, &lt;integer>) - Returns the
                                        portion of the file path specified by a positive or negative
                                        integer. </li>
                                    <li>file:removeExtension(&lt;filepath>) - Removes the file
                                        extension from a path. </li>
                                </ul></li>
                            <li>New <xref
                                    href="../Expression_Language/PipelineFunctions.dita#concept_dvg_nqn_wx"
                                    >pipeline functions</xref> - You can use the following new
                                pipeline functions to determine information about a pipeline: <ul
                                    id="ul_lc5_rkk_fy">
                                    <li>pipeline:name() - Returns the pipeline name. </li>
                                    <li>pipeline:version() - Returns the pipeline version when the
                                        pipeline has been published to Dataflow Performance Manager
                                        (DPM). </li>
                                </ul></li>
                            <li>New <xref
                                    href="../Expression_Language/TimeFunctions.dita#concept_qkr_trf_sw"
                                    >time functions</xref> - You can use the following new time
                                functions to transform datetime data:<ul id="ul_znz_skk_fy">
                                    <li> time:extractLongFromDate(&lt;Date object>, &lt;string>) -
                                        Extracts a long value from a Date object, based on the
                                        specified date format. </li>
                                    <li>time:extractStringFromDate(&lt;Date object>, &lt;string>) -
                                        Extracts a string value from a Date object, based on the
                                        specified date format. </li>
                                    <li>time:millisecondsToDateTime(&lt;long>) - Converts an epoch
                                        or UNIX time in milliseconds to a Date object. </li>
                                </ul></li>
                        </ul>
                    </dd>
                </dlentry>
            </dl></p>
 </conbody>
</concept>
