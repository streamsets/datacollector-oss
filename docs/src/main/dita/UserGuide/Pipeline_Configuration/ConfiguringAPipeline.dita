<?xml version="1.0" encoding="UTF-8"?>
<!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

-->
<!DOCTYPE task PUBLIC "-//OASIS//DTD DITA General Task//EN" "generalTask.dtd">
<task id="task_xlv_jdw_kq">
    <title>Configuring a Pipeline</title>
    <shortdesc>Configure a pipeline to define the stream of data. After you configure the pipeline,
        you can start the pipeline. </shortdesc>
    <taskbody>
        <context>
            <p><indexterm>pipelines<indexterm>configuring</indexterm></indexterm><indexterm>pipelines<indexterm>labels</indexterm></indexterm><indexterm>labels<indexterm>pipelines</indexterm></indexterm>A
                pipeline can include the following stages:<ul id="ul_eyy_b2w_kq">
                    <li>A single origin stage</li>
                    <li>Multiple processor stages</li>
                    <li>Multiple destination stages</li>
                    <li>Multiple executor stages</li>
                </ul></p>
        </context>
        <steps id="steps_tsx_d2w_kq">
            <step conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/CreatePipeline1"
                conrefend="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/CreatePipeline2">
                <cmd/>
            </step>
            <step id="pipeProperties">
                <cmd>In the Properties panel, on the <wintitle>General</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_ryh_vfm_zs">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.0*"/>
                            <thead>
                                <row>
                                    <entry>Pipeline Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Title</entry>
                                    <entry>Title of the pipeline. <p><ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            /> uses the alphanumeric characters entered for the
                                            pipeline title as a prefix for the generated pipeline
                                            ID. For example, if you enter “My Pipeline
                                            *&amp;%&amp;^^ 123” as the pipeline title, then the
                                            pipeline ID has the following value:
                                            MyPipeline123tad9f592-5f02-4695-bb10-127b2e41561c.</p><p>You
                                            can edit the pipeline title. However, because the
                                            pipeline ID is used to identify the pipeline, any
                                            changes to the pipeline title are not reflected in the
                                            pipeline ID.</p></entry>
                                </row>
                                <row>
                                    <entry>Description</entry>
                                    <entry>Optional description of the pipeline.</entry>
                                </row>
                                <row>
                                    <entry>Labels</entry>
                                    <entry>Optional labels to assign to the pipeline. <p>Use labels
                                            to group similar pipelines. For example, you might want
                                            to group pipelines by database schema or by the test or
                                            production environment. </p><p
                                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/NestedLabels"
                                        /></entry>
                                </row>
                                <row>
                                    <entry>Execution Mode <xref href="../Cluster_Mode/ClusterPipelines.dita"
                                                ><image href="../Graphics/icon_moreInfo.png"
                                                scale="10" id="image_zfp_n5g_cs"/>
                                        </xref></entry>
                                    <entry>Execution mode of the pipeline:<ul id="ul_bn5_jsz_zr">
                                            <li>Standalone - A single <ph
                                                  conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                                /> process runs the pipeline.</li>
                                            <li>Cluster Batch - <ph
                                                  conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                                /> spawns additional workers as needed to process
                                                data in HDFS or MapR. Processes all available data
                                                and then stops the pipeline.</li>
                                            <li>Cluster Yarn Streaming - <ph
                                                  conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                                /> spawns additional workers as needed to process
                                                data, by default. You can limit the number of
                                                workers with the Worker Count cluster property. And
                                                you can use the Extra Spark Configuration property
                                                to pass Spark configurations to the spark-submit
                                                  script.<p>Use to stream data from a Kafka or MapR
                                                  cluster that uses Spark Streaming on
                                                YARN.</p></li>
                                            <li>Cluster Mesos Streaming - <ph
                                                  conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                                /> spawns additional workers as needed to process
                                                data. Use to stream data from a Kafka cluster that
                                                uses Spark Streaming on Mesos.</li>
                                        </ul></entry>
                                </row>
                                <row>
                                    <entry>Delivery Guarantee <xref
                                            href="../Pipeline_Design/DeliveryGuarantee.dita#concept_ffz_hhw_kq">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_rtg_yfm_zs"/></xref></entry>
                                    <entry>Determines how <ph
                                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                        /> handles data after an unexpected event causes the
                                        pipeline to stop running:<ul id="ul_b4v_51m_sq">
                                            <li>At Least Once - Ensures all data is processed and
                                                written to the destination. Might result in
                                                duplicate rows.</li>
                                            <li>At Most Once - Ensures that data is not reprocessed
                                                to prevent writing duplicate data to the
                                                destination. Might result in missing rows.</li>
                                        </ul><p>Default is At Least Once.</p></entry>
                                </row>
                                <row>
                                    <entry>Start Event</entry>
                                    <entry>Determines how the start event is handled. Select one of
                                        the following options:<ul id="ul_jmt_hht_51b">
                                            <li>Discard - Use when you don't want to use the
                                                event.</li>
                                            <li>An executor - To use the event to trigger a task,
                                                select the executor that you want to use. For more
                                                information about the executors, see <xref
                                                  href="../Executors/Executors-overview.dita#concept_stt_2lk_fx"
                                                />.</li>
                                            <li>Write to Another Pipeline - Use to pass the event to
                                                another pipeline for more complex processing. </li>
                                        </ul><p>For more information about pipeline events, see
                                                <xref
                                                href="../Event_Handling/PipelineEvents.dita#concept_amg_2qr_t1b"
                                            />.</p></entry>
                                </row>
                                <row>
                                    <entry>Stop Event</entry>
                                    <entry>Determines how the stop event is handled. Select one of
                                        the following options:<ul id="ul_gcx_5ht_51b">
                                            <li>Discard - Use when you don't want to use the
                                                event.</li>
                                            <li>An executor - To use the event to trigger a task,
                                                select the executor that you want to use. For more
                                                information about the executors, see <xref
                                                  href="../Executors/Executors-overview.dita#concept_stt_2lk_fx"
                                                />.</li>
                                            <li>Write to Another Pipeline - Use to pass the event to
                                                another pipeline for more complex processing. </li>
                                        </ul><p>For more information about pipeline events, see
                                                <xref
                                                href="../Event_Handling/PipelineEvents.dita#concept_amg_2qr_t1b"
                                            />.</p></entry>
                                </row>
                                <row>
                                    <entry>Retry Pipeline on Error <xref
                                            href="Retry.dita"
                                                ><image href="../Graphics/icon_moreInfo.png"
                                                scale="10" id="image_wms_qvz_2t"/>
                                        </xref></entry>
                                    <entry>Retries the pipeline upon error. </entry>
                                </row>
                                <row>
                                    <entry>Retry Attempts</entry>
                                    <entry>Number of retries attempted. Use -1 to retry
                                        indefinitely. <p>The wait time between retries starts at 15
                                            seconds and doubles until reaching five
                                        minutes.</p></entry>
                                </row>
                                <row>
                                    <entry>Max Pipeline Memory <xref
                                            href="PipelineMemory.dita"
                                                ><image href="../Graphics/icon_moreInfo.png"
                                                scale="10" id="image_ldk_s5g_cs"/>
                                        </xref></entry>
                                    <entry>Maximum amount of memory for the pipeline to use. Used
                                        only when the <ph
                                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                        /> monitor.memory configuration property is set to
                                            true.<p>You can enter a numeric value or edit the
                                            default expression to use a percentage of the <ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            /> Java heap size. </p><p>Default is 65% of the <ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            /> Java heap
                                            size:<codeblock> ${jvm:maxMemoryMB() * 0.65}</codeblock></p></entry>
                                </row>
                                <row>
                                    <entry>On Memory Exceeded</entry>
                                    <entry>Action to take when the pipeline memory reaches the
                                        configured Max Pipeline Memory:<ul id="ul_vsf_3k4_1s">
                                            <li>Log - Logs a message in the pipeline history.</li>
                                            <li>Log and Alert - Logs a message and triggers an alert
                                                that displays in Monitor mode and sends an alert
                                                email to any provided email addresses.</li>
                                        </ul><ul id="ul_mtf_3k4_1s">
                                            <li>Log, Alert and Stop Pipeline - Logs a message,
                                                triggers an alert that displays in Monitor mode and
                                                sends an alert email to any provided email
                                                addresses. Stops the pipeline. This option is not
                                                supported for cluster mode pipelines at this time. </li>
                                        </ul></entry>
                                </row>
                                <row>
                                    <entry>Rate Limit (records / sec)
                                        <xref
                                            href="PipelineRateLimit.dita"
                                            ><image href="../Graphics/icon_moreInfo.png"
                                                scale="10" id="image_lgk_s6g_cs"/>
                                        </xref></entry>
                                    <entry>Maximum number of records that the pipeline can read in a
                                        second. Use 0 or no value to set no rate limit.<p>Default is
                                            0.</p></entry>
                                </row>
                                <row>
                                    <entry>Max Runners </entry>
                                    <entry>The maximum number of pipeline runners to use in a
                                        multithreaded pipeline. <p>Use 0 for no limit. When set to
                                            0, <ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            /> generates up to the maximum number of threads or
                                            concurrency configured in the origin. </p><p>You can use
                                            this property to help tune pipeline performance. For
                                            more information, see <xref
                                                href="../Multithreaded_Pipelines/Tuning.dita#concept_fmg_pjd_mz"
                                            />.</p><p>Default is 0. </p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>To define runtime parameters, on the <wintitle>Parameters</wintitle> tab, click
                    the <uicontrol>Add</uicontrol> icon and define the name and the default value
                    for each parameter. You can use <xref
                        href="../Pipeline_Configuration/SimpleBulkEdit.dita#concept_alb_b3y_cbb"
                        >simple or bulk edit mode</xref> to add the parameters.</cmd>
                <info>For more information, see <xref
                        href="RuntimeParameters.dita#concept_rjh_ntz_qr"/>.</info>
            </step>
            <step>
                <cmd>To configure notifications based on changes in pipeline state, on the
                        <wintitle>Notifications</wintitle> tab, configure the following
                    properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_zwz_34j_rz">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Notifications Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Notify on Pipeline State Changes <xref
                                            href="Notifications.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_rgb_stj_rz"/></xref></entry>
                                    <entry>Sends notifications when the pipeline encounters the
                                        listed pipeline states. </entry>
                                </row>
                                <row>
                                    <entry>Email IDs</entry>
                                    <entry>Email addresses to receive notification when the pipeline
                                        state changes to one of the specified states. Using <xref
                                            href="../Pipeline_Configuration/SimpleBulkEdit.dita#concept_alb_b3y_cbb"
                                            >simple or bulk edit mode</xref>, click the
                                            <uicontrol>Add</uicontrol> icon to add additional
                                        addresses. </entry>
                                </row>
                                <row>
                                    <entry
                                        conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/WH-Webhook-entry"/>
                                    <entry>Webhook to send when the pipeline state changes to one of
                                        the specified states. Using <xref
                                            href="../Pipeline_Configuration/SimpleBulkEdit.dita#concept_alb_b3y_cbb"
                                            >simple or bulk edit mode</xref>, click the
                                            <uicontrol>Add</uicontrol> icon to add additional
                                        webhooks.</entry>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/WH-URL-row">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/WH-Header-row">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/WH-HTTPMethod-row">
                                    <entry/>
                                </row>
                                <row>
                                    <entry
                                        conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/WH-Payload-entry"/>
                                    <entry>Optional payload to use. Available for PUT, POST, and
                                        DELETE methods. <p>Use any valid content type.</p><p>You can
                                            use webhook parameters in the payload to include
                                            information about the triggering event, such as the
                                            pipeline name or state. Enclose webhook parameters in
                                            double curly brackets as follows:
                                                <codeph>{{PIPELINE_STATE}}</codeph>.</p></entry>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/WH-ContentType-row">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/WH-AuthenticationType-row">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/WH-UserName-row">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/WH-Password-row">
                                    <entry/>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>Click the <wintitle>Error Records</wintitle> tab and configure the following
                    error handling options:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_ghf_x22_br">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.25*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.25*"/>
                            <thead>
                                <row>
                                    <entry>Error Records Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Error Records <xref
                                            href="../Pipeline_Design/ErrorHandling.dita#concept_pm4_txm_vq">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                            /></xref></entry>
                                    <entry>Determines how to handle records that cannot be processed
                                        as expected. Use one of the following options:<ul
                                            id="ul_ayr_y22_br">
                                            <li>Discard - Discards error records.</li>
                                            <li>Write to Another Pipeline - Writes error records to
                                                another pipeline. To use this option, you need an
                                                SDC RPC destination pipeline to process the error
                                                records. </li>
                                            <li>Write to Elasticsearch - Writes error records to the
                                                specified Elasticsearch cluster.</li>
                                            <li>Write to File - Writes error records to a file in
                                                the specified directory. </li>
                                            <li>Write to Kafka - Writes error records to the
                                                specified Kafka cluster.</li>
                                            <li>Write to Kinesis - Writes error records to the
                                                specified Kinesis stream.</li>
                                            <li>Write to MapR Streams - Writes error records to the
                                                specified MapR cluster.</li>
                                        </ul><p>For cluster mode, Write to File is not supported at
                                            this time. </p></entry>
                                </row>
                                <row>
                                    <entry>Error Record Policy</entry>
                                    <entry>Determines the version of the record to use as a basis
                                        for an error record. For more information, see <xref
                                            href="../Pipeline_Design/ErrorRecords.dita#concept_itr_mzw_j1b"
                                        />.</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>When writing error records to an SDC RPC pipeline, click the <wintitle>Error
                        Records - Write to Another Pipeline</wintitle> tab and configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_abt_n11_ft">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Write to Pipeline Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/row-RPCconnect">
                                    <entry/>
                                </row><row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/row-RPCID">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/row-RetriesBatch">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/row-BackOffPeriod">
                                    <entry/>
                                </row>
                                
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/row-ConTimeout">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/row-ReadTimeout">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/row-UseCompression">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/TLS-EnableTLS-row">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/TLS-TruststoreFile-row">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/TLS-TruststoreType-row">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/TLS-TruststorePassword-row">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/TLS-TruststoreKeyAlgo-row">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/TLS-DefaultProtocols-row">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/TLS-TransportProtocols-row">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/TLS-DefaultCipherSuites-Row">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/TLS-CipherSuites-row">
                                    <entry/>
                                </row>   
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>When writing error records to Elasticsearch, click the <uicontrol>Error Records
                        - Write to Elasticsearch</uicontrol> tab and configure the following
                    properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_aln_mxx_ty">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Elasticsearch Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/row_ElasticCluster"
                                    conrefend="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/row_ElasticCharset">
                                    <entry/>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
                <info>If you enabled security, configure the following security property:</info>
                <info
                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/ElasticSHIELD-Info"/>
            </step>
            <step>
                <cmd>When writing error records to file, click the <wintitle>Error Records - Write
                        to File</wintitle> tab and configure the following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_gy3_1y4_1r">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Write to File Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Directory</entry>
                                    <entry>Local directory for error record files. </entry>
                                </row>
                                <row>
                                    <entry>File Prefix</entry>
                                    <entry>Prefix used for error record files. Use to differentiate
                                        error record files from other files in the directory.<p>Uses
                                            the prefix sdc-${sdc:id()} by default. The prefix
                                            evaluates to sdc-&lt;<ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            /> ID>. This provides default differentiation in case
                                            several <ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            />s write to the same directory. </p><p>The <ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            /> ID is stored in the following file:
                                                <filepath>$SDC_DATA/sdc.id</filepath> file. For more
                                            information about environment variables, see <xref
                                                href="../Configuration/DCEnvironmentConfig.dita#concept_rng_qym_qr"
                                            />. </p></entry>
                                </row>
                                <row>
                                    <entry>File Wait Time (secs)</entry>
                                    <entry>Number of seconds <ph
                                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                        /> waits for error records. After that time, it creates a
                                        new error record file. <p>You can enter a number of seconds
                                            or use the default expression to enter the time in
                                            minutes. </p></entry>
                                </row>
                                <row>
                                    <entry>Max File Size (MB)</entry>
                                    <entry>Maximum size for error files. Exceeding this size creates
                                        a new error file. <p>Use 0 to avoid using this
                                        property.</p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>When writing error records to Kafka, click the <wintitle>Error Records - Write
                        to Kafka</wintitle> tab and configure the following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_lb5_svh_ms">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.25*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.25*"/>
                            <thead>
                                <row>
                                    <entry>Write to Kafka Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/KPBrokerURI">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/KPRuntimeTopic">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/KPTopicEx">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/KPTopicWList">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/KPTopic">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/KPPartStrategy">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/KPPartExpr">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/KPOneMessPBatch">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/KPKConfigs">
                                    <entry/>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>When writing to Kinesis, click the <wintitle>Error Records - Write to
                        Kinesis</wintitle> tab and configure the following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_cgq_yn4_yr">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Kinesis Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Access Key ID <xref
                                            href="../Destinations/KinProducer-AWSCredentials.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_v86_wcr_yv"/>
                                        </xref></entry>
                                    <entry
                                        conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/AWSDest_AccessKeyID"
                                    />
                                </row>
                                <row>
                                    <entry>Secret Access Key <xref
                                        href="../Destinations/KinProducer-AWSCredentials.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_v74_wcr_yv"/>
                                        </xref></entry>
                                    <entry
                                        conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/AWSDest_SecretAccessKey"
                                    />
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/rowKinesisRegion">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/rowKinesisEndpoint">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/rowKinesisStreamName">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/rowKinesisPartitionStrategy">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/rowKinesisPartitionExp">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/rowKinesisConfig">
                                    <entry/>
                                </row>
                                <row>
                                    <entry>Preserve Record Order</entry>
                                    <entry>Select to preserve the order of records. Enabling this
                                        option can reduce pipeline performance.</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>When writing data to a MapR Streams cluster, click the <wintitle>Error Records
                        - Write to MapR Streams</wintitle> tab and configure the following
                    properties:</cmd>
                <info
                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/MapRStreams-Info"/>
            </step>
            <step>
                <cmd>When using the cluster execution mode, click the <uicontrol>Cluster</uicontrol>
                    tab and configure the following properties:</cmd>
                <info>For Spark Streaming on Mesos, configure the following properties:<table
                        frame="all" rowsep="1" colsep="1" id="table_vb1_dsj_f5">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Mesos Cluster Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Dispatcher URL</entry>
                                    <entry>Master URL of the Mesos dispatcher. For example,
                                        mesos://dispatcher:7077.</entry>
                                </row>
                                <row>
                                    <entry>Checkpoint Configuration Directory <xref
                                            href="../Cluster_Mode/CheckpointStorage.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                            /></xref></entry>
                                    <entry>Location of the HDFS configuration files that specify
                                        whether to write checkpoint metadata to HDFS or Amazon
                                            S3.<p>Use a directory or symlink within the <ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            /> resources directory.</p><p>The directory should
                                            include the following files:<ul
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/HDFSfiles_HDFSdest"
                                                id="ul_qnc_jtt_bt">
                                                <li/>
                                            </ul></p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table></info>
                <info id="YarnStreaming-WorkerCount">For Spark Streaming or MapReduce on YARN,
                    configure the following properties. <table frame="all" rowsep="1" colsep="1"
                        id="table_mkj_kdr_wr">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Yarn Cluster Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Worker Count</entry>
                                    <entry>Number of workers used in a Cluster Yarn Streaming
                                        pipeline. Use to limit the number of workers spawned for
                                        processing. By default, one worker is spawned for every
                                        partition in the topic. <p>Default is 0 for one worker for
                                            each partition. </p></entry>
                                </row>
                                <row>
                                    <entry>Worker Java Options</entry>
                                    <entry>Additional Java properties for the pipeline. Separate
                                        properties with a space.<p>The following properties are set
                                            by default. </p><p>
                                            <ul id="ul_hf3_xqj_ws">
                                                <li>XX:+UseConcMarkSweepGC and XX:+UseParNewGC are
                                                  set to the Concurrent Mark Sweep (CMS) garbage
                                                  collector.</li>
                                                <li>Dlog4j.debug enables debug logging for
                                                  log4j.</li>
                                            </ul>
                                        </p><p>Changing the default properties is not
                                            recommended.</p><p>You can add any valid Java property.
                                        </p></entry>
                                </row>
                                <row>
                                    <entry>Launcher Env Configuration</entry>
                                    <entry>
                                        <p>Additional configuration properties for the cluster
                                            launcher. Using <xref
                                                href="../Pipeline_Configuration/SimpleBulkEdit.dita#concept_alb_b3y_cbb"
                                                >simple or bulk edit mode</xref>, click the
                                                <uicontrol>Add</uicontrol> icon and define the
                                            property name and value. </p>
                                    </entry>
                                </row>
                                <row>
                                    <entry>Worker Memory (MB)</entry>
                                    <entry>Maximum amount of memory allocated to each <ph
                                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                        /> worker in the cluster.<p>Default is 1024 MB.</p></entry>
                                </row>
                                <row>
                                    <entry>Extra Spark Configuration</entry>
                                    <entry>For Cluster Yarn Streaming pipelines, you can configure
                                        additional Spark configurations to pass to the spark-submit
                                        script. Enter the Spark configuration name and the value to
                                        use. <p>The specified configurations are passed to the
                                            spark-submit script as
                                            follows:<codeblock>spark-submit --conf &lt;key>=&lt;value></codeblock></p><p>For
                                            example, to limit the off-heap memory allocated to each
                                            executor, you can use the
                                                <codeph>spark.yarn.executor.memoryOverhead</codeph>
                                            configuration and set it to the number of MB that you
                                            want to use. </p><p><ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            /> does not validate the property names or values.
                                            </p><p>For details on additional Spark configurations
                                            that you can use, see the Spark documentation for the
                                            Spark version that you are using. </p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table></info>
            </step>
            <step>
                <cmd>If you have registered the <ph
                        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                    /> to work with <ph
                        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/DPM-LongOnly"
                    /> (<ph
                        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/DPM-short"
                    />), you can configure the pipeline to aggregate statistics on the
                        <uicontrol>Statistics</uicontrol> tab. </cmd>
                <info>
                    <p>For information on configuring a pipeline to aggregate statistics for <ph
                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/DPM-short"
                        />, see <xref href="../DPM/AggregatedStatistics.dita#concept_h2q_mb5_xw"
                        />.</p>
                </info>
            </step>
            <step>
                <cmd>If you are using the pipeline start or stop events, configure the related event
                    consumer properties on the <uicontrol>&lt;event type> - &lt;event
                        consumer></uicontrol> tab.</cmd>
                <info>For details on the Amazon S3 executor, see <xref
                        href="../Executors/AmazonS3-Configuring.dita#task_nky_cnm_f1b/step-S3tab"
                        >Configuring an Amazon S3 Executor</xref>.<p>For details on the Email
                        executor, see <xref
                            href="../Executors/Email-Configuring.dita#task_pyx_tfp_qz/step-Emailtab"
                            >Configuring an Email Executor</xref>.</p><p>For details on the HDFS
                        File Metadata executor, see <xref
                            href="../Executors/HDFSFileMeta-Configuring.dita#task_m3v_5lk_fx/step-HDFSFiletab"
                            >Configuring an HDFS File Metadata Executor</xref>.</p><p>For details on
                        the Hive Query executor, see <xref
                            href="../Executors/HiveQuery-Configuring.dita#task_mgm_4lk_fx/step-Hivetab"
                            >Configuring a Hive Query Executor</xref>.</p><p>For details on the JDBC
                        Query executor, see <xref
                            href="../Executors/JDBCQuery-Configuring.dita#task_ym2_3cv_sx/step-JDBCQuerytab"
                            >Configuring a JDBC Query Executor</xref>.</p><p>For details on the
                        MapReduce executor, see <xref
                            href="../Executors/MapReduce-Configuring.dita#task_olh_bmk_fx/step-MapReducetab"
                            >Configuring a MapReduce Executor</xref>.</p><p>For details on the Shell
                        executor, see <xref
                            href="../Executors/Shell-Configuring.dita#task_hyc_3zw_tz/step-ShellEnvtab"
                            >Configuring a Shell Executor</xref>.</p><p>For details on the Spark
                        executor, see <xref
                            href="../Executors/Spark-Configuring.dita#task_cdw_wxb_1z/step-Sparktab"
                            >Configuring a Spark Executor</xref>. </p><p>For details on writing to
                        another pipeline, see <xref
                            href="../Destinations/RPCdest-Configuring.dita#task_nbl_r2x_dt/step-SDCRPCdesttab"
                            >Configuring an SDC RPC Destination</xref>.</p></info>
            </step>
            <step>
                <cmd>Use the <uicontrol>Stage Library</uicontrol> to add an origin stage. In the
                    Properties panel, configure the stage properties.</cmd>
                <info>For configuration details about origin stages, see <xref
                        href="../Origins/Origins_overview.dita#concept_hpr_twm_jq"/>.</info>
            </step>
            <step>
                <cmd>Use the <uicontrol>Stage Library</uicontrol> to add the next stage that you
                    want to use, connect the origin to the new stage, and configure the new
                    stage.</cmd>
                <info>For configuration details about processors, see <xref
                        href="../Processors/Processors_overview.dita#concept_hpr_twm_jq"/>.<p>For
                        configuration details about destinations, see <xref
                            href="../Destinations/Destinations_overview.dita#concept_hpr_twm_jq"
                        />.</p></info>
            </step>
            <step>
                <cmd>Add additional stages as necessary.</cmd>
            </step>
            <step>
                <cmd>At any point, you can use the <uicontrol>Preview</uicontrol> icon to preview
                    data to help .configure the pipeline. For more information, see <xref
                        href="../Data_Preview/DataPreview.dita#concept_jtn_s3m_lq"/>. </cmd>
            </step>
            <step>
                <cmd>Optionally, you can create metric or data alerts to track details about a
                    pipeline run and create threshold alerts. For more information, see <xref
                        href="../Alerts/RulesAlerts_title.dita#concept_pgk_brx_rr"/>.</cmd>
            </step>
            <step>
                <cmd>When the pipeline is complete, use the <uicontrol>Start</uicontrol> icon to run
                    the pipeline. </cmd>
            </step>
        </steps>
        <result>When <ph
                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>
            starts the pipeline, Monitor mode displays real-time statistics for the pipeline. For
            more information about monitoring, see <xref
                href="../Pipeline_Monitoring/PipelineMonitoring.dita#concept_hsp_tnt_lq"/>.</result>
    </taskbody>
</task>
