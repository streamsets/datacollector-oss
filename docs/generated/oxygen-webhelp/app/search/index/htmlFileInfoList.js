define(function () {
return ["datacollector/UserGuide/Administration/Administration_title.html@@@Administration@@@StreamSets Accounts enables users without an enterprise account to download and use the latest version of Data Collector and Transformer. StreamSets Accounts also enables logging into linked Data...","datacollector/UserGuide/Alerts/RulesAlerts_title.html@@@Rules and Alerts@@@Metric rules and alerts provide notifications about real-time statistics for pipelines...","datacollector/UserGuide/Apx-DataFormats/DataFormat_Title.html@@@Data Formats by Stage@@@The following table lists the data formats supported by each origin. Origin Avro Binary Datagram Delimited Excel JSON Log Protobuf SDC Record Text Whole File XML Amazon S3 Amazon SQS Consumer \u00A0 \u00A0...","datacollector/UserGuide/Apx-GrokPatterns/GrokPatterns_title.html@@@Grok Patterns@@@You can use the grok patterns in this appendix to define the structure of log data...","datacollector/UserGuide/Apx-RegEx/RegEx-Title.html@@@Regular Expressions@@@A regular expression, also known as regex, describes a pattern for a string...","datacollector/UserGuide/Cluster_Mode/AmazonS3Requirements.html@@@Amazon S3 Requirements@@@Cluster EMR batch and cluster batch mode pipelines can process data from Amazon S3...","datacollector/UserGuide/Cluster_Mode/ClusterPipelines.html@@@Cluster Pipeline Overview@@@A cluster pipeline is a pipeline that runs in cluster execution mode. You can run a pipeline in standalone execution mode or cluster execution mode...","datacollector/UserGuide/Cluster_Mode/ClusterPipelines_title.html@@@Cluster Pipelines@@@...","datacollector/UserGuide/Cluster_Mode/HDFSRequirements.html@@@HDFS Requirements@@@Cluster mode pipelines that read from HDFS require the Cloudera distribution of Hadoop (CDH) or Hortonworks Data Platform (HDP). For a list of the supported CDH or HDP versions, see Available Stage...","datacollector/UserGuide/Cluster_Mode/KafkaRequirements.html@@@Kafka Cluster Requirements@@@Cluster mode pipelines that read from a Kafka cluster have the following requirements: Component Requirement Spark Streaming for cluster streaming modes Spark version 2.1 or later Apache Kafka Spark...","datacollector/UserGuide/Cluster_Mode/MapRRequirements.html@@@MapR Requirements@@@Complete the following steps to configure a cluster pipeline to read from MapR in cluster streaming mode...","datacollector/UserGuide/Cluster_Mode/StageLimitations.html@@@Cluster Pipeline Limitations@@@Please note the following limitations in cluster pipelines: Non-cluster origins - Do not use non-cluster origins in cluster pipelines. For a description of the origins to use, see Cluster Batch and...","datacollector/UserGuide/Configuration/Authentication.html@@@User Authentication@@@If your organization uses LDAP and you want multiple users to access Data Collector, you can configure Data Collector to use LDAP authentication. After you configure LDAP authentication, users log in to Data Collector using their LDAP username and password...","datacollector/UserGuide/Configuration/Config_title.html@@@Configuration@@@...","datacollector/UserGuide/Configuration/CredentialStores.html@@@Credential Stores@@@You can configure Data Collector to use one or more credential stores. Each credential store is identified by a unique credential store ID...","datacollector/UserGuide/Configuration/CustomStageLibraries.html@@@Custom Stage Libraries@@@If you develop custom stages, store the stage libraries in a local directory external to the Data Collector installation directory. Use an external directory to enable use of the custom stage...","datacollector/UserGuide/Configuration/DCConfig.html@@@Data Collector Configuration@@@You can edit the Data Collector configuration file, $SDC_CONF/sdc.properties, to configure properties such as the host name and port number and account information for email alerts...","datacollector/UserGuide/Configuration/DCEnvironmentConfig.html@@@Data Collector Environment Configuration@@@Data Collector includes environment variables that define the directories used to store configuration,\n        data, log, and resource files...","datacollector/UserGuide/Configuration/ExternalLibs.html@@@Install External Libraries@@@By default, external libraries are installed to the\n            $SDC_DIST/streamsets-libs-extras directory...","datacollector/UserGuide/Configuration/HTTP_protocols.html@@@Enabling HTTPS@@@Create a keystore file that includes each private key and public certificate pair signed by the CA. A keystore is used to verify the identity of the client upon a request from an SSL/TLS server...","datacollector/UserGuide/Configuration/JMXMetrics-EnableExternalTools.html@@@Enabling External JMX Tools@@@Data Collector uses JMX metrics to generate the graphical display of the status of a running pipeline. You can provide the same JMX metrics to external tools if desired...","datacollector/UserGuide/Configuration/PublishMetadata.html@@@Working with Data Governance Tools@@@You can configure Data Collector to integrate with data governance tools, giving you visibility into data movement - where the data came from, where it\u2019s going to, and who is interacting with it...","datacollector/UserGuide/Configuration/ReverseProxy.html@@@Using a Reverse Proxy@@@You can use Data Collector with a reverse proxy server, such as Nginx. To use a reverse proxy server, you simply configure the reverse proxy server to work with Data Collector . You might use a...","datacollector/UserGuide/Configuration/RolesandPermissions.html@@@Roles and Permissions@@@Data Collector allows you to assign roles and pipeline permissions to users and groups. Roles, such as Creator or Manager, enable users to perform different Data Collector tasks. Each user needs at...","datacollector/UserGuide/Configuration/Vault-Overview.html@@@Accessing Hashicorp Vault Secrets with Vault Functions (Deprecated)@@@Data Collector can use Hashicorp Vault functions to access information, called secrets , stored in Hashicorp Vault. However, the Vault functions are now deprecated and will be removed in a future...","datacollector/UserGuide/DPM/AggregatedStatistics.html@@@Pipeline Statistics@@@Pipelines can run in standalone, cluster, or edge execution mode. Some pipeline execution modes do not support all statistics aggregator options...","datacollector/UserGuide/DPM/DPM.html@@@Meet StreamSets Control Hub@@@StreamSets Control HubTM is a central point of control for all of your dataflow pipelines. Control Hub allows teams to build and execute large numbers of complex dataflows at scale...","datacollector/UserGuide/DPM/DPMConfiguration.html@@@Control Hub Configuration File@@@You can customize how a registered Data Collector works with StreamSets Control Hub by editing the Control Hub configuration file, $SDC_CONF/dpm.properties, located in the Data Collector installation...","datacollector/UserGuide/DPM/DPM_title.html@@@StreamSets Control Hub@@@...","datacollector/UserGuide/DPM/OrgUserAccount.html@@@Request a Control Hub Organization and User Account@@@To register a Data Collector with StreamSets Control Hub,\n        you must have a Control Hub user account within an organization...","datacollector/UserGuide/DPM/PipelineManagement.html@@@Pipeline Management with Control Hub@@@After you register a Data Collector with StreamSets Control Hub,\n        you can manage how the pipelines work with Control Hub...","datacollector/UserGuide/DPM/RegisterSDCwithDPM.html@@@Register Data Collector with Control Hub@@@You must register a Data Collector to work with StreamSets Control Hub.\n        When you register a Data Collector, Data Collector generates an authentication token that it uses to issue authenticated requests to Control Hub...","datacollector/UserGuide/DPM/UnregisterSDCwithDPM.html@@@Unregister Data Collector from Control Hub@@@You can unregister a Data Collector from StreamSets Control Hub when you no longer want to use that Data Collector installation with Control Hub...","datacollector/UserGuide/DPM/WorkingWithDPM.html@@@Working with Control Hub@@@Before you can work with StreamSets Control Hub, you must have an Control Hub organization and user account...","datacollector/UserGuide/Data_Formats/Avro.html@@@Avro Data Format@@@Data Collector can read and write Avro data. When reading Avro data, file- and object-based origins, such as the Directory and Amazon S3 origins, generate a Data Collector record for every Avro record...","datacollector/UserGuide/Data_Formats/Binary.html@@@Binary Data Format@@@When reading binary data, origins generate a record with a single byte array field at the root of the record. When the data exceeds the user-defined maximum data size, the origin cannot process the...","datacollector/UserGuide/Data_Formats/DataFormats-Overview.html@@@Data Formats Overview@@@Origins and processors that read files can read uncompressed files, compressed files,\n    archives, and compressed archives...","datacollector/UserGuide/Data_Formats/DataFormats-Title.html@@@Data Formats@@@...","datacollector/UserGuide/Data_Formats/Datagram.html@@@Datagram Data Format@@@When reading datagram data, origins generate a record for every message. The origins can process collectd messages, NetFlow 5 and NetFlow 9 messages , and the following types of syslog messages: RFC...","datacollector/UserGuide/Data_Formats/Delimited.html@@@Delimited Data Format@@@Data Collector can read and write delimited data. Origins that read delimited data generate a record for each delimited line in a file, object, or message. Processors that process delimited data...","datacollector/UserGuide/Data_Formats/Excel.html@@@Excel Data Format@@@You can use file-based origins, such as Directory and Google Cloud Storage , to process Microsoft Excel .xls or .xlsx files. When processing Excel files, an origin creates a record for every row in...","datacollector/UserGuide/Data_Formats/LogFormats.html@@@Log Data Format@@@When you use an origin to read log data, you define the format of the log files to be read...","datacollector/UserGuide/Data_Formats/NetFlow_Overview.html@@@NetFlow Data Processing@@@You can use Data Collector to process NetFlow 5 and NetFlow 9 data. When processing NetFlow 5 data, Data Collector processes flow records based on information in the packet header. Data Collector...","datacollector/UserGuide/Data_Formats/Protobuf-Prerequisites.html@@@Protobuf Data Format Prerequisites@@@Perform the following prerequisites before reading or writing protobuf data...","datacollector/UserGuide/Data_Formats/SDCRecordFormat.html@@@SDC Record Data Format@@@SDC Record is a proprietary data format that Data Collector uses to generate error records. Data Collector can also use the data format to read and write data...","datacollector/UserGuide/Data_Formats/TextCDelim.html@@@Text Data Format with Custom Delimiters@@@By default, the text data format creates records based on line breaks, creating a record for each line of text. You can configure origins to create records based on custom delimiters. Use custom...","datacollector/UserGuide/Data_Formats/WholeFile.html@@@Whole File Data Format@@@You can use the whole file data format to transfer entire files from an origin system to a destination system. With the whole file data format, you can transfer any type of file...","datacollector/UserGuide/Data_Formats/WritingXML.html@@@Writing XML Data@@@When writing XML data, destinations create a valid XML document for each record. The destination requires the record to have a single root field that contains the rest of the record data. When writing...","datacollector/UserGuide/Data_Formats/XMLDFormat.html@@@Reading and Processing XML Data@@@You can include the root element in the generated record by enabling the Preserve Root Element property...","datacollector/UserGuide/Data_Preview/DataPreview_Title.html@@@Data Preview@@@You can preview complete and incomplete pipelines and Control Hub pipeline fragments. The Data Preview icon becomes active when data preview is available...","datacollector/UserGuide/Destinations/ADLS-G1-D.html@@@Azure Data Lake Storage Gen1@@@Supported pipeline types: Data Collector The Azure Data Lake Storage Gen1 destination writes data to Microsoft Azure Data Lake Storage Gen1. You can use the Azure Data Lake Storage Gen1 destination in...","datacollector/UserGuide/Destinations/ADLS-G2-D.html@@@Azure Data Lake Storage Gen2@@@The Azure Data Lake Storage Gen2 destination provides several ways to authenticate connections to Azure. Depending on the authentication method that you use, the destination requires different authentication details...","datacollector/UserGuide/Destinations/Aerospike.html@@@Aerospike@@@Supported pipeline types: Data Collector The Aerospike destination writes data to Aerospike. The destination can write to a single Aerospike node or to a cluster of Aerospike nodes. When you configure...","datacollector/UserGuide/Destinations/AmazonS3.html@@@Amazon S3@@@Supported pipeline types: Data Collector Data Collector Edge The Amazon S3 destination writes data to Amazon S3. To write data to an Amazon Kinesis Firehose delivery system, use the Kinesis Firehose...","datacollector/UserGuide/Destinations/AzureEventHubProducer.html@@@Azure Event Hub Producer@@@Supported pipeline types: Data Collector The Azure Event Hub Producer writes data to Microsoft Azure Event Hub. To write to Microsoft Azure Data Lake Storage, use the Azure Data Lake Storage...","datacollector/UserGuide/Destinations/AzureIoTHub.html@@@Azure IoT Hub Producer@@@Data Collector functions as a simulated device that sends messages to Azure IoT Hub. Before you configure the Azure IoT Hub Producer destination, register Data Collector as a device in your IoT hub...","datacollector/UserGuide/Destinations/AzureSynapse.html@@@Azure Synapse SQL@@@You must install the Azure Synapse stage library before using the Azure Synapse SQL destination...","datacollector/UserGuide/Destinations/BigQuery.html@@@Google BigQuery@@@The Google BigQuery destination maps fields from records to BigQuery columns in existing tables based on matching names and compatible data types. If needed, the destination converts Data Collector data types to BigQuery data types...","datacollector/UserGuide/Destinations/Bigtable.html@@@Google Bigtable@@@The Google Bigtable destination requires the BoringSSL library. You must download and install the external library so that the Google Bigtable destination can access it...","datacollector/UserGuide/Destinations/Cassandra.html@@@Cassandra@@@Supported pipeline types: Data Collector The Cassandra destination writes data to a Cassandra cluster. When you configure the Cassandra destination, you define connection information and map incoming...","datacollector/UserGuide/Destinations/CoAPClient.html@@@CoAP Client@@@Supported pipeline types: Data Collector Data Collector Edge Constrained Application Protocol (CoAP) is a web transfer protocol designed for machine-to-machine devices. The CoAP Client destination...","datacollector/UserGuide/Destinations/Couchbase.html@@@Couchbase@@@Supported pipeline types: Data Collector The Couchbase destination writes data to Couchbase Server. Couchbase Server is a distributed NoSQL document-oriented database. The destination writes each...","datacollector/UserGuide/Destinations/DataLakeStore.html@@@Azure Data Lake Storage (Legacy) (Deprecated)@@@Supported pipeline types: Data Collector The Azure Data Lake Storage (Legacy) destination writes data to Microsoft Azure Data Lake Storage Gen1. Important: The Azure Data Lake Storage (Legacy)...","datacollector/UserGuide/Destinations/DeltaLake.html@@@Databricks Delta Lake@@@You must install the Databricks stage library before using the Databricks Delta Lake destination. The Databricks stage library includes the Databricks JDBC driver that the destination uses to access Delta Lake tables on Databricks...","datacollector/UserGuide/Destinations/Destinations-title.html@@@Destinations@@@...","datacollector/UserGuide/Destinations/Destinations_overview.html@@@Destinations@@@A destination stage represents the target for a pipeline. You can use one or more destinations in a pipeline...","datacollector/UserGuide/Destinations/Elasticsearch.html@@@Elasticsearch@@@Supported pipeline types: Data Collector The Elasticsearch destination writes data to an Elasticsearch cluster, including Elastic Cloud clusters (formerly Found clusters) and Amazon Elasticsearch...","datacollector/UserGuide/Destinations/Flume.html@@@Flume@@@Supported pipeline types: Data Collector The Flume destination writes data to a Flume source. When you write data to Flume, you pass data to a Flume client. The Flume client passes data to hosts based...","datacollector/UserGuide/Destinations/GCS.html@@@Google Cloud Storage@@@Supported pipeline types: Data Collector The Google Cloud Storage destination writes data to objects in Google Cloud Storage. You can use other destinations to write to Google BigQuery , Google...","datacollector/UserGuide/Destinations/GPSS.html@@@GPSS Producer@@@Supported pipeline types: Data Collector The GPSS Producer destination writes data to Greenplum Database through a Greenplum Stream Server (GPSS). When you configure the GPSS Producer destination, you...","datacollector/UserGuide/Destinations/HBase.html@@@HBase@@@When you configure the HBase destination, you map fields from records to HBase columns...","datacollector/UserGuide/Destinations/HTTPClient.html@@@HTTP Client@@@Supported pipeline types: Data Collector Data Collector Edge The HTTP Client destination writes data to an HTTP endpoint. The destination sends requests to an HTTP resource URL. Use the HTTP Client...","datacollector/UserGuide/Destinations/HadoopFS-destination.html@@@Hadoop FS@@@Supported pipeline types: Data Collector The Hadoop FS destination writes data to Hadoop Distributed File System (HDFS). You can also use the destination to write to Azure Blob storage. You can write...","datacollector/UserGuide/Destinations/Hive.html@@@Hive Streaming@@@Supported pipeline types: Data Collector The Hive Streaming destination writes data to Hive tables stored in the ORC (Optimized Row Columnar) file format. The Hive Streaming destination requires Hive...","datacollector/UserGuide/Destinations/HiveMetastore.html@@@Hive Metastore@@@The Hive Metastore destination queries Hive for information and caches the results. When possible, it uses the cache to avoid unnecessary Hive queries...","datacollector/UserGuide/Destinations/InfluxDB.html@@@InfluxDB@@@Supported pipeline types: Data Collector Data Collector Edge The InfluxDB destination writes data to an InfluxDB database. When you configure the InfluxDB destination, you define connection...","datacollector/UserGuide/Destinations/JDBCProducer.html@@@JDBC Producer@@@The JDBC Producer destination can write data to multiple database vendors...","datacollector/UserGuide/Destinations/JMSProducer.html@@@JMS Producer@@@Before you use the JMS Producer, install the JMS drivers for the implementation that you are using...","datacollector/UserGuide/Destinations/KProducer.html@@@Kafka Producer@@@Kafka Producer can write a record to the topic based on an expression. When Kafka Producer evaluates a record, it calculates the expression based on record values and writes the record to the resulting topic...","datacollector/UserGuide/Destinations/KinFirehose.html@@@Kinesis Firehose@@@The Kinesis Firehose destination writes data to an existing delivery stream in Amazon Kinesis Firehose. Before using the Kinesis Firehose destination, use the AWS Management Console to create a delivery stream to an Amazon S3 bucket or Amazon Redshift table...","datacollector/UserGuide/Destinations/KinProducer.html@@@Kinesis Producer@@@Supported pipeline types: Data Collector Data Collector Edge The Kinesis Producer destination writes data to Amazon Kinesis Streams. The destination can also send responses to a microservice origin...","datacollector/UserGuide/Destinations/KineticaDB.html@@@KineticaDB@@@Supported pipeline types: Data Collector The KineticaDB destination writes data to a table in a Kinetica cluster using the Kinetica bulk inserter. When you configure the KineticaDB destination, you...","datacollector/UserGuide/Destinations/Kudu.html@@@Kudu@@@Supported pipeline types: Data Collector The Kudu destination writes data to a Kudu cluster. When you configure the Kudu destination, you specify the connection information for one or more Kudu...","datacollector/UserGuide/Destinations/LocalFS.html@@@Local FS@@@Supported pipeline types: Data Collector Use the Local FS destination to write records to files in a local file system. When you configure a Local FS destination, you can define a directory template...","datacollector/UserGuide/Destinations/MQTTPublisher.html@@@MQTT Publisher@@@The MQTT Publisher destination writes messages to a single topic on the MQTT broker. Any MQTT client subscribed to that topic receives the messages. A topic is a string that the broker uses to filter messages for each connected client...","datacollector/UserGuide/Destinations/MapRDB.html@@@MapR DB@@@When you configure the MapR DB destination, you map fields from records to MapR DB columns...","datacollector/UserGuide/Destinations/MapRDBJSON.html@@@MapR DB JSON@@@MapR DB uses a row key to uniquely identify each row in a JSON table. The row key is defined by the _id field of the JSON document stored in the row...","datacollector/UserGuide/Destinations/MapRFS.html@@@MapR FS@@@You can use Kerberos authentication to connect to MapR. When you use Kerberos authentication, the Data Collector uses the Kerberos principal and keytab to connect to MapR. By default, Data Collector uses the user account who started it to connect...","datacollector/UserGuide/Destinations/MapRStreamsProd.html@@@MapR Streams Producer@@@MapR Streams Producer can write a record to the topic based on an expression. When MapR Streams Producer evaluates a record, it calculates the expression based on record values and writes the record to the resulting topic...","datacollector/UserGuide/Destinations/MemSQLLoader.html@@@MemSQL Fast Loader@@@You must install the MemSQL stage library before using the MemSQL Fast Loader destination...","datacollector/UserGuide/Destinations/MongoDB.html@@@MongoDB@@@The MongoDB destination can write to a Microsoft Azure Cosmos DB instance configured to use the MongoDB API.  Specifically, the destination can write to a collection in Azure Cosmos DB that has a shard key that matches the name of a field in the Data Collector pipeline...","datacollector/UserGuide/Destinations/NamedPipe.html@@@Named Pipe@@@Use the mkfifo command to create the named pipe on the same machine where Data Collector is installed...","datacollector/UserGuide/Destinations/PubSubPublisher.html@@@Google Pub/Sub Publisher@@@Supported pipeline types: Data Collector The Google Pub/Sub Publisher destination publishes messages to a Google Pub/Sub topic. You can use other destinations to write to Google BigQuery , Google...","datacollector/UserGuide/Destinations/PulsarProducer.html@@@Pulsar Producer@@@If the Pulsar cluster uses security features, you must configure the Pulsar Producer destination to use the same security features to connect to Pulsar...","datacollector/UserGuide/Destinations/RabbitMQ.html@@@RabbitMQ Producer@@@Supported pipeline types: Data Collector RabbitMQ Producer writes AMQP messages to a single RabbitMQ queue. When you configure the destination, you specify the information needed to connect to...","datacollector/UserGuide/Destinations/Redis.html@@@Redis@@@Supported pipeline types: Data Collector The Redis destination writes data to Redis. When you configure the destination, you specify the mode that the destination uses to process records. You also...","datacollector/UserGuide/Destinations/SDC_RPCdest.html@@@SDC RPC@@@In an SDC RPC destination, the RPC connections define where the destination passes data...","datacollector/UserGuide/Destinations/SFTP.html@@@SFTP/FTP/FTPS Client@@@Supported pipeline types: Data Collector The SFTP/FTP/FTPS Client destination writes whole files to a URL using the Secure File Transfer Protocol (SFTP), File Transfer Protocol (FTP), or FTP Secure...","datacollector/UserGuide/Destinations/SQLServerBDCBulk.html@@@SQL Server 2019 BDC Bulk Loader@@@You must install the SQL Server 2019 Big Data Cluster stage library before using the SQL Server 2019 BDC Bulk Loader destination. The SQL Server 2019 Big Data Cluster stage library includes the JDBC driver that the destination uses to access SQL Server 2019 BDC...","datacollector/UserGuide/Destinations/Salesforce.html@@@Salesforce@@@When you configure the Salesforce destination, you can override the default mapping of case-sensitive field names by mapping specific fields in the record to existing fields in the Salesforce object...","datacollector/UserGuide/Destinations/SendResponse.html@@@Send Response to Origin@@@Supported pipeline types: Data Collector The Send Response to Origin destination passes records to the REST Service origin in the pipeline and allows you to specify an HTTP status code for those...","datacollector/UserGuide/Destinations/Snowflake.html@@@Snowflake@@@You must install the Snowflake stage library before using the Snowflake destination.\n        The Snowflake stage library includes the Snowflake JDBC driver that the destination uses to access Snowflake...","datacollector/UserGuide/Destinations/Solr.html@@@Solr@@@The index mode determines how the Solr destination indexes records when writing to Solr.\n  Index mode also determines how the destination handles errors...","datacollector/UserGuide/Destinations/Splunk.html@@@Splunk@@@Splunk requires that the event data and metadata be correctly formatted in the record. If the record is formatted incorrectly, an error occurs and the destination fails to write to Splunk. When you design a pipeline with the Splunk destination, you must ensure that the record sent to the destination uses the required format...","datacollector/UserGuide/Destinations/Syslog.html@@@Syslog@@@A syslog message includes fields such as a timestamp, facility code, severity level,\n        message ID, and the log message itself. You construct the syslog message content by specifying the values for message fields on the Message tab. The content of the log message depends on how you configure the Data Format tab...","datacollector/UserGuide/Destinations/ToError.html@@@To Error@@@Supported pipeline types: Data Collector Data Collector Edge The To Error destination passes records to the pipeline for error handling. Use the To Error destination to send a stream of records to...","datacollector/UserGuide/Destinations/Trash.html@@@Trash@@@Supported pipeline types: Data Collector Data Collector Edge The Trash destination discards records. Use the Trash destination as a visual representation of records discarded from the pipeline. Or...","datacollector/UserGuide/Destinations/WaveAnalytics.html@@@Einstein Analytics@@@In previous releases, you could configure the destination to use an Einstein Analytics dataflow to combine multiple datasets together. However, using dataflows is now deprecated and will be removed in a future release. We recommend configuring the destination to use the append operation to combine data into a single dataset...","datacollector/UserGuide/Destinations/WebSocketClient.html@@@WebSocket Client@@@Supported pipeline types: Data Collector Data Collector Edge The WebSocket Client destination writes data to a WebSocket endpoint. Use the destination to send data to a WebSocket resource URL. The...","datacollector/UserGuide/Edge_Mode/DownloadPipelines.html@@@Downloading Pipelines from SDC Edge@@@When SDC Edge is running and is accessible by the Data Collector machine, you can download edge pipelines from SDC Edge into Data Collector.\n        When you download pipelines from SDC Edge, you download all edge pipelines deployed to that SDC Edge...","datacollector/UserGuide/Edge_Mode/EdgePipelineTypes.html@@@Design Edge Pipelines@@@Edge pipelines run in edge execution mode. You design edge pipelines in Data Collector...","datacollector/UserGuide/Edge_Mode/EdgePipelines_Deploy.html@@@Deploy Pipelines to SDC Edge@@@After designing edge pipelines in Data Collector, you deploy the edge pipelines to SDC Edge installed on an edge device. You run the edge pipelines on SDC Edge...","datacollector/UserGuide/Edge_Mode/EdgePipelines_Manage.html@@@Manage Pipelines on SDC Edge@@@After designing edge pipelines in Data Collector and then deploying the edge pipelines to SDC Edge, you can manage the pipelines on SDC Edge.\n        Managing edge pipelines includes previewing, validating, starting, stopping, and monitoring the pipelines as well as resetting the origin for the pipelines...","datacollector/UserGuide/Edge_Mode/EdgePipelines_Overview.html@@@Meet StreamSets Data Collector Edge@@@StreamSets Data Collector EdgeTM (SDC Edge) is a lightweight execution agent without a UI that runs pipelines on edge devices with limited resources. Use SDC Edge to read data from an edge device or to receive data from another pipeline and then act on that data to control an edge device...","datacollector/UserGuide/Edge_Mode/EdgePipelines_title.html@@@StreamSets Data Collector Edge@@@...","datacollector/UserGuide/Edge_Mode/GettingStartedSamples.html@@@Getting Started with SDC Edge@@@Data Collector Edge (SDC Edge)\n        includes several sample pipelines that make it easy to get started. You simply import one of the sample edge pipelines, create the appropriate Data Collector receiving pipeline, download and install SDC Edge on the edge device, and then run the sample edge pipeline...","datacollector/UserGuide/Edge_Mode/SDCEInstall.html@@@Install SDC Edge@@@Download and install SDC Edge on each edge device where you want to run edge pipelines...","datacollector/UserGuide/Edge_Mode/SDCReceivingPipelines.html@@@Design Data Collector Receiving Pipelines@@@Data Collector receiving pipelines run in the standalone execution mode. You design and run receiving pipelines in Data Collector...","datacollector/UserGuide/Edge_Mode/SDCeAdminister.html@@@Administer SDC Edge@@@Administering SDC Edge involves configuring, starting, shutting down, and viewing logs for the agent. When using StreamSets Control Hub, you can also use the SDC Edge command line interface to register SDC Edge with Control Hub...","datacollector/UserGuide/Edge_Mode/SupportedPlatforms.html@@@Supported Platforms@@@Install Data Collector Edge\n            (SDC Edge)\n        on each edge device where you want to run edge pipelines...","datacollector/UserGuide/Event_Handling/EventFramework-Title.html@@@Dataflow Triggers@@@Dataflow triggers are instructions for the event framework to kick off tasks in response to events that occur in the pipeline. For example, you can use dataflow triggers to start a MapReduce job after...","datacollector/UserGuide/Executors/ADLS-G1-FileMeta.html@@@ADLS Gen1 File Metadata@@@The ADLS Gen1 File Metadata executor changes file metadata, creates an empty file, or removes a file or directory in Azure Data Lake Storage Gen1 each time it receives an event. To perform these tasks...","datacollector/UserGuide/Executors/ADLS-G2-FileMeta.html@@@ADLS Gen2 File Metadata@@@The ADLS Gen2 File Metadata executor changes file metadata, creates an empty file, or removes a file or directory in Azure Data Lake Storage Gen2 each time it receives an event. To perform these tasks...","datacollector/UserGuide/Executors/AmazonS3.html@@@Amazon S3@@@You can use the Amazon S3 executor to create new Amazon S3 objects and write the specified content to the object when the executor receives an event record...","datacollector/UserGuide/Executors/Databricks.html@@@Databricks Job Launcher@@@The Databricks Job Launcher executor can generate events that you can use in an event stream. When you enable event generation, the executor generates events each time it starts a Databricks job...","datacollector/UserGuide/Executors/DatabricksQuery.html@@@Databricks Query@@@You must install the Databricks stage library before using the Databricks Query executor. The Databricks stage library includes the Databricks JDBC driver that the executor uses to access Databricks...","datacollector/UserGuide/Executors/Email.html@@@Email@@@The Email executor sends the configured email to the specified recipients upon receiving an event. You can also configure the executor to send an email based on a condition, such as the arrival of a...","datacollector/UserGuide/Executors/Executors-overview.html@@@Executors@@@An executor stage triggers a task when it receives an event. Use executors as part of a dataflow trigger in an event stream to perform event-driven, pipeline-related tasks, such as moving a...","datacollector/UserGuide/Executors/Executors-title.html@@@Executors@@@...","datacollector/UserGuide/Executors/HDFSMetadata.html@@@HDFS File Metadata@@@The HDFS File Metadata executor changes file metadata, creates an empty file, or removes a file or directory in HDFS or a local file system each time it receives an event. You cannot perform multiple...","datacollector/UserGuide/Executors/HiveQuery.html@@@Hive Query@@@The Hive Query executor connects to Hive or Impala and performs one or more user-defined Hive or Impala queries each time it receives an event record. Use the Hive Query executor as part of an event...","datacollector/UserGuide/Executors/JDBCQuery.html@@@JDBC Query@@@The JDBC Query executor can run queries on database data from multiple database vendors...","datacollector/UserGuide/Executors/MapRFSFileMeta.html@@@MapR FS File Metadata@@@The MapR FS File Metadata executor changes file metadata, creates an empty file, or removes a file or directory in MapR FS each time it receives an event. You cannot perform multiple tasks in the same...","datacollector/UserGuide/Executors/MapReduce.html@@@MapReduce@@@The MapReduce executor includes two predefined jobs: Avro to ORC and Avro to Parquet...","datacollector/UserGuide/Executors/PipelineFinisher.html@@@Pipeline Finisher@@@When the Pipeline Finisher executor receives an event, the executor stops a pipeline and transitions it to a Finished state. This allows the pipeline to complete all expected processing before...","datacollector/UserGuide/Executors/SFTP.html@@@SFTP/FTP/FTPS Client@@@The SFTP/FTP/FTPS Client executor moves or removes a file on an SFTP/FTP/FTPS server each time it receives an event. You cannot perform multiple tasks in the same executor. To perform more than one...","datacollector/UserGuide/Executors/Shell.html@@@Shell@@@When Data Collector is registered with Control Hub, you can configure Data Collector to use an abbreviated version of the Control Hub user ID for shell impersonation mode...","datacollector/UserGuide/Executors/Spark.html@@@Spark@@@The Spark executor starts a Spark application each time it receives an event. You can use the Spark executor with Spark on YARN. The executor is not compatible with Spark on Mesos at this time. Use...","datacollector/UserGuide/Expression_Language/Constants.html@@@Constants@@@The expression language provides constants for use in expressions...","datacollector/UserGuide/Expression_Language/DateTimeVariables.html@@@Datetime Variables@@@The expression language provides datetime variables for use in expressions...","datacollector/UserGuide/Expression_Language/ExpressionLanguage_overview.html@@@Expression Language@@@Use the following information and tips when you invoke expression completion...","datacollector/UserGuide/Expression_Language/ExpressionLanguage_title.html@@@Expression Language@@@...","datacollector/UserGuide/Expression_Language/Functions.html@@@Functions@@@Use record functions to determine information about a record, such as the stage that created it or whether a field exists in the record...","datacollector/UserGuide/Expression_Language/Literals.html@@@Literals@@@The expression language includes the following literals...","datacollector/UserGuide/Expression_Language/Operators.html@@@Operators@@@The precedence of operators highest to lowest, left to right is as follows...","datacollector/UserGuide/Expression_Language/ReservedWords.html@@@Reserved Words@@@The following words are reserved for the expression language and should not be used as identifiers...","datacollector/UserGuide/Getting_Started/GettingStarted_Title.html@@@Getting Started@@@StreamSets Data CollectorTM is a lightweight, powerful design and execution engine that streams data in real time. Use Data Collector to route and process data in your data streams...","datacollector/UserGuide/Glossary/Glossary_title.html@@@Glossary@@@batch A set of records that passes through a pipeline. Data Collector processes data in batches. CDC-enabled origin An origin that can process changed data and place CRUD operation information in the...","datacollector/UserGuide/Installation/AddtionalStageLibs.html@@@Install Additional Stage Libraries@@@Install additional stage libraries to use stages that are not included in the core or common installation of Data Collector . This is an optional step, but core installations typically require...","datacollector/UserGuide/Installation/CMInstall-Overview.html@@@Installation with Cloudera Manager@@@When administering Data Collector with Cloudera Manager, configure all Data Collector configuration properties and environment variables through Cloudera Manager...","datacollector/UserGuide/Installation/CloudInstall.html@@@Installation with Cloud Service Providers@@@You can install the full Data Collector on Amazon Web Services (AWS)...","datacollector/UserGuide/Installation/Commoninstall.html@@@Common Installation@@@All users can install the Data Collector common installation. The common installation includes all core Data Collector functionality and commonly-used stage libraries in a tarball installation...","datacollector/UserGuide/Installation/CoreInstall_Overview.html@@@Core Installation@@@Users with a StreamSets enterprise account can use the Data Collector core installation...","datacollector/UserGuide/Installation/CreateAnotherDC.html@@@Creating Another Data Collector Instance@@@You can create another instance of a Data Collector tarball or RPM installation on the same machine with the create-dc command.\n        The additional Data Collector instance uses the same configuration as the original Data Collector instance. You can modify the configuration properties as needed...","datacollector/UserGuide/Installation/FullInstall_ServiceStart.html@@@Full Installation and Launch (Service Start)@@@Users with a StreamSets enterprise account can install the Data Collector RPM package and start it as a service on CentOS, Oracle Linux, or Red Hat Enterprise Linux...","datacollector/UserGuide/Installation/Install_title.html@@@Installation@@@...","datacollector/UserGuide/Installation/InstallationAndConfig.html@@@Installation@@@You can install Data Collector and start it manually or run it as a service...","datacollector/UserGuide/Installation/Installing_the_DC-Docker.html@@@Run Data Collector from Docker@@@All users can run the Data Collector image from Docker Hub...","datacollector/UserGuide/Installation/Installing_the_DC.html@@@Full Installation and Launch (Manual Start)@@@Users with a StreamSets enterprise account can install the Data Collector full tarball and start it manually on all supported operating systems...","datacollector/UserGuide/Installation/MapR-Prerequisites.html@@@MapR Prerequisites@@@Install MapR stage libraries if you use a core installation of Data Collector, which does not include MapR stage libraries...","datacollector/UserGuide/Installation/Uninstall.html@@@Uninstallation@@@You can uninstall a Data Collector instance that was installed from the tarball, from the RPM package, or from Cloudera Manager. To uninstall a Data Collector instance that was installed from the...","datacollector/UserGuide/Microservice/Microservice_Title.html@@@Microservice Pipelines@@@A microservice pipeline is a pipeline that creates a fine-grained service to perform a specific task...","datacollector/UserGuide/Multithreaded_Pipelines/MultithreadedPipelines.html@@@Multithreaded Pipelines@@@A multithreaded pipeline is a Data Collector pipeline with an origin that supports parallel execution, enabling one pipeline to run in multiple threads. Not valid in Data Collector Edge pipelines...","datacollector/UserGuide/Orchestration_Pipelines/OrchestrationPipelines_Title.html@@@Orchestration Pipelines@@@An orchestration pipeline is a Data Collector pipeline that uses one or more orchestration stages to schedule and perform tasks, such as starting a sequence of Data Collector pipelines and Control Hub...","datacollector/UserGuide/Origins/ADLS-G1.html@@@Azure Data Lake Storage Gen1@@@Supported pipeline types: Data Collector The Azure Data Lake Storage Gen1 origin reads data from Microsoft Azure Data Lake Storage Gen1. The origin can create multiple threads to enable parallel...","datacollector/UserGuide/Origins/ADLS-G2.html@@@Azure Data Lake Storage Gen2@@@Supported pipeline types: Data Collector The Azure Data Lake Storage Gen2 origin reads data from Microsoft Azure Data Lake Storage Gen2. The origin can create multiple threads to enable parallel...","datacollector/UserGuide/Origins/AmazonS3.html@@@Amazon S3@@@The Amazon S3 origin uses multiple concurrent threads to process data based on the Number of Threads property...","datacollector/UserGuide/Origins/AmazonSQS.html@@@Amazon SQS Consumer@@@Supported pipeline types: Data Collector Use the Amazon SQS Consumer origin to read data from queues in Amazon Simple Queue Services (SQS). The origin can use multiple threads to enable parallel...","datacollector/UserGuide/Origins/AzureEventHub.html@@@Azure IoT/Event Hub Consumer@@@Supported pipeline types: Data Collector The Azure IoT/Event Hub Consumer origin reads data from Microsoft Azure Event Hub. The origin can use multiple threads to enable parallel processing of data...","datacollector/UserGuide/Origins/BigQuery.html@@@Google BigQuery@@@The Google BigQuery origin converts the Google BigQuery data types to Data Collector data types...","datacollector/UserGuide/Origins/CoAPServer.html@@@CoAP Server@@@Before you run a pipeline with the CoAP Server origin, configure the CoAP clients to send data to the CoAP Server listening port and resource...","datacollector/UserGuide/Origins/CronScheduler.html@@@Cron Scheduler@@@Supported pipeline types: Data Collector The Cron Scheduler origin generates records periodically based on a schedule. Use the origin to schedule tasks by triggering downstream stages in a pipeline...","datacollector/UserGuide/Origins/Directory.html@@@Directory@@@Use a file name pattern to define the files that the Directory origin processes. You can use either a glob pattern or a regular expression to define the file name pattern...","datacollector/UserGuide/Origins/Elasticsearch.html@@@Elasticsearch@@@Define the query that the origin uses to return data from Elasticsearch. You can define any valid Elasticsearch query...","datacollector/UserGuide/Origins/FileTail.html@@@File Tail@@@File Tail processes the active file and archived files based on how the source server generates files. When you specify the naming convention for archived files, File Tail determines the file generation method and processes the data accordingly...","datacollector/UserGuide/Origins/GCS.html@@@Google Cloud Storage@@@Supported pipeline types: Data Collector The Google Cloud Storage origin reads objects stored in Google Cloud Storage. The objects must be fully written and reside in a single bucket. The object names...","datacollector/UserGuide/Origins/GroovyScripting.html@@@Groovy Scripting@@@You can call external Java code from the Groovy Scripting origin. Simply install the external Java library to make it available to the origin. Then, call the external Java code from the Groovy script that you develop for the origin...","datacollector/UserGuide/Origins/HDFSStandalone.html@@@Hadoop FS Standalone@@@Supported pipeline types: Data Collector The Hadoop FS Standalone origin reads files from HDFS. You can also use the origin to read from Azure Blob storage. The files to be processed must all share a...","datacollector/UserGuide/Origins/HTTPClient.html@@@HTTP Client@@@By default, the HTTP Client origin processes only responses that include a 2xx success status code. When the response includes any other status code, such as a 4xx or 5xx status code, the origin generates an error and handles the record based on the error record handling configured for the stage...","datacollector/UserGuide/Origins/HTTPServer.html@@@HTTP Server@@@Configure the HTTP clients to send data to the HTTP Server listening port...","datacollector/UserGuide/Origins/HTTPtoKafka.html@@@HTTP to Kafka (Deprecated)@@@The HTTP to Kafka origin listens on an HTTP endpoint and writes the contents of all authorized HTTP POST requests directly to Kafka. However, the HTTP to Kafka origin is now deprecated and will be...","datacollector/UserGuide/Origins/HadoopFS-origin.html@@@Hadoop FS@@@The Hadoop FS origin included in a cluster batch or cluster EMR batch pipeline allows you to read from Amazon S3...","datacollector/UserGuide/Origins/JDBCConsumer.html@@@JDBC Query Consumer@@@The JDBC Query Consumer origin can read database data from multiple database vendors...","datacollector/UserGuide/Origins/JMS.html@@@JMS Consumer@@@Before you use the JMS Consumer, install the JMS drivers for the implementation that you are using...","datacollector/UserGuide/Origins/JavaScriptScripting.html@@@JavaScript Scripting@@@Supported pipeline types: Data Collector The JavaScript Scripting origin runs a JavaScript script to create Data Collector records. The JavaScript Scripting origin supports Java version 8u40 and later...","datacollector/UserGuide/Origins/JythonScripting.html@@@Jython Scripting@@@Supported pipeline types: Data Collector The Jython Scripting origin runs a Jython script to create Data Collector records. The Jython Scripting origin supports Jython version 2.7.x . The script runs...","datacollector/UserGuide/Origins/KConsumer.html@@@Kafka Consumer@@@The first time that a Kafka Consumer origin identified by a consumer group receives messages from a topic, an offset entry is created for that consumer group and topic. The offset entry is created in ZooKeeper or Kafka, depending on your Kafka version and broker configuration...","datacollector/UserGuide/Origins/KafkaMultiConsumer.html@@@Kafka Multitopic Consumer@@@The first time that a Kafka Multitopic Consumer origin identified by a consumer group receives messages from a topic, an offset entry is created for that consumer group and topic.\n    The offset entry is created in Kafka...","datacollector/UserGuide/Origins/KinConsumer.html@@@Kinesis Consumer@@@You can configure the read interval for the Kinesis Consumer. The read interval determines how long Kinesis Consumer waits before requesting additional data from Kinesis shards.\n  By default, the Kinesis Consumer waits one second between requests...","datacollector/UserGuide/Origins/MQTTSubscriber.html@@@MQTT Subscriber@@@The MQTT Subscriber origin reads messages from one or more topics on the MQTT broker. A topic is a string that the broker uses to filter messages for each connected client...","datacollector/UserGuide/Origins/MapRDBJSON.html@@@MapR DB JSON@@@When the origin converts a JSON document into a record, it includes the _id field of the JSON document in the record. If needed, you can use the Field Remover processor in the pipeline to remove the _id field...","datacollector/UserGuide/Origins/MapRFS.html@@@MapR FS@@@You can use Kerberos authentication to connect to MapR. When you use Kerberos authentication, Data Collector uses the Kerberos principal and keytab to connect to MapR. By default, Data Collector uses the user account who started it to connect...","datacollector/UserGuide/Origins/MapRFSStandalone.html@@@MapR FS Standalone@@@Use a file name pattern to define the files that the MapR FS Standalone origin processes. You can use either a glob pattern or a regular expression to define the file name pattern...","datacollector/UserGuide/Origins/MapRStreamsCons.html@@@MapR Streams Consumer@@@When you start a pipeline for the first time, the MapR Streams Consumer becomes a new consumer group for the topic. It reads only incoming data, processing data from all partitions, and ignores any existing data in the topic by default...","datacollector/UserGuide/Origins/MapRStreamsMultiConsumer.html@@@MapR Multitopic Streams Consumer@@@You can add custom configuration properties to the MapR Multitopic Streams Consumer. You can use any MapR or Kafka property supported by MapR Streams. For more information, see the MapR Streams documentation...","datacollector/UserGuide/Origins/MapRdbCDC.html@@@MapR DB CDC@@@Supported pipeline types: Data Collector The MapR DB CDC origin reads changed data from MapR DB that has been written to MapR Streams. The origin can use multiple threads to enable parallel processing...","datacollector/UserGuide/Origins/MongoDB.html@@@MongoDB@@@The MongoDB origin can read from a Microsoft Azure Cosmos DB instance configured to use the MongoDB API...","datacollector/UserGuide/Origins/MongoDBOplog.html@@@MongoDB Oplog@@@Supported pipeline types: Data Collector The MongoDB Oplog origin reads entries from MongoDB Oplog. MongoDB stores information about changes to the database in a local capped collection called an...","datacollector/UserGuide/Origins/MultiTableJDBCConsumer.html@@@JDBC Multitable Consumer@@@The JDBC Multitable Consumer origin can read database data from multiple database vendors...","datacollector/UserGuide/Origins/MySQLBinaryLog.html@@@MySQL Binary Log@@@The MySQL Binary Log origin can process binary logs from a MySQL server configured to use row-based logging...","datacollector/UserGuide/Origins/NiFi.html@@@NiFi HTTP Server@@@Supported pipeline types: Data Collector The NiFi HTTP Server origin listens for requests from a NiFi PutHTTP processor and processes NiFi FlowFiles. The documentation for this stage is forthcoming...","datacollector/UserGuide/Origins/OPCUAClient.html@@@OPC UA Client@@@Supported pipeline types: Data Collector The OPC UA Client origin processes data from an OPC UA server. The OPC UA Client origin can poll the server at regular intervals, returning the latest data...","datacollector/UserGuide/Origins/Omniture.html@@@Omniture@@@Supported pipeline types: Data Collector The Omniture origin processes JSON website usage reports generated by the Omniture reporting APIs. Omniture is also known as the Adobe Marketing Cloud. When...","datacollector/UserGuide/Origins/OracleBulk.html@@@Oracle Bulkload@@@You must install the Oracle Bulkload stage library before using the Oracle Bulkload origin...","datacollector/UserGuide/Origins/OracleCDC.html@@@Oracle CDC Client@@@The Oracle CDC Client origin provides an alternate PEG parser that you can try when concerned about pipeline performance...","datacollector/UserGuide/Origins/Origins_overview.html@@@Origins@@@An origin stage represents the source for the pipeline. You can use a single origin stage in a pipeline...","datacollector/UserGuide/Origins/Origins_title.html@@@Origins@@@...","datacollector/UserGuide/Origins/PostgreSQL.html@@@PostgreSQL CDC Client@@@Supported pipeline types: Data Collector The PostgreSQL CDC Client origin processes Write-Ahead Logging (WAL) data to generate change data capture records for a PostgreSQL database. Use the PostgreSQL...","datacollector/UserGuide/Origins/PubSub.html@@@Google Pub/Sub Subscriber@@@Supported pipeline types: Data Collector The Google Pub/Sub Subscriber origin consumes messages from a Google Pub/Sub subscription. When you configure the origin, you define the Google Pub/Sub...","datacollector/UserGuide/Origins/PulsarConsumer.html@@@Pulsar Consumer@@@The Pulsar Consumer origin can subscribe to a single topic or to multiple topics. In either case, the origin uses one thread for the read...","datacollector/UserGuide/Origins/RESTService.html@@@REST Service@@@You can configure the origin to use Data Collector as an API gateway instead of listening for messages at an HTTP endpoint...","datacollector/UserGuide/Origins/RabbitMQ.html@@@RabbitMQ Consumer@@@RabbitMQ Consumer reads messages from a specified queue...","datacollector/UserGuide/Origins/Redis.html@@@Redis Consumer@@@You can specify channels and patterns to define the data that the Redis Consumer origin processes...","datacollector/UserGuide/Origins/SAPHana.html@@@SAP HANA Query Consumer@@@Supported pipeline types: Data Collector The SAP HANA Query Consumer origin reads from an SAP HANA database using the specified SQL query. The SQL query can read data from a single table or from a...","datacollector/UserGuide/Origins/SDCRPCtoKafka.html@@@SDC RPC to Kafka (Deprecated)@@@You can specify the maximum number of requests the SDC RPC to Kafka origin handles at one time...","datacollector/UserGuide/Origins/SDC_RPCorigin.html@@@SDC RPC@@@Supported pipeline types: Data Collector The SDC RPC origin enables connectivity between two SDC RPC pipelines. The SDC RPC origin reads data passed from an SDC RPC destination. Use the SDC RPC origin...","datacollector/UserGuide/Origins/SFTP.html@@@SFTP/FTP/FTPS Client@@@Use a file name pattern to define the files that the SFTP/FTP/FTPS Client origin processes. You can use either a glob pattern or a regular expression to define the file name pattern...","datacollector/UserGuide/Origins/SQLServerBDCMultitable.html@@@SQL Server 2019 BDC Multitable Consumer@@@You must install the SQL Server 2019 Big Data Cluster stage library before using the SQL Server 2019 BDC Multitable Consumer origin. The SQL Server 2019 Big Data Cluster stage library includes the JDBC driver that the origin uses to access SQL Server 2019 Big Data Cluster...","datacollector/UserGuide/Origins/SQLServerCDC.html@@@SQL Server CDC Client@@@Supported pipeline types: Data Collector The SQL Server CDC Client origin processes data in Microsoft SQL Server change data capture (CDC) tables. The origin fetches changes in time windows and uses...","datacollector/UserGuide/Origins/SQLServerChange.html@@@SQL Server Change Tracking@@@Supported pipeline types: Data Collector The SQL Server Change Tracking origin processes data from Microsoft SQL Server change tracking tables. The origin can process data from tables with simple...","datacollector/UserGuide/Origins/Salesforce.html@@@Salesforce@@@The Salesforce origin can execute a query to read existing data from Salesforce. Use the Salesforce Object Query Language (SOQL) to write the query...","datacollector/UserGuide/Origins/StartJob.html@@@Start Jobs@@@Supported pipeline types: Data Collector The Start Jobs origin starts one or more Control Hub jobs in parallel when the pipeline starts. The origin can also start job instances from a job template...","datacollector/UserGuide/Origins/StartPipe.html@@@Start Pipelines@@@The Start Pipelines origin creates an orchestration record that includes information about the pipelines that it starts...","datacollector/UserGuide/Origins/SystemMetrics.html@@@System Metrics@@@The System Metrics origin uses the psutil package for the Go programming language (or Golang) to collect system metrics...","datacollector/UserGuide/Origins/TCPServer.html@@@TCP Server@@@Supported pipeline types: Data Collector The TCP Server origin listens at the specified port numbers, establishes TCP sessions with clients that initiate TCP connections, and then processes the...","datacollector/UserGuide/Origins/Teradata.html@@@Teradata Consumer@@@You must install the Teradata stage library before using the Teradata Consumer origin...","datacollector/UserGuide/Origins/UDP.html@@@UDP Source@@@Supported pipeline types: Data Collector The UDP Source origin reads messages from one or more UDP ports. To use multiple threads for pipeline processing, use the UDP Multithreaded Source . For a...","datacollector/UserGuide/Origins/UDPMulti.html@@@UDP Multithreaded Source@@@Supported pipeline types: Data Collector The UDP Multithreaded Source origin reads messages from one or more UDP ports. The origin can create multiple worker threads to enable parallel processing in a...","datacollector/UserGuide/Origins/UDPtoKafka.html@@@UDP to Kafka (Deprecated)@@@When you use a UDP to Kafka origin in a pipeline, connect the origin to a Trash destination...","datacollector/UserGuide/Origins/WebSocketClient.html@@@WebSocket Client@@@Supported pipeline types: Data Collector Data Collector Edge The WebSocket Client origin reads data from a WebSocket server endpoint. Use the origin to read data from a WebSocket resource URL. The...","datacollector/UserGuide/Origins/WebSocketServer.html@@@WebSocket Server@@@Configure the WebSocket clients to send data to the WebSocket Server listening port...","datacollector/UserGuide/Origins/WindowsLog.html@@@Windows Event Log@@@Supported pipeline types: Data Collector Data Collector Edge The Windows Event Log origin reads data from a Microsoft Windows event log located on a Windows machine. The origin generates a record for...","datacollector/UserGuide/Origins/gRPCClient.html@@@gRPC Client@@@Before the gRPC Client origin can process data from a gRPC server, you must enable reflection for the server...","datacollector/UserGuide/Pipeline_Configuration/AdvancedOptions.html@@@Advanced Options@@@Pipelines and most pipeline stages include advanced options with default values that should work in most cases. By default, each pipeline and stage hides the advanced options.\n        Advanced options can include individual properties or complete tabs...","datacollector/UserGuide/Pipeline_Configuration/AmazonSecurity.html@@@Security in Amazon Stages@@@When using instance profile authentication, you can configure an Amazon stage to assume another IAM role...","datacollector/UserGuide/Pipeline_Configuration/ConfiguringAPipeline.html@@@Configuring a Pipeline@@@Configure a pipeline to define the stream of data. After you configure the pipeline,\n        you can start the pipeline...","datacollector/UserGuide/Pipeline_Configuration/DataCollectorUI-Config.html@@@Data Collector UI - Edit Mode@@@The following image shows the Data Collector UI when you configure a pipeline: Area / Icon Name Description 1 Pipeline canvas Displays the pipeline. Use to configure the pipeline data flow. 2 Pipeline...","datacollector/UserGuide/Pipeline_Configuration/EventGeneration.html@@@Event Generation@@@The event framework generates pipeline events for Data Collector standalone pipelines when the pipeline starts and stops. Not available in Data Collector Edge pipelines. In a Data Collector standalone...","datacollector/UserGuide/Pipeline_Configuration/Expressions.html@@@Expression Configuration@@@Precede all expressions with a dollar sign and enclose them with curly brackets, as follows: ${&lt;expression&gt;}...","datacollector/UserGuide/Pipeline_Configuration/KMessageKey.html@@@Kafka Message Keys@@@You can configure the Kafka Consumer and Kafka Multitopic Consumer origins to capture the message keys included in each Kafka message and store them in generated records. Kafka message keys can be...","datacollector/UserGuide/Pipeline_Configuration/KafkaSecurity.html@@@Security in Kafka Stages@@@You can define Kerberos keytabs in a credential store, then call the appropriate keytab from a Kafka stage...","datacollector/UserGuide/Pipeline_Configuration/Notifications.html@@@Notifications@@@You can configure a pipeline to send an email or webhook when the pipeline changes to specified states. For example, you might send an email when someone manually stops the pipeline, causing it to...","datacollector/UserGuide/Pipeline_Configuration/PipelineConfiguration_title.html@@@Pipeline Configuration@@@...","datacollector/UserGuide/Pipeline_Configuration/PipelineRateLimit.html@@@Rate Limit@@@You can limit the rate at which a Data Collector pipeline processes records by defining the maximum number of records that the pipeline can read in a second...","datacollector/UserGuide/Pipeline_Configuration/ProductIcons_Doc.html@@@Pipeline Types and Icons in Documentation@@@In Data Collector, you can configure pipelines that are run by Data Collector and pipelines that are run by Data Collector Edge...","datacollector/UserGuide/Pipeline_Configuration/Retry.html@@@Retrying the Pipeline@@@By default, when Data Collector encounters a stage-level error that might cause a standalone pipeline to fail, it retries the pipeline. That is, it waits a period of time, and then tries again to run the pipeline...","datacollector/UserGuide/Pipeline_Configuration/RuntimeValues.html@@@Runtime Values@@@Runtime values are values that you define outside of the pipeline and use for stage and pipeline properties. You can change the values for each pipeline run without having to edit the pipeline...","datacollector/UserGuide/Pipeline_Configuration/SSL-TLS.html@@@SSL/TLS Encryption@@@Many stages can use SSL/TLS encryption to securely connect to the external system...","datacollector/UserGuide/Pipeline_Configuration/SimpleBulkEdit.html@@@Simple and Bulk Edit Mode@@@Some pipeline and pipeline stage properties include an Add icon ( ) where you add additional configurations. You can add the configurations in simple or bulk edit mode. By default, each property uses...","datacollector/UserGuide/Pipeline_Configuration/Validation.html@@@Implicit and Explicit Validation@@@Data Collector performs two types of validation: Implicit validation Implicit validation occurs by default as the Data Collector UI saves your changes. Implicit validation lists missing or incomplete...","datacollector/UserGuide/Pipeline_Configuration/Webhooks.html@@@Webhooks@@@You can configure a pipeline to use webhooks. Not available in Data Collector Edge pipelines. A webhook is a user-defined HTTP callback - an HTTP request that the pipeline sends automatically when...","datacollector/UserGuide/Pipeline_Design/CDC-Overview.html@@@Processing Changed Data@@@Certain stages enable you to easily process data changes, such as change capture data (CDC) or transactional data, in a pipeline. CDC-enabled origins can read change capture data. Some exclusively...","datacollector/UserGuide/Pipeline_Design/ControlCharacters.html@@@Control Character Removal@@@You can use several stages to remove control characters - such as escape or end-of-transmission characters - from data. Remove control characters to avoid creating invalid records...","datacollector/UserGuide/Pipeline_Design/DatainMotion.html@@@Data in Motion@@@Data passes through the pipeline in batches. This is how it works...","datacollector/UserGuide/Pipeline_Design/DesigningDataFlow.html@@@Designing the Data Flow@@@You can branch and merge streams in the pipeline...","datacollector/UserGuide/Pipeline_Design/DevStages.html@@@Development Stages@@@You can use several development stages to help develop and test pipelines. Note: Do not use development stages in production pipelines. You can use the following stages when developing or testing...","datacollector/UserGuide/Pipeline_Design/DroppingUnwantedRecords.html@@@Dropping Unwanted Records@@@You can drop records from the pipeline at each stage by defining required fields or preconditions for a record to enter a stage...","datacollector/UserGuide/Pipeline_Design/ErrorHandling.html@@@Error Record Handling@@@You can configure error record handling at a stage level and at a pipeline level. You can also specify the version of the record to use as the basis for the error record. When an error occurs as a...","datacollector/UserGuide/Pipeline_Design/FieldAttributes.html@@@Field Attributes@@@Field attributes are attributes that provide additional information about each field that you can use in pipeline logic, as needed...","datacollector/UserGuide/Pipeline_Design/PipelineDesign_title.html@@@Pipeline Concepts and Design@@@...","datacollector/UserGuide/Pipeline_Design/RecordHeaderAttributes.html@@@Record Header Attributes@@@Record header attributes are attributes in record headers that you can use in pipeline logic, as needed. Some stages create record header attributes for a particular purpose. For example, CDC-enabled...","datacollector/UserGuide/Pipeline_Design/SamplePipelines.html@@@Sample Pipelines@@@Data Collector provides sample pipelines that you can use to learn about Data Collector features or as a basis for building your own pipelines...","datacollector/UserGuide/Pipeline_Design/ShortcutKeys.html@@@Shortcut Keys for Pipeline Design@@@When designing a pipeline, you can use the following shortcut keys: Shortcut Key Description Command+Z Undo an action in the pipeline canvas or properties panel. Command+Shift+Z Redo an action in the...","datacollector/UserGuide/Pipeline_Design/TechPreview.html@@@Technology Preview Functionality@@@Data Collector includes certain new features and stages with the Technology Preview designation. Technology Preview functionality is available for use in development and testing, but is not meant for use in production...","datacollector/UserGuide/Pipeline_Design/TestOrigin.html@@@Test Origin for Preview@@@A test origin can provide test data for data preview to aid in pipeline development.\n        In Control Hub, you can also use test origins when developing pipeline fragments. Test origins are not used when running a pipeline...","datacollector/UserGuide/Pipeline_Design/What_isa_Pipeline.html@@@What is a Pipeline?@@@A pipeline describes the flow of data from the origin system to destination systems and defines how to transform the data along the way...","datacollector/UserGuide/Pipeline_Maintenance/PipelineMaintenance_title.html@@@Pipeline Maintenance@@@If you defined runtime parameters for a pipeline, you can specify the parameter values to use when you start the pipeline...","datacollector/UserGuide/Pipeline_Maintenance/PipelineStates-Examples.html@@@State Transition Examples@@@Here are some examples of how pipelines can move through states: Starting a pipeline When you successfully start a pipeline for the first time, a pipeline transitions through the following states...","datacollector/UserGuide/Pipeline_Maintenance/PipelineStates-Understanding.html@@@Understanding Pipeline States@@@A pipeline state is the current condition of the pipeline, such as &quot;running&quot; or &quot;stopped&quot;. The pipeline state can display in the All Pipelines list. The state of a pipeline can also appear in the Data...","datacollector/UserGuide/Pipeline_Monitoring/PipelineMonitoring_title.html@@@Pipeline Monitoring@@@When the Data Collector runs a pipeline, you can view real-time statistics about the pipeline, examine a sample of the data being processed, and create rules and alerts...","datacollector/UserGuide/Processors/Aggregator.html@@@Windowing Aggregator@@@Supported pipeline types: Data Collector The Windowing Aggregator processor performs one or more aggregations within a window of time. The Windowing Aggregator processor displays the results in...","datacollector/UserGuide/Processors/Base64Decoder.html@@@Base64 Field Decoder@@@Supported pipeline types: Data Collector The Base64 Field Decoder decodes Base64 encoded data to binary data. Use the processor to decode Base64 encoded data before evaluating data in the field. When...","datacollector/UserGuide/Processors/Base64Encoder.html@@@Base64 Field Encoder@@@Supported pipeline types: Data Collector The Base64 Field Encoder encodes binary data using Base64. Use the processor to encode binary data that must be sent over channels that expect ASCII data. For...","datacollector/UserGuide/Processors/ControlHubAPI.html@@@Control Hub API@@@Supported pipeline types: Data Collector The Control Hub API processor sends requests to the Control Hub REST API and writes data from the response to a specified output field. The Control Hub API...","datacollector/UserGuide/Processors/CouchbaseLookup.html@@@Couchbase Lookup@@@Supported pipeline types: Data Collector The Couchbase Lookup processor looks up documents in Couchbase Server and returns values to fields in the record. Use the Couchbase Lookup processor to enrich...","datacollector/UserGuide/Processors/DataGenerator.html@@@Data Generator@@@When you use the Data Generator processor, you specify the target field for the serialized record...","datacollector/UserGuide/Processors/DataParser.html@@@Data Parser@@@Supported pipeline types: Data Collector The Data Parser processor allows you to parse supported data formats embedded in a field. You can parse NetFlow embedded in a byte array field or syslog...","datacollector/UserGuide/Processors/DatabricksML.html@@@Databricks ML Evaluator (Deprecated)@@@Supported pipeline types: Data Collector The Databricks ML Evaluator processor uses a machine learning model exported with Databricks ML Model Export to generate evaluations, scoring, or...","datacollector/UserGuide/Processors/Delay.html@@@Delay@@@Supported pipeline types: Data Collector Data Collector Edge The Delay processor delays passing a batch to rest of the pipeline by the specified number of milliseconds. Use the Delay processor to...","datacollector/UserGuide/Processors/EncryptDecrypt.html@@@Encrypt and Decrypt Fields@@@When you use the Encrypt and Decrypt Fields processor, you specify the key provider for the stage...","datacollector/UserGuide/Processors/Expression.html@@@Expression Evaluator@@@Supported pipeline types: Data Collector Data Collector Edge The Expression Evaluator performs calculations and writes the results to new or existing fields. You can also use the Expression Evaluator...","datacollector/UserGuide/Processors/FieldFlattener.html@@@Field Flattener@@@Supported pipeline types: Data Collector The Field Flattener processor flattens list and map fields. The processor can flatten the entire record to produce a record with no nested fields. Or it can...","datacollector/UserGuide/Processors/FieldHasher.html@@@Field Hasher@@@Field Hasher provides several methods to hash data. When you hash a field more than once, Field Hasher uses the existing hash when generating the next hash...","datacollector/UserGuide/Processors/FieldMapper.html@@@Field Mapper@@@Supported pipeline types: Data Collector The Field Mapper processor maps an expression to a set of fields to alter field paths, field names, or field values. For example, you might use the Field...","datacollector/UserGuide/Processors/FieldMasker.html@@@Field Masker@@@You can use the following mask types to mask data...","datacollector/UserGuide/Processors/FieldMerger.html@@@Field Merger@@@Supported pipeline types: Data Collector The Field Merger merges one or more fields in a record to a different location in the record. Use only for records with a list or map structure. When you...","datacollector/UserGuide/Processors/FieldOrder.html@@@Field Order@@@When you configure the Field Order processor, you specify the list of fields to order.\n        An incoming record might be missing one of those fields or it might have some extra fields.\n        You configure how the processor handles any missing or extra fields...","datacollector/UserGuide/Processors/FieldRemover.html@@@Field Remover@@@Supported pipeline types: Data Collector Data Collector Edge The Field Remover processor removes fields from records. Use the processor to discard field data that you do not need in the pipeline. When...","datacollector/UserGuide/Processors/FieldRenamer.html@@@Field Renamer@@@You can use regular expressions, or regex, along with the StreamSets expression language to rename sets of fields. You can use regex to define the set of source fields to rename and to define the target field names. You can also use the StreamSets expression language to define target field names...","datacollector/UserGuide/Processors/FieldReplacer.html@@@Field Replacer@@@Use the Field Replacer processor to replace values in a field with null values. You can replace all values with nulls or you can replace a set of values based on a condition...","datacollector/UserGuide/Processors/FieldSplitter.html@@@Field Splitter@@@Supported pipeline types: Data Collector The Field Splitter splits string data based on a regular expression and passes the separated data to new fields. Use the Field Splitter to split complex string...","datacollector/UserGuide/Processors/FieldTypeConverter.html@@@Field Type Converter@@@The following table lists the data types that can be converted to another data type.\n  List, Map, and List-Map data types cannot be converted...","datacollector/UserGuide/Processors/FieldZip.html@@@Field Zip@@@Supported pipeline types: Data Collector The Field Zip processor merges list data from two fields into a single field. You can use the Field Zip processor to merge two List fields or List-Map fields...","datacollector/UserGuide/Processors/GeoIP.html@@@Geo IP@@@Supported pipeline types: Data Collector The Geo IP processor is a lookup processor that can return geolocation and IP intelligence information for a specified IP address. The Geo IP processor uses...","datacollector/UserGuide/Processors/Groovy.html@@@Groovy Evaluator@@@You can choose the processing mode that the Groovy Evaluator uses to process the main script. You can use the same script in each processing mode. However, you should include error handling in the main script before you run in batch mode...","datacollector/UserGuide/Processors/HBaseLookup.html@@@HBase Lookup@@@When you define the lookup key, you specify the row and optionally the column and timestamp to look up in HBase...","datacollector/UserGuide/Processors/HTTPClient.html@@@HTTP Client@@@By default, the HTTP Client processor processes only responses that include a 2xx success status code. When the response includes any other status code, such as a 4xx or 5xx status code, the processor generates an error and handles the record based on the error record handling configured for the stage...","datacollector/UserGuide/Processors/HTTPRouter.html@@@HTTP Router@@@Supported pipeline types: Data Collector The HTTP Router processor passes records to data streams based on the HTTP method and URL path in the record header attributes. You can use the HTTP Router...","datacollector/UserGuide/Processors/HiveMetadata.html@@@Hive Metadata@@@The Hive Metadata processor queries Hive for information and caches the results. When possible, it uses the cache for record comparison to avoid unnecessary Hive queries...","datacollector/UserGuide/Processors/JDBCLookup.html@@@JDBC Lookup@@@The JDBC Lookup processor can perform lookups of database data from multiple database vendors...","datacollector/UserGuide/Processors/JDBCTee.html@@@JDBC Tee@@@The JDBC Tee processor can write data to a MySQL or PostgreSQL database...","datacollector/UserGuide/Processors/JSONGenerator.html@@@JSON Generator@@@Supported pipeline types: Data Collector The JSON Generator serializes data in a field to a JSON-encoded string. You can serialize data from List, Map, or List-Map fields. When you configure the...","datacollector/UserGuide/Processors/JSONParser.html@@@JSON Parser@@@Configure a JSON Parser to parse a JSON object in a String field...","datacollector/UserGuide/Processors/JavaScript.html@@@JavaScript Evaluator@@@You can choose the processing mode that the JavaScript Evaluator uses to process the main script. You can use the same script in each processing mode. However, you should include error handling in the main script before you run in batch mode...","datacollector/UserGuide/Processors/Jython.html@@@Jython Evaluator@@@You can choose the processing mode that the Jython Evaluator uses to process the main script. You can use the same script in each processing mode. However, you should include error handling in the main script before you run in batch mode...","datacollector/UserGuide/Processors/KuduLookup.html@@@Kudu Lookup@@@Supported pipeline types: Data Collector The Kudu Lookup processor performs lookups in a Kudu table and passes the lookup values to fields. Use the Kudu Lookup to enrich records with additional data...","datacollector/UserGuide/Processors/ListPivoter.html@@@Field Pivoter@@@Configure a Field Pivoter to pivot data in a list, map, or list-map field and generate a record for each item in the field...","datacollector/UserGuide/Processors/LogParser.html@@@Log Parser@@@When you use Log Parser to parse log data, you define the format of the log files to be read...","datacollector/UserGuide/Processors/MLeap.html@@@MLeap Evaluator@@@Supported pipeline types: Data Collector The MLeap Evaluator processor uses a machine learning model stored in an MLeap bundle to generate evaluations, scoring, or classifications of data. With the...","datacollector/UserGuide/Processors/MongoDBLookup.html@@@MongoDB Lookup@@@When you configure the MongoDB Lookup processor, you define the document fields to look up in MongoDB. You map these document fields to fields in the record that contain the values to look up...","datacollector/UserGuide/Processors/PMML.html@@@PMML Evaluator@@@Supported pipeline types: Data Collector The PMML Evaluator processor uses a machine learning model stored in the Predictive Model Markup Language (PMML) format to generate predictions or...","datacollector/UserGuide/Processors/PostgreSQLMetadata.html@@@PostgreSQL Metadata@@@Supported pipeline types: Data Collector The PostgreSQL Metadata processor determines the PostgreSQL table where each record should be written, compares the record structure against the table...","datacollector/UserGuide/Processors/Processors_overview.html@@@Processors@@@A processor stage represents a type of data processing that you want to perform. You can use as many processors in a pipeline as you need. You can use different processors based on the execution mode...","datacollector/UserGuide/Processors/Processors_title.html@@@Processors@@@...","datacollector/UserGuide/Processors/RDeduplicator.html@@@Record Deduplicator@@@The Record Deduplicator caches record information for comparison until it reaches a specified number of records. Then, it discards the information in the cache and starts over...","datacollector/UserGuide/Processors/RedisLookup.html@@@Redis Lookup@@@When you define the key to look up in Redis, you specify the data type of the Redis value. The Redis Lookup processor converts the Redis data type to a Data Collector data type...","datacollector/UserGuide/Processors/SQLParser.html@@@SQL Parser@@@Supported pipeline types: Data Collector The SQL Parser parses a SQL query in a string field. When parsing a query, the processor generates fields based on the fields defined in the SQL query and...","datacollector/UserGuide/Processors/SalesforceLookup.html@@@Salesforce Lookup@@@Supported pipeline types: Data Collector The Salesforce Lookup processor performs lookups in a Salesforce object and passes the lookup values to fields. Use the Salesforce Lookup to enrich records...","datacollector/UserGuide/Processors/SchemaGenerator.html@@@Schema Generator@@@Supported pipeline types: Data Collector The Schema Generator processor generates a schema based on the structure of a record and writes the schema into a record header attribute. The Schema Generator...","datacollector/UserGuide/Processors/Spark.html@@@Spark Evaluator@@@Supported pipeline types: Data Collector The Spark Evaluator performs custom processing within a pipeline based on a Spark application that you develop. The Spark Evaluator processor is especially...","datacollector/UserGuide/Processors/StartJob-P.html@@@Start Jobs@@@Supported pipeline types: Data Collector The Start Jobs processor starts one or more Control Hub jobs in parallel upon receiving a record. The processor can also start job instances from a job...","datacollector/UserGuide/Processors/StartPipe-P.html@@@Start Pipelines@@@Supported pipeline types: Data Collector The Start Pipelines processor starts one or more pipelines in parallel upon receiving a record. The Start Pipelines processor is an orchestration stage that...","datacollector/UserGuide/Processors/StaticLookup.html@@@Static Lookup@@@Configure a Static Lookup processor to perform key-value lookups in memory...","datacollector/UserGuide/Processors/StreamSelector.html@@@Stream Selector@@@The default stream captures records that do not match user-defined conditions. Use the default stream to manage unmatched records...","datacollector/UserGuide/Processors/TensorFlow.html@@@TensorFlow Evaluator@@@The TensorFlow Evaluator processor can evaluate each record or evaluate the entire batch at once...","datacollector/UserGuide/Processors/ValueReplacer.html@@@Value Replacer (Deprecated)@@@The Value Replacer replaces values in fields. However, the Value Replacer is now deprecated and will be removed in a future release. We recommend using the Field Replacer processor. You can use the...","datacollector/UserGuide/Processors/WaitJob.html@@@Wait for Jobs@@@Supported pipeline types: Data Collector The Wait for Jobs processor waits for one or more jobs or job instances to complete. The Wait for Jobs processor is an orchestration stage that you use in...","datacollector/UserGuide/Processors/WaitPipe.html@@@Wait for Pipelines@@@Supported pipeline types: Data Collector The Wait for Pipelines processor waits for one or more pipelines to complete. The Wait for Pipelines processor is an orchestration stage that you use in...","datacollector/UserGuide/Processors/WholeFileTransformer.html@@@Whole File Transformer@@@The Write Avro pipeline reads data from the origin, performs any required processing,\n        and uses a file-based Avro destination such as Local FS to write Avro files to a file system...","datacollector/UserGuide/Processors/XMLFlattener.html@@@XML Flattener@@@Configure an XML Flattener to flatten XML data embedded in a string field...","datacollector/UserGuide/Processors/XMLParser.html@@@XML Parser@@@Configure an XML Parser to parse XML data in a string field...","datacollector/UserGuide/RPC_Pipelines/SDC_RPCpipelines_title.html@@@SDC RPC Pipelines@@@Data Collector Remote Protocol Call pipelines, called SDC RPC pipelines, are a set of StreamSets pipelines that pass data from one pipeline to another without writing to an intermediary system...","datacollector/UserGuide/Solutions/DeltaLake.html@@@Loading Data into Databricks Delta Lake@@@You can use several solutions to load data into a Delta Lake table on Databricks...","datacollector/UserGuide/Solutions/EventStorage.html@@@Preserving an Audit Trail of Events@@@This solution describes how to design a pipeline that preserves an audit trail of pipeline and stage events that occur...","datacollector/UserGuide/Solutions/FileManagement.html@@@Managing Output Files@@@This solution describes how to design a pipeline that writes output files to a destination, moves the files to a different location, and then changes the permissions for the files...","datacollector/UserGuide/Solutions/HiveDrift-Overview.html@@@Drift Synchronization Solution for Hive@@@The Drift Synchronization Solution for Hive detects drift in incoming data and updates corresponding Hive tables...","datacollector/UserGuide/Solutions/Impala.html@@@Automating Impala Metadata Updates for Drift Synchronization for Hive@@@This solution describes how to configure a Drift Synchronization Solution for Hive pipeline to automatically refresh the Impala metadata cache each time changes occur in the Hive metastore...","datacollector/UserGuide/Solutions/JDBC_DriftSyncSolution.html@@@Drift Synchronization Solution for PostgreSQL@@@The Drift Synchronization Solution for PostgreSQL detects drift in incoming data and automatically creates or alters corresponding PostgreSQL tables as needed before the data is written...","datacollector/UserGuide/Solutions/Overview.html@@@Solutions Overview@@@This chapter includes the following solutions that describe how to design pipelines for common use cases: Converting Data to the Parquet Data Format Automating Impala Metadata Updates for Drift...","datacollector/UserGuide/Solutions/Parquet.html@@@Converting Data to the Parquet Data Format@@@This solution describes how to convert Avro files to the columnar format,\n        Parquet...","datacollector/UserGuide/Solutions/SendEmail.html@@@Sending Email During Pipeline Processing@@@This solution describes how to design a pipeline to send email notifications at different moments during pipeline processing...","datacollector/UserGuide/Solutions/Solutions-title.html@@@Solutions@@@...","datacollector/UserGuide/Solutions/SqoopReplacement.html@@@Offloading Data from Relational Sources to Hadoop@@@This solution describes how to offload data from relational database tables to Hadoop...","datacollector/UserGuide/Solutions/StopPipeline.html@@@Stopping a Pipeline After Processing All Available Data@@@This solution describes how to design a pipeline that stops automatically after it finishes processing all available data...","datacollector/UserGuide/Troubleshooting/Troubleshooting_title.html@@@Troubleshooting@@@Informational and error messages display in different locations based on the type of information: Pipeline configuration issues The Data Collector UI provides guidance and error details as follows...","datacollector/UserGuide/Tutorial/BasicTutorial.html@@@Basic Tutorial@@@The basic tutorial creates a pipeline that reads a file from a directory, processes the data in two branches, and writes all data to a file system. You&apos;ll use data preview to help configure the pipeline, and you&apos;ll create a data alert and run the pipeline...","datacollector/UserGuide/Tutorial/BeforeYouBegin.html@@@Before You Begin@@@Before you start this tutorial, you&apos;ll need to do a few things: Download sample data. You can download sample data from the following location...","datacollector/UserGuide/Tutorial/ExtendedTutorial.html@@@Extended Tutorial@@@The extended tutorial builds on the basic tutorial, using an additional set of stages to perform some data transformations and write to the Trash development destination. We&apos;ll also use data preview to test stage configuration...","datacollector/UserGuide/Tutorial/Overview.html@@@Tutorial Overview@@@This tutorial walks through creating and running a pipeline. You can download sample data so you can perform data preview, run the completed pipeline, and monitor the results...","datacollector/UserGuide/Tutorial/Tutorial-title.html@@@Tutorial@@@...","datacollector/UserGuide/Upgrade/CMUpgrade.html@@@Upgrade an Installation with Cloudera Manager@@@Stop all pipelines running on the Data Collector to be upgraded...","datacollector/UserGuide/Upgrade/PostUpgrade.html@@@Post Upgrade Tasks@@@In some situations, you must complete tasks within Data Collector or your Control Hub on-premises installation after you upgrade...","datacollector/UserGuide/Upgrade/PreUpgrade.html@@@Pre Upgrade Tasks@@@In some situations, you must complete tasks before you upgrade...","datacollector/UserGuide/Upgrade/RPM.html@@@Upgrade an Installation from the RPM Package@@@Stop all pipelines and then shut down the previous version of Data Collector...","datacollector/UserGuide/Upgrade/Tarball.html@@@Upgrade an Installation from the Tarball@@@Stop all running pipelines and then shut down the previous version of Data Collector...","datacollector/UserGuide/Upgrade/Upgrade-ExternalSystems.html@@@Working with Upgraded External Systems@@@When an external system is upgraded to a new version, you can continue to use existing Data Collector pipelines that connected to the previous version of the external system. You simply configure the pipelines to work with the upgraded system...","datacollector/UserGuide/Upgrade/Upgrade.html@@@Upgrade@@@You can upgrade a previous version of Data Collector to a new version. You can upgrade an installation from the tarball, an installation from the RPM package, or an installation with Cloudera Manager...","datacollector/UserGuide/Upgrade/UpgradeTroubleshooting.html@@@Troubleshooting an Upgrade@@@Use the following tips for help with upgrades: After upgrading a Data Collector that is registered with StreamSets Control Hub , the Data Collector fails to start with the following error about a...","datacollector/UserGuide/Upgrade/Upgrade_title.html@@@Upgrade@@@...","datacollector/UserGuide/WhatsNew/WhatsNew_Title.html@@@What&apos;s New@@@Data Collector version 3.19.x includes the following new features and enhancements: StreamSets Accounts StreamSets Accounts enables users without an enterprise account to download the latest version..."];
});