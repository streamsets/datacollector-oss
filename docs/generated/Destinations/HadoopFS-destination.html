
<!DOCTYPE html
  SYSTEM "about:legacy-compat">
<html xml:lang="en-us" lang="en-us">
<head><meta name="description" content="The Hadoop FS destination writes data to the Hadoop Distributed File System (HDFS). You can write the data to HDFS as flat files or Hadoop sequence files. When you configure a Hadoop FS destination, ..."></meta><meta http-equiv="Content-Type" content="text/html; charset=utf-8"></meta><meta name="copyright" content="(C) Copyright 2005"></meta><meta name="DC.rights.owner" content="(C) Copyright 2005"></meta><meta name="DC.Type" content="concept"></meta><meta name="DC.Title" content="Hadoop FS"></meta><meta name="abstract" content="The Hadoop FS destination writes data to the Hadoop Distributed File System (HDFS). You can write the data to HDFS as flat files or Hadoop sequence files."></meta><meta name="description" content="The Hadoop FS destination writes data to the Hadoop Distributed File System (HDFS). You can write the data to HDFS as flat files or Hadoop sequence files."></meta><meta name="DC.Relation" scheme="URI" content="../Destinations/Destinations-title.html"></meta><meta name="DC.Relation" scheme="URI" content="Destinations_overview.html#concept_h4y_ycm_xs"></meta><meta name="DC.Format" content="XHTML"></meta><meta name="DC.Identifier" content="concept_awl_4km_zq"></meta><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/commonltr.css"><!----></link><title>Hadoop FS</title><meta http-equiv="Content-Type" content="text/html; charset=utf-8"></meta><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/webhelp_topic.css"><!----></link><link rel="stylesheet" type="text/css" href="../skin.css"></link><script type="text/javascript"><!--
          
          var prefix = "../index.html";
          
          --></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-1.8.2.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.cookie.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-ui.custom.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.highlight-3.js"><!----></script><script type="text/javascript" charset="utf-8" src="../oxygen-webhelp/resources/js/webhelp_topic.js"><!----></script></head>
<body onload="highlightSearchTerm()" class="frmBody">
<table class="nav"><tbody><tr><td colspan="2"><div id="printlink"><a href="javascript:window.print();" title="Print this page"></a></div><div id="permalink"><a href="#" title="Link to this page"></a></div></td></tr><tr><td width="75%"><a class="navheader_parent_path" href="../Destinations/Destinations-title.html" title="Destinations">Destinations</a></td><td><div class="navheader">
<span class="navparent"><a class="link" href="../Destinations/Destinations-title.html" title="Destinations"><span class="navheader_label">Parent topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Destinations</span></a></span>  </div></td></tr></tbody></table>
<div class="nested0" id="concept_awl_4km_zq">
 <h1 class="title topictitle1">Hadoop FS</h1>

 
 <div class="body conbody"><p class="shortdesc">The Hadoop FS destination writes data to the Hadoop Distributed File System (HDFS). You
    can write the data to HDFS as flat files or Hadoop sequence files. </p>

  <p class="p">When you configure a Hadoop FS destination, you define the output
   directory template. Hadoop FS creates output directories based the template and the time basis.
   Hadoop FS writes records to the directories based on the time basis as well.</p>

  <p class="p">You can specify the amount of time that a record can be written to its associated directory and
      what happens to late records. </p>

    <p class="p">When necessary, you can enable Kerberos authentication or use an
                  HDFS user to connect to HDFS. You can also use HDFS properties files and add other
                  HDFS configuration properties as needed. </p>

    <p class="p">If you want to use Snappy or LZO compression to write to HDFS, make sure the configuration
      requirements are complete. </p>

 </div>

  <div class="related-links"><div class="relinfo relconcepts"><strong>Related concepts</strong><br></br>
<div class="related_link"><a class="navheader_parent_path" href="Destinations_overview.html#concept_h4y_ycm_xs" title="You can use LZO or Snappy compression for Hadoop FS and HBase destinations. Or you can use Snappy compression for Cassandra.">LZO and Snappy Compression</a></div>
</div>
</div>
<div class="topic concept nested1" id="concept_xy5_4tm_vs">
 <h2 class="title topictitle2">Kerberos Authentication</h2>

 
 <div class="body conbody"><p class="shortdesc">You can use Kerberos authentication to connect to HDFS. When you use Kerberos
  authentication the <span class="ph">Data
                  Collector</span> uses the
  Kerberos principal and keytab to connect to HDFS.  </p>

  <p class="p">The Kerberos principal and keytab are defined in the <span class="ph">Data
                  Collector</span> configuration
   file. To use Kerberos authentication, configure all Kerberos properties in the <span class="ph">Data
                  Collector</span> configuration
   file. </p>

 </div>

 <div class="related-links"><div class="relinfo relconcepts"><strong>Related concepts</strong><br></br>
<div class="related_link"><a class="navheader_parent_path" href="../Install_Config/DCConfig.html#concept_hnm_n4l_xs" title="You can use Kerberos authentication to connect to origin and destination systems, as well as YARN clusters.">Enabling Kerberos Authentication</a></div>
</div>
</div>
</div>
<div class="topic concept nested1" id="concept_u4h_lwt_ls">
 <h2 class="title topictitle2">Using an HDFS User</h2>

 
 <div class="body conbody"><p class="shortdesc">You can configure the destination to use an HDFS user to write data to HDFS. By default,
    Hadoop FS uses the <span class="ph">Data
                  Collector</span> user. </p>

  <p class="p">The <span class="ph">Data
                  Collector</span> user is
      the user who started the <span class="ph">Data
                  Collector</span>. Or, when
      using Kerberos, the <span class="ph">Data
                  Collector</span> user is
      the Kerberos principal. </p>

  <div class="p">To use an HDFS user, perform the following tasks:<ol class="ol" id="concept_u4h_lwt_ls__ul_mb1_xpt_ls">
    <li class="li">On HDFS, configure the <span class="ph">Data
                  Collector</span> user as a
     proxy user and authorize the <span class="ph">Data
                  Collector</span> user to
     impersonate the HDFS user. <p class="p">For more information, see the HDFS documentation. </p>
</li>

    <li class="li">In the Hadoop FS destination, enter the HDFS user name.</li>

   </ol>
</div>

 </div>

</div>
<div class="topic concept nested1" id="concept_lww_3b3_kr">
 <h2 class="title topictitle2">Data Formats</h2>

 <div class="body conbody">
  <div class="p">Hadoop FS writes data to HDFS based on
      the data format that you select. You can use the following data formats: <dl class="dl">
                        
                              <dt class="dt dlterm">Text</dt>

                              <dd class="dd">The destination writes a single text field of a record. When you
                                    configure the stage, you select the field to use. When
                                    necessary, merge record data into the field earlier in the
                                    pipeline. </dd>

                        
                        
                              <dt class="dt dlterm">JSON</dt>

                              <dd class="dd">The destination writes records as JSON data. You can use one of
                                    the following formats:<ul class="ul" id="concept_lww_3b3_kr__d877e162">
                                          <li class="li">Array - Each file includes a single array. In the
                                                array, each element is a JSON representation of each
                                                record.</li>

                                          <li class="li">Multiple objects - Each file includes multiple JSON
                                                objects. Each object is a JSON representation of a
                                                record. </li>

                                    </ul>
</dd>

                        
                        
                              <dt class="dt dlterm">Delimited</dt>

                              <dd class="dd">The destination writes records as delimited data. When you use
                                    this data format, you must ensure that the data uses the
                                    following data structure:<ul class="ul" id="concept_lww_3b3_kr__d877e179">
                                          <li class="li">A root field that contains an array of maps.</li>

                                          <li class="li">Each map includes a <dfn class="term">value</dfn> element and an
                                                optional <dfn class="term">header</dfn> element.</li>

                                    </ul>
</dd>

                        
                        
                              <dt class="dt dlterm">SDC Record</dt>

                              <dd class="dd">The destination writes records in the SDC Record data format.
                              </dd>

                        
                  </dl>
</div>

    <dl class="dl">
      
        <dt class="dt dlterm">Avro</dt>

        <dd class="dd">The destination writes records based on the Avro schema that you provide. The schema
          definition is included in each file.</dd>

      
    </dl>

 </div>

</div>
<div class="topic concept nested1" id="concept_cvc_skd_br">
 <h2 class="title topictitle2">Directory Templates</h2>

 
 <div class="body conbody"><p class="shortdesc">The Hadoop FS destination uses directory templates to create output and late record
  directories. Hadoop FS writes records to the directories based on the configured time
  basis.</p>

  <p class="p">You can use a mix of constants, field values, and datetime variables
   in a directory template. You can use the <samp class="ph codeph">every</samp> function to create new
   directories at regular intervals based on seconds or minutes. You can also use the
    <samp class="ph codeph">record:valueOrDefault</samp> function to use field values or a default in the
   directory template. </p>

  <div class="p">For example, the following directory template creates output directories for event data based
   on the state and timestamp of a record with hours as the smallest unit of measure, creating a new
   directory every twelve
   hours:<pre class="pre codeblock"> /outputfiles/${record:valueOrDefault("/State", "unknown")}/${YY()}-${MM()}-${DD()}-${every(12,hh())}</pre>
</div>

  <div class="p">You can use the following elements in a directory template:<dl class="dl">
    
     <dt class="dt dlterm">Constants</dt>

     <dd class="dd">You can use any constant, such as "output" or "lateRecords."</dd>

    
    
     <dt class="dt dlterm">Datetime Variables</dt>

     <dd class="dd">Hadoop FS creates directories as needed, based on the smallest datetime variable that you
      use. For example, if the smallest variable is hours, then the directories are created for
      every hour of the day that receives output records.</dd>

     <dd class="dd">You can use the following datetime variables in a directory template:<ul class="ul" id="concept_cvc_skd_br__ul_s2x_5qq_1r">
       <li class="li">${YYYY()} - four digit year</li>

       <li class="li">${YY()} - two digit year</li>

       <li class="li">${MM()} - two digit month</li>

       <li class="li">${DD()} - two digit date</li>

       <li class="li">${hh()} - two digit hour</li>

       <li class="li">${mm()} - two digit minute</li>

       <li class="li">${ss()} - two digit second</li>

      </ul>
</dd>

     <dd class="dd">When you define a directory template, use all of the datetime variables between one of the
      year variables and the smallest variable that you want to use. For example, to create
      directories on a daily basis, you might use one of the following datetime variable
      progressions:</dd>

     <dd class="dd">
      <pre class="pre codeblock">${YYYY()}-${MM()}-${DD()}
${YY()}_${MM()}_${DD()}</pre>

     </dd>

    
    
     <dt class="dt dlterm">every() function</dt>

     <dd class="dd">You can use the <samp class="ph codeph">every()</samp> function in a directory template to create
      directories at regular intervals based on minutes or seconds. The intervals should be a
      submultiple or integer factor of 60. For example, you can create directories every 15 minutes
      or 30 seconds. </dd>

     <dd class="dd">Use the every() function to replace the smallest datetime variable used in the
      template.</dd>

     <dd class="dd">For example, the following directory template creates directories every 5
      minutes:<pre class="pre codeblock">/HDFS_output/${YYYY()}-${MM()}-${DD()}-${hh()}-${every(5,mm())}</pre>
</dd>

     <dd class="dd">For details about the <samp class="ph codeph">every()</samp> function, see <a class="xref" href="../Expression_Language/Functions.html#concept_ddw_ld1_1s">Miscellaneous Functions</a>.</dd>

    
    
     <dt class="dt dlterm">record:valueOrDefault function</dt>

     <dd class="dd">You can use the following expression to use the value of a field and the specified default
      value if the field does ont exist: ${record:valueOrDefault("/&lt;field name&gt;", &lt;default
      value&gt;)}.</dd>

     <dd class="dd">For example, the following directory template creates a directory based on the product
      field every day, and if the product field is empty, uses Misc in the directory path:
      <pre class="pre codeblock">/${record:valueOrDefault("/Product", "Misc")}/${YY()}-${MM()}-${DD()}</pre>
</dd>

     <dd class="dd">This template might create the following
      paths:<pre class="pre codeblock">/Shirts/2015-07-31 
/Misc/2015-07-31</pre>
</dd>

     <dd class="dd">For a tip on how to replace null values, see <a class="xref" href="../Expression_Language/Functions.html#concept_p1z_ggv_1r" title="Use record functions to determine information about a record, such as the stage that created it or whether a field exists in the record.">Record Functions</a>.</dd>

    
   </dl>
</div>

 </div>

</div>
<div class="topic concept nested1" id="concept_gkz_smd_br">
 <h2 class="title topictitle2">Time Basis</h2>

 
 <div class="body conbody"><p class="shortdesc">The time basis helps determine when directories are created and which directory Hadoop
  FS uses when writing a record. You can use the following times as the time basis: </p>

  <div class="p">
   <dl class="dl">
    
     <dt class="dt dlterm">Processing Time</dt>

     <dd class="dd">When you use processing time as the time basis, Hadoop FS creates
      directories based on the processing time and the directory template, and writes records to the
      directories based on when they are processed.</dd>

     <dd class="dd">For example, say a directory template creates directories every minute and the time basis
      is the time of processing. Then, directories are created for every minute that Hadoop FS
      writes output records. And the output records are written to the directory for that minute of
      processing. </dd>

     <dd class="dd">To use the processing time as the time basis, use the following expression:
       <samp class="ph codeph">${time:now()}</samp>. This is the default time basis. </dd>

    
    
     <dt class="dt dlterm">Record Time</dt>

     <dd class="dd">When you use the time associated with a record as the time basis, you specify a Date field
      in the record. Hadoop FS creates directories based on the datetimes associated with the
      records and writes the records to the appropriate directories. </dd>

     <dd class="dd">For example, say a directory template creates directories every hour and the time basis is
      based on the record. Then, directories are created for every hour associated with output
      records and Hadoop FS writes the records to the related output directory. </dd>

     <dd class="dd">To use a time associated with the record, use an expression that calls a field and resolves
      to a datetime value, such as <samp class="ph codeph">${record:value("/Timestamp")}</samp>. </dd>

    
   </dl>

  </div>

 </div>

</div>
<div class="topic concept nested1" id="concept_xgm_g4d_br">
 <h2 class="title topictitle2">Late Records and Late Record Handling </h2>

 
 <div class="body conbody"><p class="shortdesc">You can define a time limit for records to be written to its associated output
    directory. Any record that arrives past this limit is considered late. This limit is appropriate
    when you use the time of the record as the time basis. </p>

  <p class="p">For example, a Hadoop FS destination writes event data to hourly
      output directories based on the timestamp of the event, and it has a time limit of one day.
      Any records that arrive more than a day after the hourly output directory window are
      considered late. </p>

  <p class="p">You can send late records to a late records file or to the stage for error handling. When you
            send records to a late records file, you define a late records directory template. </p>

 </div>

</div>
<div class="topic concept nested1" id="concept_xh5_y4d_br">
 <h2 class="title topictitle2">Hadoop Properties and Configuration Files</h2>

    
    <div class="body conbody"><p class="shortdesc">You can configure the Hadoop destination to use HDFS properties. </p>

        <div class="p">The Hadoop FS destination provides the
            following options for using HDFS properties:<dl class="dl">
                
                    <dt class="dt dlterm">HDFS configuration files: core-site.xml and hdfs-site.xml</dt>

                    <dd class="dd">You can store the core-site.xml and hdfs-site.xml configuration files in a
                        directory local to the <span class="ph">Data
                  Collector</span> and define the directory in the destination.</dd>

                
                
                    <dt class="dt dlterm">Individual properties</dt>

                    <dd class="dd">You can configure individual HDFS properties in the destination. To add an
                        HDFS property, you specify the exact property name and the value. The Hadoop
                        FS destination does not validate the property names or
                            values.<div class="note note"><span class="notetitle">Note:</span> Individual properties override properties defined in the
                            HDFS configuration file. </div>
</dd>

                
            </dl>
</div>

    </div>

</div>
<div class="topic task nested1" id="task_m2m_skm_zq">
    <h2 class="title topictitle2">Configuring a Hadoop FS Destination</h2>

    <div class="body taskbody">
        <div class="section context">
            <p class="p">Configure a Hadoop FS
                destination to write data to HDFS.</p>

        </div>

        <ol class="ol steps" id="task_m2m_skm_zq__steps_ljw_44d_br"><li class="li step stepexpand">
                <span class="ph cmd">In the Properties panel, on the <span class="keyword wintitle">General</span> tab, configure the
                    following properties:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_m2m_skm_zq__d862e581" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr class="row">
                                    <th class="entry" valign="top" width="24.509803921568626%" id="d11336e598">General Property</th>

                                    <th class="entry" valign="top" width="75.49019607843137%" id="d11336e601">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr class="row">
                                    <td class="entry" valign="top" width="24.509803921568626%" headers="d11336e598 ">Name</td>

                                    <td class="entry" valign="top" width="75.49019607843137%" headers="d11336e601 ">Stage name.</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="24.509803921568626%" headers="d11336e598 ">Description</td>

                                    <td class="entry" valign="top" width="75.49019607843137%" headers="d11336e601 ">Optional description.</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="24.509803921568626%" headers="d11336e598 ">Stage Library</td>

                                    <td class="entry" valign="top" width="75.49019607843137%" headers="d11336e601 ">Library version that you want to use. </td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="24.509803921568626%" headers="d11336e598 ">Required Fields <a class="xref" href="../Pipeline_Configuration/PipelineDesign.html#concept_dnj_bkm_vq" title="A required field is a field that must exist in a record to allow it into the stage for processing. When a record does not include a required field, the record is diverted to the pipeline for error handling. You can define required fields for any processor and most destination stages.">
                                            <img class="image" id="task_m2m_skm_zq__d862e636" src="../Reusable_Content/../Graphics/icon_moreInfo.png" height="12" width="12"></img></a></td>

                                    <td class="entry" valign="top" width="75.49019607843137%" headers="d11336e601 ">Fields that must include data to be passed into the
                                        stage. <div class="note tip"><span class="tiptitle">Tip:</span> You might include
                                            fields that the stage uses.</div>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="24.509803921568626%" headers="d11336e598 ">Preconditions <a class="xref" href="../Pipeline_Configuration/PipelineDesign.html#concept_msl_yd4_fs" title="Preconditions are conditions that a record must satisfy to enter the stage for processing. Like required fields, if a record does not meet a precondition, it is diverted to the pipeline for error handling. You can define preconditions for any processor and most destination stages.">
                                            <img class="image" id="task_m2m_skm_zq__d862e650" src="../Reusable_Content/../Graphics/icon_moreInfo.png" height="12" width="12"></img></a></td>

                                    <td class="entry" valign="top" width="75.49019607843137%" headers="d11336e601 ">Conditions that must evaluate to TRUE to allow a record
                                        to enter the stage for processing. Click
                                            <span class="ph uicontrol">Add</span> to create additional
                                        preconditions. </td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="24.509803921568626%" headers="d11336e598 ">On Record Error</td>

                                    <td class="entry" valign="top" width="75.49019607843137%" headers="d11336e601 ">Error record handling for the stage: <ul class="ul" id="task_m2m_skm_zq__d862e667">
                                            <li class="li">Discard - Discards the record.</li>

                                            <li class="li">Send to Error - Sends the record to the pipeline for
                                                error handling.</li>

                                            <li class="li">Stop Pipeline - Stops the pipeline. Not valid for a
                                                cluster pipeline.</li>

                                        </ul>
</td>

                                </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">On the <span class="keyword wintitle">Hadoop FS</span> tab, configure the following
                    properties:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_m2m_skm_zq__table_rst_t4d_br" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr class="row">
                                    <th class="entry" valign="top" width="25.97402597402597%" id="d11336e720">Hadoop FS Property</th>

                                    <th class="entry" valign="top" width="74.02597402597402%" id="d11336e723">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr class="row">
                                    <td class="entry" valign="top" width="25.97402597402597%" headers="d11336e720 ">Hadoop FS URI</td>

                                    <td class="entry" valign="top" width="74.02597402597402%" headers="d11336e723 ">HDFS URI.</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="25.97402597402597%" headers="d11336e720 ">HDFS User <a class="xref" href="HadoopFS-destination.html#concept_u4h_lwt_ls" title="You can configure the destination to use an HDFS user to write data to HDFS. By default, Hadoop FS uses the Data Collector user.">
                                            <img class="image" id="task_m2m_skm_zq__image_byg_yqg_xs" src="../Graphics/icon_moreInfo.png" height="12" width="12"></img></a></td>

                                    <td class="entry" valign="top" width="74.02597402597402%" headers="d11336e723 ">The HDFS user to use to connect to HDFS. When using this
                                        property, make sure HDFS is configured appropriately.<p class="p">By
                                            default, the pipeline uses the <span class="ph">Data
                  Collector</span> user to connect to HDFS.</p>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="25.97402597402597%" headers="d11336e720 ">Kerberos Authentication<a class="xref" href="HadoopFS-destination.html#concept_xy5_4tm_vs" title="You can use Kerberos authentication to connect to HDFS. When you use Kerberos authentication the Data Collector uses the Kerberos principal and keytab to connect to HDFS.">
                                            <img class="image" id="task_m2m_skm_zq__image_a5x_jzn_vs" src="../Graphics/icon_moreInfo.png" height="12" width="12"></img></a></td>

                                    <td class="entry" valign="top" width="74.02597402597402%" headers="d11336e723 ">Uses Kerberos credentials to connect to HDFS. <p class="p">When
                                            selected, uses the Kerberos principal and keytab defined
                                            in the <span class="ph">Data
                  Collector</span> configuration file. </p>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="25.97402597402597%" headers="d11336e720 ">Hadoop FS Configuration Directory <a class="xref" href="HadoopFS-destination.html#concept_xh5_y4d_br" title="You can configure the Hadoop destination to use HDFS properties.">
                                            <img class="image" id="task_m2m_skm_zq__image_br4_fgs_5r" src="../Graphics/icon_moreInfo.png" height="12" width="12"></img></a></td>

                                    <td class="entry" valign="top" width="74.02597402597402%" headers="d11336e723 ">Directory for a local HDFS configuration files. To use
                                        properties defined in the core-site.xml and hdfs-site.xml
                                        configuration files, enter the directory where the files are
                                        located. <div class="note note"><span class="notetitle">Note:</span> Properties in the configuration files are
                                            overridden by individual properties defined in the
                                            stage.</div>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="25.97402597402597%" headers="d11336e720 ">Hadoop FS Configuration</td>

                                    <td class="entry" valign="top" width="74.02597402597402%" headers="d11336e723 ">Additional HDFS properties to use. <p class="p">To add properties,
                                            click <span class="ph uicontrol">Add</span> and define the property
                                            name and value. Use the property names and values as
                                            expected by HDFS.</p>
</td>

                                </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">On the <span class="keyword wintitle">Output Files</span> tab, configure the following
                    options:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_m2m_skm_zq__table_byd_xpd_br" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr class="row">
                                    <th class="entry" valign="top" width="25.97402597402597%" id="d11336e846">Output Files Property</th>

                                    <th class="entry" valign="top" width="74.02597402597402%" id="d11336e849">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr class="row">
                                    <td class="entry" valign="top" width="25.97402597402597%" headers="d11336e846 ">Data Format</td>

                                    <td class="entry" valign="top" width="74.02597402597402%" headers="d11336e849 ">Format of data to be written. Use one of the following
                                            options:<ul class="ul" id="task_m2m_skm_zq__ul_un2_cqd_br">
                                            <li class="li">Text</li>

                                            <li class="li">JSON</li>

                                            <li class="li">Delimited</li>

                                            <li class="li">SDC Record <a class="xref" href="../Pipeline_Configuration/PipelineDesign.html#concept_qkk_mwk_br" title="SDC Record is a proprietary data format that the Data Collector uses to generate error records. The Data Collector can also use the data format to read and write data.">
                                                  <img class="image" id="task_m2m_skm_zq__image_wjh_ycl_br" src="../Graphics/icon_moreInfo.png" height="12" width="12"></img></a></li>

                                            <li class="li">Avro</li>

                                        </ul>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="25.97402597402597%" headers="d11336e846 ">File Type</td>

                                    <td class="entry" valign="top" width="74.02597402597402%" headers="d11336e849 ">Output file type:<ul class="ul" id="task_m2m_skm_zq__ul_lgf_j3g_br">
                                            <li class="li">Text files</li>

                                            <li class="li">Sequence files</li>

                                        </ul>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="25.97402597402597%" headers="d11336e846 ">Files Prefix</td>

                                    <td class="entry" valign="top" width="74.02597402597402%" headers="d11336e849 ">Prefix to use for output files. Use when writing to a
                                        directory that receives files from other sources.<p class="p">Uses the
                                            prefix sdc-${sdc:id()} by default. The prefix evaluates
                                            to sdc-&lt;Data Collector ID&gt;. </p>
<p class="p">The Data Collector
                                            ID is stored in the following file:
                                                <span class="ph filepath">&lt;SDCinstalldir&gt;/data/sdc.id</span>.
                                        </p>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="25.97402597402597%" headers="d11336e846 ">Data Charset</td>

                                    <td class="entry" valign="top" width="74.02597402597402%" headers="d11336e849 ">Character encoding to use when writing data. <p class="p">Not used
                                            for the SDC Record data format.</p>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="25.97402597402597%" headers="d11336e846 ">Directory Template <a class="xref" href="HadoopFS-destination.html#concept_cvc_skd_br" title="The Hadoop FS destination uses directory templates to create output and late record directories. Hadoop FS writes records to the directories based on the configured time basis.">
                                            <img class="image" id="task_m2m_skm_zq__image_c4p_p5v_yq" src="../Graphics/icon_moreInfo.png" height="12" width="12"></img></a></td>

                                    <td class="entry" valign="top" width="74.02597402597402%" headers="d11336e849 ">Template for creating output directories. You can use
                                        constants, field values, and datetime variables. <p class="p">Output
                                            directories are created based on the smallest datetime
                                            variable in the template.</p>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="25.97402597402597%" headers="d11336e846 ">Data Time Zone</td>

                                    <td class="entry" valign="top" width="74.02597402597402%" headers="d11336e849 ">Time zone to use to create directories and evaluate where
                                        records are written.</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="25.97402597402597%" headers="d11336e846 ">Time Basis <a class="xref" href="HadoopFS-destination.html#concept_gkz_smd_br" title="The time basis helps determine when directories are created and which directory Hadoop FS uses when writing a record. You can use the following times as the time basis:">
                                            <img class="image" src="../Graphics/icon_moreInfo.png" height="12" width="12"></img></a></td>

                                    <td class="entry" valign="top" width="74.02597402597402%" headers="d11336e849 ">Time basis to use for creating output directories and
                                        writing records to the directories. Use one of the following
                                            expressions:<ul class="ul" id="task_m2m_skm_zq__ul_ggs_43g_br">
                                            <li class="li">${time:now()} - Uses the processing time as the time
                                                basis. </li>

                                            <li class="li">${record:value("/&lt;date field&gt;")} - Uses the time
                                                associated with the record as the time basis.</li>

                                        </ul>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="25.97402597402597%" headers="d11336e846 ">Max Records in a File</td>

                                    <td class="entry" valign="top" width="74.02597402597402%" headers="d11336e849 ">Maximum number of records to be written to an output
                                        file. Additional records are written to a new file. <p class="p">Use 0
                                            to opt out of this property.</p>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="25.97402597402597%" headers="d11336e846 ">Max File Size (MB)</td>

                                    <td class="entry" valign="top" width="74.02597402597402%" headers="d11336e849 ">Maximum size of an output file. Additional records are
                                        written to a new file. <p class="p">Use 0 to opt out of this
                                            property.</p>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="25.97402597402597%" headers="d11336e846 ">Compression Codec</td>

                                    <td class="entry" valign="top" width="74.02597402597402%" headers="d11336e849 ">Program to use to compress output files:<ul class="ul" id="task_m2m_skm_zq__ul_ltx_djg_br">
                                            <li class="li">None </li>

                                            <li class="li">gzip</li>

                                            <li class="li">bzip2</li>

                                            <li class="li">Snappy</li>

                                            <li class="li">Other - use for LZO or other compression types.
                                            </li>

                                        </ul>
<p class="p">LZO and Snappy compression require additional
                                            configuration. For more information, see <a class="xref" href="Destinations_overview.html#concept_h4y_ycm_xs" title="You can use LZO or Snappy compression for Hadoop FS and HBase destinations. Or you can use Snappy compression for Cassandra.">LZO and Snappy Compression</a>.</p>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="25.97402597402597%" headers="d11336e846 ">Sequence File Key</td>

                                    <td class="entry" valign="top" width="74.02597402597402%" headers="d11336e849 ">Record key for creating Hadoop sequence files. Use one of
                                        the following options:<ul class="ul" id="task_m2m_skm_zq__ul_xzr_vkg_br">
                                            <li class="li">${record:value("/&lt;field name&gt;")}</li>

                                            <li class="li">${uuid()}</li>

                                        </ul>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="25.97402597402597%" headers="d11336e846 ">Compression Type</td>

                                    <td class="entry" valign="top" width="74.02597402597402%" headers="d11336e849 ">Compression type for sequence files when using a
                                        compression codec:<ul class="ul" id="task_m2m_skm_zq__ul_etm_1lg_br">
                                            <li class="li">Block Compression</li>

                                            <li class="li">Record Compression</li>

                                        </ul>
</td>

                                </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">On the <span class="keyword wintitle">Late Records</span> tab, configure the following
                    properties:</span>
                <div class="itemgroup info">
                    <div class="note note"><span class="notetitle">Note:</span> These properties are relevant for a time basis based on the time of a
                        record.</div>

                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_m2m_skm_zq__table_wv3_xzd_br" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr class="row">
                                    <th class="entry" valign="top" width="26.954177897574123%" id="d11336e1113">Late Records Property <a class="xref" href="HadoopFS-destination.html#concept_xgm_g4d_br" title="You can define a time limit for records to be written to its associated output directory. Any record that arrives past this limit is considered late. This limit is appropriate when you use the time of the record as the time basis.">
                                            <img class="image" src="../Graphics/icon_moreInfo.png" height="12" width="12"></img>
                                        </a>
                                    </th>

                                    <th class="entry" valign="top" width="73.04582210242587%" id="d11336e1125">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr class="row">
                                    <td class="entry" valign="top" width="26.954177897574123%" headers="d11336e1113 ">Late Record Time Limit (secs)</td>

                                    <td class="entry" valign="top" width="73.04582210242587%" headers="d11336e1125 ">Time limit for output directories to accept data. <p class="p">You
                                            can enter a time in seconds, or use the expression to
                                            enter a time in hours. You can also use MINUTES in the
                                            default expression to define the time in minutes.
                                        </p>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="26.954177897574123%" headers="d11336e1113 ">Late Record Handling</td>

                                    <td class="entry" valign="top" width="73.04582210242587%" headers="d11336e1125 ">Determines how to handle late records:<ul class="ul" id="task_m2m_skm_zq__ul_gx4_c12_br">
                                            <li class="li">Send to error - Sends the record to the stage for
                                                error handling. </li>

                                            <li class="li">Send to late records file - Sends the record to a
                                                late records file.</li>

                                        </ul>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="26.954177897574123%" headers="d11336e1113 ">Late Record Directory Template <a class="xref" href="HadoopFS-destination.html#concept_cvc_skd_br" title="The Hadoop FS destination uses directory templates to create output and late record directories. Hadoop FS writes records to the directories based on the configured time basis.">
                                            <img class="image" src="../Graphics/icon_moreInfo.png" height="12" width="12"></img></a></td>

                                    <td class="entry" valign="top" width="73.04582210242587%" headers="d11336e1125 ">Template for creating late record directories. You can
                                        use constants, field values, and datetime variables.
                                            <p class="p">Output directories are created based on the smallest
                                            datetime variable in the template.</p>
</td>

                                </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">For text data, click the <span class="ph uicontrol">Text</span> tab and configure the
                    following properties:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_m2m_skm_zq__d862e930" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr class="row">
                                    <th class="entry" valign="top" width="23.980815347721823%" id="d11336e1205">Text Property</th>

                                    <th class="entry" valign="top" width="76.01918465227818%" id="d11336e1208">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr class="row">
                                    <td class="entry" valign="top" width="23.980815347721823%" headers="d11336e1205 ">Text Field Path</td>

                                    <td class="entry" valign="top" width="76.01918465227818%" headers="d11336e1208 ">Field that contains the text data to be written. All data
                                        must be incorporated into the specified field. </td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="23.980815347721823%" headers="d11336e1205 ">Empty Line if No Text</td>

                                    <td class="entry" valign="top" width="76.01918465227818%" headers="d11336e1208 ">Creates an empty line when a record does not include the
                                        text field specified above. <p class="p">When not selected, records
                                            without the specified text field are
                                        discarded.</p>
</td>

                                </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">For JSON data, click the <span class="ph uicontrol">JSON</span> tab and configure the
                    following property:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_m2m_skm_zq__d862e996" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr class="row">
                                    <th class="entry" valign="top" width="28.011204481792717%" id="d11336e1263">JSON Property</th>

                                    <th class="entry" valign="top" width="71.98879551820728%" id="d11336e1266">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr class="row">
                                    <td class="entry" valign="top" width="28.011204481792717%" headers="d11336e1263 ">JSON Content</td>

                                    <td class="entry" valign="top" width="71.98879551820728%" headers="d11336e1266 ">Determines how JSON data is written:<ul class="ul" id="task_m2m_skm_zq__d862e1025">
                                            <li class="li">JSON Array of Objects - Each file includes a single
                                                array. In the array, each element is a JSON
                                                representation of each record.</li>

                                            <li class="li">Multiple JSON Objects - Each file includes multiple
                                                JSON objects. Each object is a JSON representation
                                                of a record.</li>

                                        </ul>
</td>

                                </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">For delimited data, click the <span class="keyword wintitle">Delimited</span> tab and configure
                    the following properties:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_m2m_skm_zq__d862e1060" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr class="row">
                                    <th class="entry" valign="top" width="28.328611898016998%" id="d11336e1317">Delimited Property</th>

                                    <th class="entry" valign="top" width="71.671388101983%" id="d11336e1320">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr class="row">
                                    <td class="entry" valign="top" width="28.328611898016998%" headers="d11336e1317 ">CSV Format</td>

                                    <td class="entry" valign="top" width="71.671388101983%" headers="d11336e1320 ">Format for delimited data:<ul class="ul" id="task_m2m_skm_zq__ul_k3j_vvf_jr">
                        <li class="li"><span class="ph uicontrol">Default CSV</span> - File that includes comma-separated
                              values. Ignores empty lines in the file.</li>

                        <li class="li"><span class="ph uicontrol">RFC4180 CSV</span> - Comma-separated file that strictly
                              follows RFC4180 guidelines.</li>

                        <li class="li"><span class="ph uicontrol">MS Excel CSV</span> - Microsoft Excel comma-separated
                              file.</li>

                        <li class="li"><span class="ph uicontrol">MySQL CSV</span> - MySQL comma separated file.</li>

                        <li class="li"><span class="ph uicontrol">Tab-Separated Values</span> - File that includes
                              tab-separated values.</li>

                        <li class="li"><span class="ph uicontrol">Custom</span> - File that uses user-defined delimiter,
                              escape, and quote characters.</li>

                  </ul>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="28.328611898016998%" headers="d11336e1317 ">Header Line</td>

                                    <td class="entry" valign="top" width="71.671388101983%" headers="d11336e1320 ">Indicates whether to create a header line.</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="28.328611898016998%" headers="d11336e1317 ">Remove New Line Characters</td>

                                    <td class="entry" valign="top" width="71.671388101983%" headers="d11336e1320 ">Removes new line characters from within a record.
                                            <p class="p">Recommended when writing data as a single line of
                                            text.</p>
</td>

                                </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
<li class="li step stepexpand" id="task_m2m_skm_zq__AvroProps">
                <span class="ph cmd">For Avro data, click the <span class="keyword wintitle">Avro</span> tab and configure the
                    following properties:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_m2m_skm_zq__table_fpg_rx3_ks" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr class="row">
                                    <th class="entry" valign="top" width="27.027027027027025%" id="d11336e1416">Avro Property</th>

                                    <th class="entry" valign="top" width="72.97297297297297%" id="d11336e1419">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr class="row">
                                    <td class="entry" valign="top" width="27.027027027027025%" headers="d11336e1416 ">Avro Schema</td>

                                    <td class="entry" valign="top" width="72.97297297297297%" headers="d11336e1419 ">Schema definition to use when writing data. Hadoop FS
                                        includes the schema definition in each generated file.
                                    </td>

                                </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
</ol>

    </div>

</div>
</div>
<div class="navfooter"><!---->
<span class="navparent"><a class="link" href="../Destinations/Destinations-title.html" title="Destinations"><span class="navheader_label">Parent topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Destinations</span></a></span>  </div><div class="footer"><div> </div><!--  <a href="http://creativecommons.org/licenses/by-nc/4.0/legalcode">CC BY-NC 4.0.</a> StreamSets, 2015. --></div>
</body>
</html>