
<!DOCTYPE html
  SYSTEM "about:legacy-compat">
<html xml:lang="en-us" lang="en-us">
<head><meta name="description" content="The Hadoop FS destination writes data to the Hadoop Distributed File System (HDFS). You can write the data to HDFS as flat files or Hadoop sequence files. When you configure a Hadoop FS destination, ..."></meta><meta http-equiv="Content-Type" content="text/html; charset=utf-8"></meta><meta name="copyright" content="(C) Copyright 2005"></meta><meta name="DC.rights.owner" content="(C) Copyright 2005"></meta><meta name="DC.Type" content="concept"></meta><meta name="DC.Title" content="Hadoop FS"></meta><meta name="abstract" content="The Hadoop FS destination writes data to the Hadoop Distributed File System (HDFS). You can write the data to HDFS as flat files or Hadoop sequence files."></meta><meta name="description" content="The Hadoop FS destination writes data to the Hadoop Distributed File System (HDFS). You can write the data to HDFS as flat files or Hadoop sequence files."></meta><meta name="DC.Relation" scheme="URI" content="../Destinations/Destinations-title.html"></meta><meta name="DC.Relation" scheme="URI" content="../Hive_Metadata/HiveDrift-Overview.html#concept_phk_bdf_2w"></meta><meta name="DC.Format" content="XHTML"></meta><meta name="DC.Identifier" content="concept_awl_4km_zq"></meta><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/commonltr.css"><!----></link><title>Hadoop FS</title><!--  Generated with Oxygen version 17.1, build number 2016020417.  --><meta http-equiv="Content-Type" content="text/html; charset=utf-8"></meta><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/webhelp_topic.css"><!----></link><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/skins/skin.css"><!----></link><link rel="stylesheet" type="text/css" href="../skin.css"></link><script type="text/javascript"><!--
          
          var prefix = "../index.html";
          
          --></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-1.8.2.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.cookie.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-ui.custom.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.highlight-3.js"><!----></script><script type="text/javascript" charset="utf-8" src="../oxygen-webhelp/resources/js/webhelp_topic.js"><!----></script><!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
--></head>
<body onload="highlightSearchTerm()" class="frmBody"><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
​
  ga('create', 'UA-53969024-1', 'auto');
  ga('send', 'pageview');
​
</script>
<table class="nav"><tbody><tr><td colspan="2"><div id="printlink"><a href="javascript:window.print();" title="Print this page"></a></div><div id="permalink"><a href="#" title="Link to this page"></a></div></td></tr><tr><td width="75%"><a class="navheader_parent_path" href="../Destinations/Destinations-title.html" title="Destinations">Destinations</a></td><td><div class="navheader">
<span class="navparent"><a class="link" href="../Destinations/Destinations-title.html" title="Destinations"><span class="navheader_label">Parent topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Destinations</span></a></span>  </div></td></tr></tbody></table>
<div class="nested0" id="concept_awl_4km_zq">
 <h1 class="title topictitle1">Hadoop FS</h1>

 
 <div class="body conbody"><p class="shortdesc">The Hadoop FS destination writes data to the Hadoop Distributed File System (HDFS). You
    can write the data to HDFS as flat files or Hadoop sequence files. </p>

  <p class="p">When you configure a Hadoop FS destination, you can
            define a directory template and time basis to determine the output directories that the
            destination creates and the files where records are written. </p>

        <p class="p">Alternatively, you can use a stage attribute in record headers to specify where records
            are written. You can also use stage attributes to indicate when to roll a file and the
            schema to use for Avro data as part of the Hive Drift Solution.</p>

        <p class="p">You can define a file prefix, the data time zone, and properties that define when the
            destination closes a file. You can specify the amount of time that a record can be
            written to its associated directory and what happens to late records.</p>

    <p class="p">When necessary, you can enable Kerberos authentication or use an
                  HDFS user to connect to HDFS. You can also use HDFS configuration files and add
                  other HDFS configuration properties as needed. </p>

    <p class="p">You can use Gzip, Bzip2, Snappy, LZ4, and other compression formats to write output files.
                </p>

 </div>

    <div class="related-links"><div class="relinfo relconcepts"><strong>Related concepts</strong><br xmlns="http://www.w3.org/1999/xhtml" />
<div class="related_link"><a class="navheader_parent_path" href="../Hive_Metadata/HiveDrift-Overview.html#concept_phk_bdf_2w" title="Hive Drift Solution: Ingesting Drifting Data into Hive">Hive Drift Solution: Ingesting Drifting Data into Hive</a></div>
</div>
</div>
<div class="topic concept nested1" id="concept_cvc_skd_br">
 <h2 class="title topictitle2">Directory Templates</h2>

 <div class="body conbody">
  <p class="p">By default, the Hadoop FS destination uses directory templates
      to create output and late record directories. Hadoop FS writes records to the directories
      based on the configured time basis. </p>

    <p class="p">You can alternatively write records to directories based on the targetDirectory stage
      attribute. Using the targetDirectory attribute disables the ability to define directory
      templates.</p>

  <p class="p">When you define a directory template, you can use a mix
                  of constants, field values, and datetime variables. You can use the
                        <samp class="ph codeph">every</samp> function to create new directories at regular
                  intervals based on seconds or minutes. You can also use the
                        <samp class="ph codeph">record:valueOrDefault</samp> function to use field values or a
                  default in the directory template. </p>

  <div class="p">For example, the following directory template creates
                  output directories for event data based on the state and timestamp of a record
                  with hours as the smallest unit of measure, creating a new directory every twelve
                  hours:<pre class="pre codeblock"> /outputfiles/${record:valueOrDefault("/State", "unknown")}/${YY()}-${MM()}-${DD()}-${every(12,hh())}</pre>
</div>

  <div class="p">You can use the following elements in a directory template:<dl class="dl">
                        
                              <dt class="dt dlterm">Constants</dt>

                              <dd class="dd">You can use any constant, such as "output" or "lateRecords."</dd>

                        
                        
                              <dt class="dt dlterm">Datetime Variables</dt>

                              <dd class="dd">You can use datetime variables, such as <samp class="ph codeph">${YYYY()}</samp>
                                    or <samp class="ph codeph">${DD()}</samp>. The destination creates directories
                                    as needed, based on the smallest datetime variable that you use.
                                    For example, if the smallest variable is hours, then the
                                    directories are created for every hour of the day that receives
                                    output records.</dd>

                              <dd class="dd">When you use datetime variables in an expression, use all of
                                  the datetime variables between one of the year variables
                                  and the smallest variable that you want to use. For
                                  example, to create directories on a daily basis for a
                                  Hadoop FS destination, use a year variable, a month
                                  variable, and then a day variable. You might use one of
                                  the following datetime variable progressions:
                              </dd>

                              <dd class="dd">
                                    <pre class="pre codeblock">${YYYY()}-${MM()}-${DD()}
${YY()}_${MM()}_${DD()}</pre>

                              </dd>

                              <dd class="dd">For details about datetime variables, see <a class="xref" href="../Expression_Language/DateTimeVariables.html#concept_gh4_qd2_sv" title="The expression language provides datetime variables for use in expressions.">Datetime Variables</a>.</dd>

                        
                        
                              <dt class="dt dlterm">every() function</dt>

                              <dd class="dd">You can use the <samp class="ph codeph">every()</samp> function in a directory
                                    template to create directories at regular intervals based on
                                    minutes or seconds. The intervals should be a submultiple or
                                    integer factor of 60. For example, you can create directories
                                    every 15 minutes or 30 seconds. </dd>

                              <dd class="dd">Use the <samp class="ph codeph">every()</samp> function to replace the smallest
                                    datetime variable used in the template.</dd>

                              <dd class="dd">For example, the following directory template creates directories
                                    every 5
                                    minutes:<pre class="pre codeblock">/HDFS_output/${YYYY()}-${MM()}-${DD()}-${hh()}-${every(5,mm())}</pre>
</dd>

                              <dd class="dd">For details about the <samp class="ph codeph">every()</samp> function, see <a class="xref" href="../Expression_Language/Functions.html#concept_ddw_ld1_1s">Miscellaneous Functions</a>.</dd>

                        
                        
                              <dt class="dt dlterm">record:valueOrDefault function</dt>

                              <dd class="dd">You can use the following expression to use the value of a field
                                    and the specified default value if the field does not exist or
                                    if the field is null:
                                    <pre class="pre codeblock">${record:valueOrDefault("/&lt;field name&gt;", &lt;default value&gt;)}</pre>
</dd>

                              <dd class="dd">For example, the following directory template creates a directory
                                    based on the product field every day, and if the product field
                                    is empty or null, uses Misc in the directory path:
                                    <pre class="pre codeblock">/${record:valueOrDefault("/Product", "Misc")}/${YY()}-${MM()}-${DD()}</pre>
</dd>

                              <dd class="dd">This template might create the following
                                    paths:<pre class="pre codeblock">/Shirts/2015-07-31 
/Misc/2015-07-31</pre>
</dd>

                        
                  </dl>
</div>

 </div>

</div>
<div class="topic concept nested1" id="concept_gkz_smd_br">
 <h2 class="title topictitle2">Time Basis</h2>

 
 <div class="body conbody"><p class="shortdesc">When using directory templates, the time basis helps determine when directories are
  created. It also determines the directory Hadoop FS uses when writing a record, and whether a
  record is late.</p>

  <p class="p">When using the targetDirectory stage attribute
   to write records, the time basis determines only whether a record is late.</p>

  <p class="p">You can use the following times as the time basis: </p>

  <dl class="dl">
                  
                        <dt class="dt dlterm">Processing Time</dt>

                        <dd class="dd">When you use processing time as the time basis, the destination creates
                              directories based on the processing time and the directory template,
                              and writes records to the directories based on when they are
                              processed.</dd>

                        <dd class="dd">For example, say a directory template creates directories every minute
                              and the time basis is the time of processing. Then, directories are
                              created for every minute that the destination writes output records.
                              And the output records are written to the directory for that minute of
                              processing. </dd>

                        <dd class="dd">To use the processing time as the time basis, use the following
                              expression: <samp class="ph codeph">${time:now()}</samp>. This is the default time
                              basis. </dd>

                  
                  
                        <dt class="dt dlterm">Record Time</dt>

                        <dd class="dd">When you use the time associated with a record as the time basis, you
                              specify a Date field in the record. The destination creates
                              directories based on the datetimes associated with the records and
                              writes the records to the appropriate directories. </dd>

                        <dd class="dd">For example, say a directory template creates directories every hour and
                              the time basis is based on the record. Then, directories are created
                              for every hour associated with output records and the destination
                              writes the records to the related output directory. </dd>

                        <dd class="dd">To use a time associated with the record, use an expression that calls a
                              field and resolves to a datetime value, such as
                                    <samp class="ph codeph">${record:value("/Timestamp")}</samp>. </dd>

                  
            </dl>

 </div>

</div>
<div class="topic concept nested1" id="concept_xgm_g4d_br">
 <h2 class="title topictitle2">Late Records and Late Record Handling </h2>

 <div class="body conbody">
        <p class="p"></p>

        <p class="p">When you use a record time as the time basis, you can define
                  a time limit for records to be written to their associated output file. When the
                  destination creates a new output file in a new directory, the previous output file
                  is kept open for the specified late record time limit. When records that belong in
                  that file arrive within the time limit, the destination writes the records to the
                  open output file. When the late record time limit is reached, the output file is
                  closed and any record that arrives past this limit is considered late.</p>

        <div class="note tip"><span class="tiptitle">Tip:</span> The late records properties are not applicable if
                  you use processing time as the time basis. If you use processing time, set the
                  late record time limit to one second.</div>

     <p class="p">You can send late records to a late records file or to the
                  stage for error handling. When you send records to a late records file, you define
                  a late records directory template. </p>

        <div class="p">For example, you use a record time as the time basis,
                  configure a one hour late record time limit, configure late records to be sent to
                  the stage for error handling, and use the default directory template value:
                  <pre class="pre codeblock">/tmp/out/${YYYY()}-${MM()}-${DD()}-${hh()} </pre>
</div>

        <p class="p">The first records that arrive have a datetime between the
                  hours of 02:00 and 02:59, and so are written to an output file in the 02
                  directory. When records with a datetime between the hours of 03:00 and 03:59
                  arrive, the destination creates a new file in an 03 directory. The destination
                  keeps the file in the 02 directory open for another hour. </p>

        <p class="p">If a record with a datetime between the hours of 02:00 and
                  02:59 arrives before the hour time limit, the destination writes the record to the
                  open file in the 02 directory. After one hour, the destination closes the output
                  file in the 02 directory. Any records with a datetime between the hours of 02:00
                  and 02:59 that arrive after the one hour time limit are considered late. The late
                  records are sent to the stage for error handling.</p>

 </div>

</div>
<div class="topic concept nested1" id="concept_qjs_dw3_tv">
 <h2 class="title topictitle2">Timeout to Close Idle Files</h2>

 <div class="body conbody">
        <p class="p">You can configure the maximum time that an open output
            file can remain idle. After no records are written to an output file for the specified
            amount of time, the Hadoop FS destination closes the file. </p>

        <p class="p">You might want to configure an idle timeout when output
                  files remain open and idle for too long, thus delaying another system from
                  processing the files.</p>

        <div class="p">Output files might remain idle for too long for the following
                  reasons:            <ul class="ul">
                        <li class="li">You configured the maximum number of records to be written to output
                              files or the maximum size of output files, but records have stopped
                              arriving. An output file that has not reached the maximum number of
                              records or the maximum file size stays open until more records
                              arrive.</li>

                        <li class="li">You configured a date field in the record as the time basis and have
                              configured a late record time limit, but records arrive in
                              chronological order. When a new directory is created, the output file
                              in the previous directory remains open for the configured late record
                              time limit. However, no records are ever written to the open file in
                              the previous directory.<p class="p">For example, when a record with a datetime
                                    of 03:00 arrives, the destination creates a new file in a new 03
                                    directory. The previous file in the 02 directory is kept open
                                    for the late record time limit, which is an hour by default.
                                    However, when records arrive in chronological order, no records
                                    that belong in the 02 directory arrive after the 03 directory is
                                    created. </p>
</li>

            </ul>
</div>

        <p class="p">In either situation, configure an idle timeout so that other
                  systems can process the files sooner, instead of waiting for the configured
                  maximum records, maximum file size, or late records conditions to occur. </p>

    </div>

</div>
<div class="topic concept nested1" id="concept_lmn_gdc_1w">
    <h2 class="title topictitle2">Stage Attributes</h2>

    
    <div class="body conbody"><p class="shortdesc">A stage attribute is a record header attribute with information that a stage can use
        to process records. Hadoop FS can use stage attributes to determine the directory to write a
        record to, the Avro schema to use, and when to roll a file. </p>

        <p class="p">The Hive Metadata processor generates stage attributes that Hadoop FS can use as part of
            the Hive Drift Solution. Or, you can use an Expression Evaluator to add the
            targetDirectory stage attribute to record headers. </p>

        <p class="p">To pass a stage attribute to Hadoop FS, you configure the destination to use the
            attribute, and you ensure that the record headers include the stage attribute. </p>

        <div class="p">You can use the following stage attributes in Hadoop FS:<dl class="dl">
                
                    <dt class="dt dlterm">targetDirectory</dt>

                    <dd class="dd">The targetDirectory stage attribute defines the directory where the record
                        is written. If the directory does not exist, the destination creates the
                        directory. The targetDirectory stage attribute replaces the Directory
                        Template property in the destination.</dd>

                    <dd class="dd">When you use targetDirectory to provide the directory, the time basis
                        configured for the destination is used only for determining whether a record
                        is late. Time basis is not used to determine the output directories to
                        create or to write records to directories.</dd>

                    <dd class="dd">To use the targetDirectory attribute, on the <span class="keyword wintitle">Output</span>
                        tab, select <span class="ph uicontrol">Directory in Header</span>.</dd>

                
                
                    <dt class="dt dlterm">avroSchema</dt>

                    <dd class="dd">The avroSchema stage attribute defines the Avro schema for the record. When
                        you use this stage attribute, you cannot define an Avro schema to use in the
                        destination. </dd>

                    <dd class="dd">To use the avroSchema attribute, on the <span class="keyword wintitle">Avro</span> tab,
                        select <span class="ph uicontrol">Load Schema from Header</span>.</dd>

                
                
                    <dt class="dt dlterm">roll</dt>

                    <dd class="dd">The roll attribute, when present in the record header, triggers a roll of
                        the file. </dd>

                    <dd class="dd">You can define the name of the roll attribute. Use the default "roll"
                        attribute name when using the Hive Metadata processor to generate the stage
                        attribute.</dd>

                    <dd class="dd">To use a roll attribute, on the <span class="keyword wintitle">Output</span> tab, select
                            <span class="ph uicontrol">Use Roll Attribute</span> and define the name of the
                        attribute. </dd>

                
            </dl>
</div>

    </div>

    <div class="related-links"><div class="relinfo relconcepts"><strong>Related concepts</strong><br xmlns="http://www.w3.org/1999/xhtml" />
<div class="related_link"><a class="navheader_parent_path" href="../Hive_Metadata/HiveDrift-Overview.html#concept_phk_bdf_2w" title="Hive Drift Solution: Ingesting Drifting Data into Hive">Hive Drift Solution: Ingesting Drifting Data into Hive</a></div>
<div class="related_link"><a class="navheader_parent_path" href="../Processors/HiveMetadata.html#concept_rz5_nft_zv" title="The Hive Metadata processor works with the Hive Metastore destination and Hadoop FS destination as part of the Hive Drift Solution.">Hive Metadata</a></div>
</div>
</div>
</div>
<div class="topic concept nested1" id="concept_lww_3b3_kr">
 <h2 class="title topictitle2">Data Formats</h2>

 <div class="body conbody">
  <div class="p">Hadoop FS writes data to
      HDFS based on the data format that you select. You can use the following data formats: <dl class="dl">
        
                              <dt class="dt dlterm">Avro</dt>

                              <dd class="dd">The destination writes records based on the Avro schema in the
                                    record header or on the schema that you define in the stage. The
                                    schema definition is included in each file.</dd>

                              <dd class="dd">You can compress data with an Avro-supported compression codec.
                                    When using Avro compression, avoid using other compression
                                    available in the destination. </dd>

                        
        
                              <dt class="dt dlterm">Delimited</dt>

                              <dd class="dd">The destination writes records as delimited data. When you use
                                    this data format, the root field must be list or list-map.</dd>

                        
        
                              <dt class="dt dlterm">JSON</dt>

                              <dd class="dd">The destination writes records as JSON data. You can use one of
                                    the following formats:<ul class="ul" id="concept_lww_3b3_kr__d1848e900">
                                          <li class="li">Array - Each file includes a single array. In the
                                                array, each element is a JSON representation of each
                                                record.</li>

                                          <li class="li">Multiple objects - Each file includes multiple JSON
                                                objects. Each object is a JSON representation of a
                                                record. </li>

                                    </ul>
</dd>

                        
        
                        <dt class="dt dlterm">Protobuf</dt>

                        <dd class="dd">Writes a batch of messages in each file. </dd>

                        <dd class="dd">Uses the user-defined message type and the definition of the message
                              type in the descriptor file to generate the messages in the file. </dd>

                        <dd class="dd">For information about generating the descriptor file, see <a class="xref" href="../Pipeline_Design/Protobuf-Prerequisites.html" title="Perform the following prerequisites before reading or writing protobuf data.">Protobuf Data Format Prerequisites</a>.</dd>

                  
        
                              <dt class="dt dlterm">Text</dt>

                              <dd class="dd">The destination writes a single text field of a record. When you
                                    configure the stage, you select the field to use. When
                                    necessary, merge record data into the field earlier in the
                                    pipeline. </dd>

                        
      </dl>
</div>

 </div>

</div>
<div class="topic concept nested1" id="concept_xy5_4tm_vs">
 <h2 class="title topictitle2">Kerberos Authentication</h2>

 
 <div class="body conbody"><p class="shortdesc">You can use Kerberos authentication to connect to HDFS. When you use Kerberos
    authentication, the <span class="ph">Data
                  Collector</span> uses the
    Kerberos principal and keytab to connect to HDFS. </p>

  <p class="p">The Kerberos principal and keytab are defined
      in the <span class="ph">Data
                  Collector</span>
    configuration file, <samp class="ph codeph">$SDC_CONF/sdc.properties</samp>. To use Kerberos authentication, configure all Kerberos properties in the
        <span class="ph">Data
                  Collector</span>
      configuration file, and then enable Kerberos in the Hadoop FS destination.</p>

 </div>

 <div class="related-links"><div class="relinfo relconcepts"><strong>Related concepts</strong><br xmlns="http://www.w3.org/1999/xhtml" />
<div class="related_link"><a class="navheader_parent_path" href="../Install_Config/DCConfig.html#concept_hnm_n4l_xs" title="You can use Kerberos authentication to connect to external systems as well as YARN clusters.">Enabling Kerberos Authentication</a></div>
</div>
</div>
</div>
<div class="topic concept nested1" id="concept_u4h_lwt_ls">
 <h2 class="title topictitle2">Using an HDFS User</h2>

 
 <div class="body conbody"><p class="shortdesc">You can configure the Hadoop FS destination to use an HDFS user to write data to HDFS. </p>

  <p class="p">By default, the <span class="ph">Data
                  Collector</span> uses the
      user account who started it to connect to external systems. When using Kerberos, the <span class="ph">Data
                  Collector</span> uses the
      Kerberos principal. </p>

  <div class="p">To use an HDFS user to connect to HDFS, perform the following tasks:<ol class="ol" id="concept_u4h_lwt_ls__ul_mb1_xpt_ls">
        <li class="li">On HDFS, configure the <span class="ph">Data
                  Collector</span> user
          as a proxy user and authorize the <span class="ph">Data
                  Collector</span> user
          to impersonate the HDFS user. <p class="p">For more information, see the HDFS documentation.
          </p>
</li>

        <li class="li">In the Hadoop FS destination, enter the HDFS user name.</li>

      </ol>
</div>

 </div>

</div>
<div class="topic concept nested1" id="concept_xh5_y4d_br">
 <h2 class="title topictitle2">HDFS Properties and Configuration Files</h2>

    <div class="body conbody">
        <div class="p">You can configure the Hadoop FS destination
            to use individual HDFS properties or HDFS configuration files:<dl class="dl">
                
                    <dt class="dt dlterm">HDFS configuration files</dt>

                    <dd class="dd">You can use the following HDFS configuration files with the Hadoop FS
                            destination:<ul class="ul" id="concept_xh5_y4d_br__ul_qhn_ytr_bt">
                        <li class="li">core-site.xml</li>

                        <li class="li">hdfs-site.xml </li>

                  </ul>
</dd>

                    <dd class="dd">To use HDFS configuration files: <ol class="ol" id="concept_xh5_y4d_br__ol_rb2_2nr_bt">
                            <li class="li">Store the files or a symlink to the files in the <span class="ph">Data
                  Collector</span> resources directory. </li>

                            <li class="li">In the Hadoop FS destination, specify the location of the files.
                            </li>

                        </ol>
<div class="note note"><span class="notetitle">Note:</span>  For a Cloudera Manager installation, Data Collector
                            automatically creates a symlink to the files named
                                <samp class="ph codeph">hadoop-conf</samp>. Enter <samp class="ph codeph">hadoop-conf</samp> for
                            the location of the files in the Hadoop FS destination.</div>
</dd>

                
                
                    <dt class="dt dlterm">Individual properties</dt>

                    <dd class="dd">You can configure individual HDFS properties in the destination. To add an
                        HDFS property, you specify the exact property name and the value. The Hadoop
                        FS destination does not validate the property names or
                            values.<div class="note note"><span class="notetitle">Note:</span> Individual properties override properties defined in the
                            HDFS configuration file. </div>
</dd>

                
            </dl>
</div>

    </div>

</div>
<div class="topic task nested1" id="task_m2m_skm_zq">
    <h2 class="title topictitle2">Configuring a Hadoop FS Destination</h2>

    <div class="body taskbody">
        <div class="section context">
            <p class="p">Configure a Hadoop FS
                destination to write data to HDFS.</p>

        </div>

        <ol class="ol steps" id="task_m2m_skm_zq__steps_ljw_44d_br"><li class="li step stepexpand">
                <span class="ph cmd">In the Properties panel, on the <span class="keyword wintitle">General</span> tab, configure the
                    following properties:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_m2m_skm_zq__d1735e2834" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr class="row">
                                    <th class="entry" valign="top" width="30%" id="d32258e763">General Property</th>

                                    <th class="entry" valign="top" width="70%" id="d32258e766">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e763 ">Name</td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e766 ">Stage name.</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e763 ">Description</td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e766 ">Optional description.</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e763 ">Stage Library</td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e766 ">Library version that you want to use. </td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e763 ">Required Fields <a class="xref" href="../Pipeline_Design/DroppingUnwantedRecords.html#concept_dnj_bkm_vq" title="A required field is a field that must exist in a record to allow it into the stage for processing. When a record does not include a required field, the record is diverted to the pipeline for error handling. You can define required fields for any processor and most destination stages.">
                                            <img class="image" id="task_m2m_skm_zq__d1735e2889" src="../Reusable_Content/../Graphics/icon_moreInfo.png" height="12" width="12"></img></a></td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e766 ">Fields that must include data to be passed into the
                                        stage. <div class="note tip"><span class="tiptitle">Tip:</span> You might include
                                            fields that the stage uses.</div>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e763 ">Preconditions <a class="xref" href="../Pipeline_Design/DroppingUnwantedRecords.html#concept_msl_yd4_fs" title="Preconditions are conditions that a record must satisfy to enter the stage for processing. Like required fields, if a record does not meet a precondition, it is diverted to the pipeline for error handling. You can define preconditions for any processor and most destination stages.">
                                            <img class="image" id="task_m2m_skm_zq__d1735e2903" src="../Reusable_Content/../Graphics/icon_moreInfo.png" height="12" width="12"></img></a></td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e766 ">Conditions that must evaluate to TRUE to allow a record
                                        to enter the stage for processing. Click
                                            <span class="ph uicontrol">Add</span> to create additional
                                        preconditions. </td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e763 ">On Record Error</td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e766 ">Error record handling for the stage: <ul class="ul" id="task_m2m_skm_zq__d1735e2920">
                                            <li class="li">Discard - Discards the record.</li>

                                            <li class="li">Send to Error - Sends the record to the pipeline for
                                                error handling.</li>

                                            <li class="li">Stop Pipeline - Stops the pipeline.</li>

                                        </ul>
</td>

                                </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">On the <span class="keyword wintitle">Hadoop FS</span> tab, configure the following
                    properties:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_m2m_skm_zq__table_rst_t4d_br" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr class="row">
                                    <th class="entry" valign="top" width="33.33333333333333%" id="d32258e885">Hadoop FS Property</th>

                                    <th class="entry" valign="top" width="66.66666666666666%" id="d32258e888">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr class="row">
                                    <td class="entry" valign="top" width="33.33333333333333%" headers="d32258e885 ">Hadoop FS URI</td>

                                    <td class="entry" valign="top" width="66.66666666666666%" headers="d32258e888 ">HDFS URI.</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="33.33333333333333%" headers="d32258e885 ">HDFS User <a class="xref" href="HadoopFS-destination.html#concept_u4h_lwt_ls" title="You can configure the Hadoop FS destination to use an HDFS user to write data to HDFS.">
                                            <img class="image" id="task_m2m_skm_zq__image_byg_yqg_xs" src="../Graphics/icon_moreInfo.png" height="12" width="12"></img></a></td>

                                    <td class="entry" valign="top" width="66.66666666666666%" headers="d32258e888 ">The HDFS user to use to connect to HDFS. When using this
                                        property, make sure HDFS is configured appropriately.<p class="p">By
                                            default, the pipeline uses the <span class="ph">Data
                  Collector</span> user to connect to HDFS.</p>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="33.33333333333333%" headers="d32258e885 ">Kerberos Authentication<a class="xref" href="HadoopFS-destination.html#concept_xy5_4tm_vs" title="You can use Kerberos authentication to connect to HDFS. When you use Kerberos authentication, the Data Collector uses the Kerberos principal and keytab to connect to HDFS.">
                                            <img class="image" id="task_m2m_skm_zq__image_a5x_jzn_vs" src="../Graphics/icon_moreInfo.png" height="12" width="12"></img></a></td>

                                    <td class="entry" valign="top" width="66.66666666666666%" headers="d32258e888 ">Uses Kerberos credentials to connect to HDFS. <p class="p">When
                                            selected, uses the Kerberos principal and keytab defined
                                            in the <span class="ph">Data
                  Collector</span> configuration file,
                                                <samp class="ph codeph">$SDC_CONF/sdc.properties</samp>.
                                        </p>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="33.33333333333333%" headers="d32258e885 ">Hadoop FS Configuration Directory <a class="xref" href="HadoopFS-destination.html#concept_xh5_y4d_br">
                                            <img class="image" id="task_m2m_skm_zq__image_br4_fgs_5r" src="../Graphics/icon_moreInfo.png" height="12" width="12"></img></a></td>

                                    <td class="entry" valign="top" width="66.66666666666666%" headers="d32258e888 ">Location of the HDFS configuration files.<p class="p">For a
                                            Cloudera Manager installation, enter
                                                <samp class="ph codeph">hadoop-conf</samp>. For all other
                                            installations, use a directory or symlink within the <span class="ph">Data
                  Collector</span> resources directory.</p>
<div class="p">You can use the following
                                            files with the Hadoop FS destination:<ul class="ul" id="task_m2m_skm_zq__ul_qnc_jtt_bt">
                        <li class="li">core-site.xml</li>

                        <li class="li">hdfs-site.xml </li>

                  </ul>
</div>
<div class="note note"><span class="notetitle">Note:</span> Properties in the configuration files are
                                            overridden by individual properties defined in the
                                            stage.</div>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="33.33333333333333%" headers="d32258e885 ">Hadoop FS Configuration</td>

                                    <td class="entry" valign="top" width="66.66666666666666%" headers="d32258e888 ">Additional HDFS properties to use. <p class="p">To add properties,
                                            click <span class="ph uicontrol">Add</span> and define the property
                                            name and value. Use the property names and values as
                                            expected by HDFS.</p>
</td>

                                </tr>

                                <tr class="row">
       <td class="entry" valign="top" width="33.33333333333333%" headers="d32258e885 ">Charset</td>

       <td class="entry" valign="top" width="66.66666666666666%" headers="d32258e888 ">Character set to use when writing files. <p class="p">Not used with all data
        formats.</p>
</td>

      </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">On the <span class="keyword wintitle">Output Files</span> tab, configure the following
                    options:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_m2m_skm_zq__table_i4x_jh5_sv" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr class="row">
                                    <th class="entry" valign="top" width="30%" id="d32258e1038">Output Files Property</th>

                                    <th class="entry" valign="top" width="70%" id="d32258e1041">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1038 ">File Type</td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e1041 ">Output file type:<ul class="ul" id="task_m2m_skm_zq__d1735e3742">
                                            <li class="li">Text files</li>

                                            <li class="li">Sequence files</li>

                                        </ul>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1038 ">Data Format</td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e1041 ">Format of data to be written. Use one of the following
                                            options:<ul class="ul" id="task_m2m_skm_zq__d1735e3759">
                                            <li class="li">Avro</li>

                                            <li class="li">Delimited</li>

                                            <li class="li">JSON</li>

                                            <li class="li">Protobuf</li>

                                            <li class="li">Text</li>

                                        </ul>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1038 ">File Prefix</td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e1041 ">Prefix to use for output files. Use when writing to a
                                        directory that receives files from other sources.<p class="p">Uses the
                                            prefix sdc-${sdc:id()} by default. The prefix evaluates
                                            to sdc-&lt;Data Collector ID&gt;. </p>
<p class="p">The Data Collector
                                            ID is stored in the following file:
                                                <span class="ph filepath">$SDC_DATA/sdc.id</span>. For more information about environment variables, see <a class="xref" href="../Install_Config/DCEnvironmentConfig.html#concept_rng_qym_qr" title="You can edit the Data Collector environment configuration file to modify the directories used to store configuration, data, log, and resource files.When you run Data Collector as a service, you must create a system user and group named sdc, or you must edit the values of the SDC_USER and SDC_GROUP environment variables to point to an existing system user or group.You can define the Data Collector Java heap size. By default, the Java heap size is 1024 MB. When you use Data Collector with Java 7, you can define the Java Permanent Generation size, also known as the PermGen size.Data Collector includes a Java Security Manager that is enabled by default. You can edit the Data Collector environment configuration file to configure the path to jar files to be added to the Data Collector root classloader.">Data Collector Environment Configuration</a>.</p>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1038 ">Directory in Header <a class="xref" href="HadoopFS-destination.html#concept_lmn_gdc_1w" title="A stage attribute is a record header attribute with information that a stage can use to process records. Hadoop FS can use stage attributes to determine the directory to write a record to, the Avro schema to use, and when to roll a file.">
                                            <img class="image" id="task_m2m_skm_zq__image_ydt_vvg_1w" src="../Graphics/icon_moreInfo.png" height="12" width="12"></img></a>
                                    </td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e1041 ">Indicates that the target directory is defined in record
                                        headers. Use only when the targetDirectory stage attribute
                                        is defined for all records. </td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1038 ">Directory Template <a class="xref" href="HadoopFS-destination.html#concept_cvc_skd_br">
                                            <img class="image" id="task_m2m_skm_zq__d1735e3802" src="../Reusable_Content/../Graphics/icon_moreInfo.png" height="12" width="12"></img></a></td>

                                    <td class="entry" id="task_m2m_skm_zq__d1735e3804" valign="top" width="70%" headers="d32258e1041 ">Template for creating output directories. You can use
                                        constants, field values, and datetime variables. <p class="p">Output
                                            directories are created based on the smallest datetime
                                            variable in the template.</p>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1038 ">Data Time Zone</td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e1041 ">Time zone for the destination system. Used to resolve
                                        datetimes in the directory template and evaluate where
                                        records are written.</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1038 ">Time Basis <a class="xref" href="HadoopFS-destination.html#concept_gkz_smd_br" title="When using directory templates, the time basis helps determine when directories are created. It also determines the directory Hadoop FS uses when writing a record, and whether a record is late.">
                                            <img class="image" id="task_m2m_skm_zq__d1735e3826" src="../Reusable_Content/../Graphics/icon_moreInfo.png" height="12" width="12"></img></a></td>

                                    <td class="entry" id="task_m2m_skm_zq__d1735e3828" valign="top" width="70%" headers="d32258e1041 ">Time basis to use for creating output directories and
                                        writing records to the directories. Use one of the following
                                            expressions:<ul class="ul" id="task_m2m_skm_zq__d1735e3830">
                                            <li class="li">${time:now()} - Uses the processing time as the time
                                                basis. </li>

                                            <li class="li">${record:value("/&lt;date field&gt;")} - Uses the time
                                                associated with the record as the time basis.</li>

                                        </ul>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1038 ">Max Records in a File</td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e1041 ">Maximum number of records to be written to an output
                                        file. Additional records are written to a new file. <p class="p">Use 0
                                            to opt out of this property.</p>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1038 ">Max File Size (MB)</td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e1041 ">Maximum size of an output file. Additional records are
                                        written to a new file. <p class="p">Use 0 to opt out of this
                                            property.</p>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1038 ">Idle Timeout (secs) <a class="xref" href="HadoopFS-destination.html#concept_qjs_dw3_tv">
                                            <img class="image" id="task_m2m_skm_zq__image_br5_fgs_7r" src="../Graphics/icon_moreInfo.png" height="12" width="12"></img></a>
                                    </td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e1041 ">Maximum time that an output file
                                        can remain idle. After no records are written to a file for
                                        this amount of time, the destination closes the file. Enter
                                        a time in seconds or use the <samp class="ph codeph">MINUTES</samp> or
                                            <samp class="ph codeph">HOURS</samp> constant in an expression to
                                        define the time increment.<p class="p">Use -1 to set no limit. Default
                                            is 1 hour, defined as follows: <samp class="ph codeph">${1 *
                                                HOURS}</samp>.</p>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1038 ">Compression Codec</td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e1041 ">Compression type for output files:<ul class="ul" id="task_m2m_skm_zq__d1735e3869">
                                            <li class="li">None </li>

                                            <li class="li">gzip</li>

                                            <li class="li">bzip2</li>

                                            <li class="li">Snappy</li>

                                            <li class="li">LZ4</li>

                                            <li class="li">Other</li>

                                        </ul>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1038 ">Compression Codec Class</td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e1041 ">Full class name of the other compression codec that you
                                        want to use. </td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1038 ">Sequence File Key</td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e1041 ">Record key for creating sequence files. Use one of the
                                        following options:<ul class="ul" id="task_m2m_skm_zq__d1735e3909">
                                            <li class="li">${record:value("/&lt;field name&gt;")}</li>

                                            <li class="li">${uuid()}</li>

                                        </ul>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1038 ">Compression Type</td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e1041 ">Compression type for sequence files when using a
                                        compression codec:<ul class="ul" id="task_m2m_skm_zq__d1735e3926">
                                            <li class="li">Block Compression</li>

                                            <li class="li">Record Compression</li>

                                        </ul>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1038 ">Use Roll Attribute  <a class="xref" href="HadoopFS-destination.html#concept_lmn_gdc_1w" title="A stage attribute is a record header attribute with information that a stage can use to process records. Hadoop FS can use stage attributes to determine the directory to write a record to, the Avro schema to use, and when to roll a file.">
                                            <img class="image" id="task_m2m_skm_zq__image_tfb_1ql_gw" src="../Graphics/icon_moreInfo.png" height="12" width="12"></img></a>
                                    </td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e1041 ">Checks the record header for the roll attribute and
                                        closes the current file when the attribute exists. </td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1038 ">Roll Attribute Name</td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e1041 ">Name of the roll attribute.<p class="p">Default is roll, the name
                                            used by the Hive Metadata processor.</p>
</td>

                                </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">On the <span class="keyword wintitle">Late Records</span> tab, configure the following
                    properties:</span>
                <div class="itemgroup info">
                    <div class="note tip"><span class="tiptitle">Tip:</span> These properties are relevant for a time basis based on the
                        time of a record. If you use processing time as the time basis, set the late
                        record time limit to one second.</div>

                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_m2m_skm_zq__d1735e4316" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr class="row">
                                    <th class="entry" valign="top" width="30%" id="d32258e1381">Late Records Property <a class="xref" href="HadoopFS-destination.html#concept_xgm_g4d_br">
                                            <img class="image" id="task_m2m_skm_zq__d1735e4332" src="../Reusable_Content/../Graphics/icon_moreInfo.png" height="12" width="12"></img>
                                        </a>
                                    </th>

                                    <th class="entry" valign="top" width="70%" id="d32258e1390">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1381 ">Late Record Time Limit (secs)</td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e1390 ">Time limit for output directories to accept data. <p class="p">You
                                            can enter a time in seconds, or use the expression to
                                            enter a time in hours. You can also use MINUTES in the
                                            default expression to define the time in minutes.
                                        </p>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1381 ">Late Record Handling</td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e1390 ">Determines how to handle late records:<ul class="ul" id="task_m2m_skm_zq__d1735e4361">
                                            <li class="li">Send to error - Sends the record to the stage for
                                                error handling. </li>

                                            <li class="li">Send to late records file - Sends the record to a
                                                late records file.</li>

                                        </ul>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1381 ">Late Record Directory Template <a class="xref" href="HadoopFS-destination.html#concept_cvc_skd_br">
                                            <img class="image" id="task_m2m_skm_zq__d1735e4377" src="../Reusable_Content/../Graphics/icon_moreInfo.png" height="12" width="12"></img></a></td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e1390 ">Template for creating late record directories. You can
                                        use constants, field values, and datetime variables.
                                            <p class="p">Output directories are created based on the smallest
                                            datetime variable in the template.</p>
</td>

                                </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">For Avro data, on the <span class="keyword wintitle">Avro</span> tab, configure the following
                    properties:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_m2m_skm_zq__d1735e3099" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr class="row">
                                    <th class="entry" valign="top" width="30%" id="d32258e1467">Avro Property</th>

                                    <th class="entry" valign="top" width="70%" id="d32258e1470">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1467 ">Avro Schema in Header <a class="xref" href="HadoopFS-destination.html#concept_lmn_gdc_1w" title="A stage attribute is a record header attribute with information that a stage can use to process records. Hadoop FS can use stage attributes to determine the directory to write a record to, the Avro schema to use, and when to roll a file.">
                                            <img class="image" id="task_m2m_skm_zq__d1735e3127" src="../Reusable_Content/../Graphics/icon_moreInfo.png" height="12" width="12"></img></a></td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e1470 ">Indicates that the Avro schema is embedded in the
                                        avroSchema header attribute. Use only when the avroSchema
                                        attribute is defined for all records.</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1467 ">Avro Schema</td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e1470 ">Schema definition to use when writing data. The
                                        destination includes the schema definition in each generated
                                        file. </td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1467 ">Avro Compression Codec</td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e1470 ">The Avro-supported compression type to use. <p class="p">When you
                                            enable Avro compression, do not enable other compression
                                            available in the destination. </p>
</td>

                                </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">For delimited data, on the <span class="keyword wintitle">Delimited</span> tab, configure the
                    following properties:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_m2m_skm_zq__d1735e3330" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr class="row">
                                    <th class="entry" valign="top" width="30%" id="d32258e1541">Delimited Property</th>

                                    <th class="entry" valign="top" width="70%" id="d32258e1544">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1541 ">Delimiter Format</td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e1544 ">Format for delimited data:<ul class="ul" id="task_m2m_skm_zq__ul_k3j_vvf_jr">
                        <li class="li"><span class="ph uicontrol">Default CSV</span> - File that includes comma-separated
                              values. Ignores empty lines in the file.</li>

                        <li class="li"><span class="ph uicontrol">RFC4180 CSV</span> - Comma-separated file that strictly
                              follows RFC4180 guidelines.</li>

                        <li class="li"><span class="ph uicontrol">MS Excel CSV</span> - Microsoft Excel comma-separated
                              file.</li>

                        <li class="li"><span class="ph uicontrol">MySQL CSV</span> - MySQL comma separated file.</li>

                        <li class="li"><span class="ph uicontrol">Tab-Separated Values</span> - File that includes
                              tab-separated values.</li>

                        <li class="li"><span class="ph uicontrol">Custom</span> - File that uses user-defined delimiter,
                              escape, and quote characters.</li>

                  </ul>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1541 ">Header Line</td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e1544 ">Indicates whether to create a header line.</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1541 ">Replace New Line Characters</td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e1544 ">Replaces new line characters with the configured
                                            string.<p class="p">Recommended when writing data as a single line
                                            of text.</p>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1541 ">New Line Character Replacement</td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e1544 ">String to replace each new line character. For example,
                                        enter a space to replace each new line character with a
                                        space. <p class="p">Leave empty to remove the new line
                                        characters.</p>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1541 ">Delimiter Character</td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e1544 ">Delimiter character for a custom delimiter format. Select
                                        one of the available options or use Other to enter a custom
                                        character. <p class="p">You can enter a Unicode control character
                                            using the format \u<em class="ph i">NNNN</em>, where ​<em class="ph i">N</em> is a
                                            hexadecimal digit from the numbers 0-9 or the letters
                                            A-F. For example, enter \u0000 to use the null character
                                            as the delimiter or \u2028 to use a line separator as
                                            the delimiter.</p>
<p class="p">Default is the pipe character ( |
                                            ).</p>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1541 ">Escape Character </td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e1544 ">Escape character for a custom delimiter format. Select
                                        one of the available options or use Other to enter a custom
                                        character. <p class="p">Default is the backslash character ( \
                                        ).</p>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1541 ">Quote Character</td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e1544 ">Quote character for a custom delimiter format. Select one
                                        of the available options or use Other to enter a custom
                                        character. <p class="p">Default is the quotation mark character ( "
                                            ).</p>
</td>

                                </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">For JSON data, on the <span class="ph uicontrol">JSON</span> tab, configure the following
                    property:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_m2m_skm_zq__d1735e3464" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr class="row">
                                    <th class="entry" valign="top" width="30%" id="d32258e1693">JSON Property</th>

                                    <th class="entry" valign="top" width="70%" id="d32258e1696">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1693 ">JSON Content</td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e1696 ">Determines how JSON data is written:<ul class="ul" id="task_m2m_skm_zq__d1735e3493">
                                            <li class="li">JSON Array of Objects - Each file includes a single
                                                array. In the array, each element is a JSON
                                                representation of each record.</li>

                                            <li class="li">Multiple JSON Objects - Each file includes multiple
                                                JSON objects. Each object is a JSON representation
                                                of a record.</li>

                                        </ul>
</td>

                                </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">For protobuf data, on the <span class="keyword wintitle">Protobuf</span> tab, configure the
                    following properties:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_m2m_skm_zq__d1735e3600" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr class="row">
                                    <th class="entry" valign="top" width="30%" id="d32258e1747">Protobuf Property</th>

                                    <th class="entry" valign="top" width="70%" id="d32258e1750">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1747 ">Protobuf Descriptor File </td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e1750 ">Descriptor file (.desc) to use. The descriptor file must
                                        be in the <span class="ph">Data
                  Collector</span> resources directory, <samp class="ph codeph">$SDC_RESOURCES</samp>. <p class="p">For more information about environment variables, see <a class="xref" href="../Install_Config/DCEnvironmentConfig.html#concept_rng_qym_qr" title="You can edit the Data Collector environment configuration file to modify the directories used to store configuration, data, log, and resource files.When you run Data Collector as a service, you must create a system user and group named sdc, or you must edit the values of the SDC_USER and SDC_GROUP environment variables to point to an existing system user or group.You can define the Data Collector Java heap size. By default, the Java heap size is 1024 MB. When you use Data Collector with Java 7, you can define the Java Permanent Generation size, also known as the PermGen size.Data Collector includes a Java Security Manager that is enabled by default. You can edit the Data Collector environment configuration file to configure the path to jar files to be added to the Data Collector root classloader.">Data Collector Environment Configuration</a>. For information
                                            about generating the descriptor file, see <a class="xref" href="../Pipeline_Design/Protobuf-Prerequisites.html" title="Perform the following prerequisites before reading or writing protobuf data.">Protobuf Data Format Prerequisites</a>.</p>
</td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1747 ">Message Type</td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e1750 ">The fully-qualified name for the message type to use when
                                        reading data.<p class="p">Use the following format:
                                                <samp class="ph codeph">&lt;package name&gt;.&lt;message
                                            type&gt;</samp>. </p>
Use a message type defined in the
                                        descriptor file.</td>

                                </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">For text data, on the <span class="ph uicontrol">Text</span> tab, configure the following
                    properties:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_m2m_skm_zq__d1735e3529" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr class="row">
                                    <th class="entry" valign="top" width="30%" id="d32258e1852">Text Property</th>

                                    <th class="entry" valign="top" width="70%" id="d32258e1855">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1852 ">Text Field Path</td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e1855 ">Field that contains the text data to be written. All data
                                        must be incorporated into the specified field. </td>

                                </tr>

                                <tr class="row">
                                    <td class="entry" valign="top" width="30%" headers="d32258e1852 ">Empty Line If No Text</td>

                                    <td class="entry" valign="top" width="70%" headers="d32258e1855 ">Creates an empty line when a record does not include the
                                        text field specified above. <p class="p">When not selected, records
                                            without the specified text field are
                                        discarded.</p>
</td>

                                </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
</ol>

    </div>

</div>
</div>
<div class="navfooter"><!---->
<span class="navparent"><a class="link" href="../Destinations/Destinations-title.html" title="Destinations"><span class="navheader_label">Parent topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Destinations</span></a></span>  </div>
</body>
</html>