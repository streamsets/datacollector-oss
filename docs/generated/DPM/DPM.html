
<!DOCTYPE html
  SYSTEM "about:legacy-compat">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />        
      <meta name="copyright" content="(C) Copyright 2005" /><meta name="DC.rights.owner" content="(C) Copyright 2005" /><meta name="DC.Type" content="concept" /><meta name="DC.Title" content="Meet StreamSets Control Hub" /><meta name="abstract" content="StreamSets Control HubTM (SCH) is a central point of control for all of your dataflow pipelines. Control Hub allows teams to build and execute large numbers of complex dataflows at scale." /><meta name="description" content="StreamSets Control HubTM (SCH) is a central point of control for all of your dataflow pipelines. Control Hub allows teams to build and execute large numbers of complex dataflows at scale." /><meta name="DC.Relation" scheme="URI" content="../DPM/DPM_title.html" /><meta name="DC.Format" content="XHTML" /><meta name="DC.Identifier" content="concept_l45_qwf_xw" /><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/commonltr.css"><!----></link><title>Meet StreamSets Control Hub</title><!--  Generated with Oxygen version 18.1, build number 2016112217.  --><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/webhelp_topic.css"><!----></link><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/skins/skin.css" /><link rel="stylesheet" type="text/css" href="../skin.css" /><script type="text/javascript"><!--
            
            var prefix = "../index.html";
            
            --></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-1.11.3.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.cookie.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-ui.custom.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.highlight-3.js"><!----></script><script type="text/javascript" charset="utf-8" src="../oxygen-webhelp/resources/js/webhelp_topic.js"><!----></script>
<!--
    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
-->
</head>
<body onload="highlightSearchTerm()" class="frmBody">
<table class="nav"><tbody><tr><td colspan="2"><div id="printlink"><a href="javascript:window.print();" title="Print this page"></a></div><div id="permalink"><a href="#" title="Link to this page"></a></div></td></tr><tr><td style="width:75%;"><span class="topic_breadcrumb_links"><span class="topic_breadcrumb_link"><a class="navheader_parent_path" href="../DPM/DPM_title.html" title="StreamSets Control Hub">StreamSets Control Hub</a></span></span></td><td><span id="topic_navigation_links" class="navheader">
<span class="navparent"><a class="link" href="../DPM/DPM_title.html" title="StreamSets Control Hub"><span class="navheader_label">Parent topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">StreamSets Control Hub</span></a></span>  </span></td></tr></tbody></table>
<div class="nested0" id="concept_l45_qwf_xw">
    <h1 class="title topictitle1">Meet <span class="ph">StreamSets Control Hub</span></h1>

    
    <div class="body conbody"><p class="shortdesc"><span class="ph">StreamSets Control Hub</span><sup class="ph sup">TM</sup> (SCH) is a central point of control for all of your dataflow pipelines.
            <span class="ph">Control Hub</span>
        allows teams to build and execute large numbers of complex dataflows at scale. </p>

        <p class="p">Teams of data
            engineers use the shared repository provided with <span class="ph">Control Hub</span> to
            collaboratively build pipelines. <span class="ph">Control Hub</span>
            provides full life cycle management of the pipelines, allowing you to track the version
            history and giving you full control of the evolving development process. <span class="ph">Control Hub</span>
            lets you deploy and execute dataflows at scale on manually administered or automatically
            provisioned <span class="ph">Data
                  Collector</span>s
            or on edge devices using <span class="ph">Data Collector Edge</span>.
            You can map multiple dataflows in a single visual topology and can view real-time
            statistics to measure dataflow performance across each topology, from end-to-end or
            point-to-point. You can also monitor alerts to ensure that incoming data meets business
            requirements for availability and accuracy. </p>

        <p class="p">Multiple types of users within your organization can perform different roles in <span class="ph">Control Hub</span>.
            For example, a data architect typically creates a high-level design of how data needs to
            flow through multiple systems. Teams of data engineers use this high-level design to
            build individual pipelines in <span class="ph">Control Hub</span>
            <span class="ph">Pipeline Designer</span> or
            a development <span class="ph">Data
                  Collector</span>.
            The data engineers then publish the finished pipelines to <span class="ph">Control Hub</span>. </p>

        <p class="p">A DevOps or site reliability engineer adds published pipelines to jobs and then starts
            the jobs across multiple execution <span class="ph">Data
                  Collector</span>s
            or <span class="ph">Edge
                        Data Collectors</span>, running a remote pipeline instance on each execution component. Data architects map
            the related jobs into a single visual topology, and then use the topology to monitor and
            measure the complete dataflow. DevOps engineers create data SLAs (service level
            agreements) for topologies to define thresholds that the dataflows cannot exceed,
            ensuring that data is delivered in a timely manner.</p>

        <p class="p">Let’s take a closer look at what data architects, data engineers, and DevOps engineers
            can accomplish with <span class="ph">Control Hub</span>.</p>

    </div>

<div class="related-links"></div>
<div class="topic concept nested1" id="concept_knb_cjq_dx">
 <h2 class="title topictitle2">Design the Complete Data Architecture</h2>

 <div class="body conbody">
        <p class="p">As a data architect - the person responsible for defining how data is stored, consumed,
            and managed by different systems - you design the complete flow of data through multiple
            systems. You might architect the high-level design in a design document or diagram.
            Then, you work with your team to develop <span class="ph">Data
                  Collector</span>
            pipelines that meet those dataflow needs.</p>

        <p class="p">For example, you need to create a 360-degree view of your customers by collecting all
            customer data captured in your organization’s social feeds, enterprise data warehouse,
            and website logs. You need to send all of the data to the Hadoop Distributed File System
            (HDFS) for further analysis using tools such as Hive and Impala. You determine that the
            website logs must be written to Kafka as an intermediary system before being streamed to
            HDFS. </p>

        <p class="p">To address this need, you create the following high-level design of the complete data
            flow:</p>

        <p class="p"><img class="image" id="concept_knb_cjq_dx__image_w1d_k5r_dx" src="../Graphics/DPM_DesignArchitecture.png" height="266" width="644" /></p>

        <p class="p">Then the rest of your team uses this high-level design to develop the necessary
            pipelines, jobs, and topologies within <span class="ph">Data
                  Collector</span>
            and <span class="ph">Control Hub</span>.</p>

 </div>

</div>
<div class="topic concept nested1" id="concept_ung_wbs_bx">
 <h2 class="title topictitle2">Collaboratively Build Pipelines</h2>

 
 <div class="body conbody"><p class="shortdesc">As a data engineer - the person responsible for making sure that data flows smoothly
        between systems - you build the pipelines needed to implement the designed data
        architecture. You use <span class="ph">Control Hub</span>
        <span class="ph">Pipeline Designer</span> or a
        development <span class="ph">Data
                  Collector</span> to
        build pipelines. </p>

        <p class="p">
            
        </p>

        <p class="p">During the development process, you share your pipelines with other data engineers so
            that you can collaboratively build pipelines as a team using your organization's best
            pratices. When the pipelines are complete, you publish the pipelines to the pipeline
            repository in <span class="ph">Control Hub</span>. </p>

        <p class="p"><span class="ph">Control Hub</span>
            provides release management of pipelines. A typical pipeline development cycle involves
            iterative changes to the pipeline. <span class="ph">Control Hub</span>
            maintains the version history of each published pipeline. For example, as you design the
            Social Feeds Dataflows pipeline, you test the pipeline and then make changes to it. As a
            result, you will likely publish the pipeline to the <span class="ph">Control Hub</span>
            pipeline repository multiple times, as displayed in the following image of the pipeline
            history: </p>

        <p class="p"><img class="image" id="concept_ung_wbs_bx__image_llt_s4t_bx" src="../Graphics/DPM_ManagePipelineRepository.png" height="209" width="505" /></p>

        <p class="p">When viewing the pipeline history in <span class="ph">Control Hub</span>,
            you can view the configuration details for any pipeline version and can compare pipeline
            versions side-by-side. For example, if you click the <span class="ph uicontrol">Compare with Previous
                Version</span> icon <img class="image" id="concept_ung_wbs_bx__image_zqv_3st_bx" src="../Graphics/icon_DPM_ComparePreviousVersion.png" height="18" width="18" /> for version 3 in the image above, <span class="ph">Control Hub</span>
            displays version 2 and version 3 in the comparison window as follows: </p>

        <p class="p"><img class="image" id="concept_ung_wbs_bx__image_bzq_3d5_bx" src="../Graphics/DPM_ComparePipelineVersions.png" height="253" width="613" /></p>

        <p class="p">You can see that version 3 of the pipeline added an Expression Evaluator processor. You
            can drill into the details of each pipeline stage, and compare the configuration of each
            stage between the versions.</p>

        <p class="p">You can add tags to pipeline versions to mark release points or to separate development
            and production environments. For example, when you finish developing the Social Feeds
            Dataflows pipeline, you add a "Ready to Deploy" tag to the latest version. That tag
            informs your DevOps engineer which pipeline version is ready to be added to a job and
            run.</p>

    </div>

</div>
<div class="topic concept nested1" id="concept_inh_ccs_bx">
 <h2 class="title topictitle2">Execute Jobs at Scale</h2>

 
 <div class="body conbody"><p class="shortdesc">Pipelines are the design of the dataflow. Jobs are the execution of the dataflow. Data
        engineers build pipelines with the <span class="ph">Control Hub</span>
        <span class="ph">Pipeline Designer</span> or a
        development <span class="ph">Data
                  Collector</span>.
        DevOps or site reliability engineers run jobs on groups of execution <span class="ph">Data
                  Collector</span>s or
            <span class="ph">Edge
                        Data Collectors</span>.</p>

        <p class="p">
            
        </p>

        <p class="p">As a DevOps or site reliability engineer - the person responsible for ensuring that all
            services and systems are scalable and reliable - you register both authoring and
            execution <span class="ph">Data
                  Collector</span>s
            with <span class="ph">Control Hub</span>. You register <span class="ph">Data
                  Collector</span>s
            by manually administering them or by automatically provisioning them on a container
            orchestration framework such as Kubernetes. All registered <span class="ph">Data
                  Collector</span>s -
            either manually administered or automatically provisioned - function in the same
            way.</p>

        <p class="p">If your organization runs dataflows on edge devices, you also register and manually
            administer execution <span class="ph">Edge
                        Data Collectors</span> with <span class="ph">Control Hub</span>. </p>

        <p class="p">You create <span class="ph">Control Hub</span>
            jobs to run pipelines on groups of execution <span class="ph">Data
                  Collector</span>s
            or <span class="ph">Edge
                        Data Collectors</span>. When you start a job on a group of <span class="ph">Data
                  Collector</span>s
            or <span class="ph">Edge
                        Data Collectors</span>, <span class="ph">Control Hub</span>
            remotely runs the pipeline on each execution component in the group. This enables you to
            manage and orchestrate large scale dataflows run across multiple <span class="ph">Data
                  Collector</span>s
            or <span class="ph">Edge
                        Data Collectors</span>.</p>

        <p class="p">You might organize jobs by project, geographic region, or department. For example, your
            data engineer has developed and published the WebLog Collection pipeline and the EDW
            Replication Flows pipeline. In your data center, you designate <span class="ph">Data
                  Collector</span>s
            that run on several of the servers as the web server group. You designate another group
            of <span class="ph">Data
                  Collector</span>s running on other servers as the data warehouse group. You then create a job to run
            the WebLog Collection pipeline on the group of web server <span class="ph">Data
                  Collector</span>s. </p>

        <p class="p">The following image displays how the WebLog Collection job runs a remote pipeline
            instance on each <span class="ph">Data
                  Collector</span> in
            the web server group. The job does not run pipelines on <span class="ph">Data
                  Collector</span>s
            in the data warehouse group, which are <span class="ph">Data
                  Collector</span>s
            that are reserved for pipelines that read from the enterprise data warehouse.</p>

        <p class="p"><img class="image" id="concept_inh_ccs_bx__image_fkl_vcc_fbb" src="../Graphics/DPM_ManageOrchestration.png" height="201" width="578" /></p>

    </div>

</div>
<div class="topic concept nested1" id="concept_t34_2ds_bx">
 <h2 class="title topictitle2">Map Jobs into a Topology</h2>

 
 <div class="body conbody"><p class="shortdesc">In <span class="ph">Data
                  Collector</span>, you can monitor and view the details of a single pipeline. However, you typically run
        multiple intermediary pipelines, all of which work together to create a complete
        dataflow.</p>

        <p class="p">As a data architect,
            you create a topology in <span class="ph">Control Hub</span> to
            map multiple related jobs into a single view. A topology provides interactive end-to-end
            views of data as it traverses multiple pipelines. You can add any number of jobs to a
            topology.</p>

        <p class="p">To continue our Customer 360 example, after the WebLog Collection pipeline reads web
            server log files and writes the data to Kafka, another pipeline consumes the Kafka data,
            processes it, and streams the data to HDFS. Additional pipelines read from Twitter
            social feeds and from an enterprise data warehouse and also write the data to HDFS. In
                <span class="ph">Control Hub</span>, you can create a topology that includes jobs for all the pipelines, as follows: </p>

        <p class="p"><img class="image" id="concept_t34_2ds_bx__image_q5h_1dc_fbb" src="../Graphics/DPM_MapPipelinesTopology.png" height="406" width="825" /></p>

        <p class="p">From the topology, you can select each job and then drill into the configuration details
            of each pipeline. For example, if we select the Social Feeds Dataflows job in the
            topology canvas above, we can the three stages included in the pipeline in the detail
            pane on the right.</p>

    </div>

</div>
<div class="topic concept nested1" id="concept_sx5_zds_bx">
 <h2 class="title topictitle2">Measure Dataflow Quality</h2>

 
 <div class="body conbody"><p class="shortdesc"><span class="ph">Control Hub</span>
        provides the same level of detailed monitoring for topologies and jobs that you are
        accustomed to seeing for pipelines within <span class="ph">Data
                  Collector</span>. </p>

        <p class="p">As a data architect or
            as a DevOps or site reliability engineer, you can measure the health of the topology and
            the performance of all jobs and connecting systems included in the topology. <span class="ph">Control Hub</span>
            monitoring provides real-time statistics and error information about the running
            pipelines. </p>

        <p class="p">For example, the detail pane of the Customer 360 topology provides a single view into the
            record count and throughput for all running pipelines in the topology:</p>

        <p class="p"><img class="image" id="concept_sx5_zds_bx__image_t1k_s2c_fbb" src="../Graphics/DPM_MeasureQualityTopology.png" height="397" width="885" /></p>

        <p class="p">You can select a job or a connecting system within the topology to discover more detailed
            monitoring about the job or system. </p>

        <p class="p">When you start a job on a group of <span class="ph">Data
                  Collector</span>s,
                <span class="ph">Control Hub</span>
            provides a single view of the statistics for the complete job. From a job, you can view
            the statistics for a single pipeline or you can view the aggregated statistics across
            all remote pipeline instances that are run on a group of <span class="ph">Data
                  Collector</span>s.</p>

        <p class="p">For example, if we select the Social Feeds Dataflows job in the topology canvas, the
            detail pane displays metrics for the selected job:</p>

        <p class="p"><img class="image" id="concept_sx5_zds_bx__image_hrt_t2c_fbb" src="../Graphics/DPM_MeasureQualityJob.png" height="441" width="889" /></p>

    </div>

</div>
<div class="topic concept nested1" id="concept_xvp_x2x_fz">
 <h2 class="title topictitle2">Monitor Dataflow Operations</h2>

 
 <div class="body conbody"><p class="shortdesc">As a DevOps or site reliability engineer, you can monitor your day-to-day operations by
        defining data SLAs (service level agreements) to ensure that incoming data meets business
        requirements for availability and accuracy.</p>

  <p class="p">
            
        </p>

        <p class="p">In addition to measuring the health of a topology, you define data SLAs to define the
            expected thresholds of the data throughput rate or the error record rate. Data SLAs
            trigger an alert when the specified threshold is reached. Data SLA alerts provide
            immediate feedback on the data processing rates expected by your team. They enable you
            to monitor your dataflow operations and quickly investigate and resolve issues that
            arise.</p>

        <p class="p">For example, you have service level agreements with the operational analytics team to
            ensure that all of the data captured and processed in the Customer 360 topology is clean
            and available for immediate analysis. If any of the Customer 360 jobs encounter
            processing errors, you must immediately resolve those issues. You define and activate a
            data SLA that triggers an alert when a job in the topology encounters more than 100
            error records per second. </p>

        <p class="p">If the alert triggers, <span class="ph">Control Hub</span>
            notifies you with a red Notifications icon in the top toolbar: <img class="image" id="concept_xvp_x2x_fz__image_gk1_rfx_fz" src="../Graphics/icon_Notifications.png" height="16" width="14" />. You
            drill into the details of the data SLA to discover which threshold was reached and to
            investigate the issues that need to be resolved. The triggered data SLA displays a graph
            of the error record rate. The red line in the graph represents the defined threshold, as
            follows:</p>

        <p class="p"><img class="image" id="concept_xvp_x2x_fz__image_q5c_y2c_fbb" src="../Graphics/DPM_MasterOperations.png" height="443" width="538" /></p>

        <p class="p">We've seen how you can use <span class="ph">Control Hub</span> to
            turn a high-level architecture diagram of your dataflows into pipelines and jobs that
            you can then manage and measure from a single topology. Give it a try, and see for
            yourself how easily you can control all of your complex dataflow pipelines with <span class="ph">Control Hub</span>.</p>

 </div>

</div>
</div>
<div class="navfooter"><!---->
<span class="navparent"><a class="link" href="../DPM/DPM_title.html" title="StreamSets Control Hub"><span class="navheader_label">Parent topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">StreamSets Control Hub</span></a></span>  </div><div class="footer" id="webhelp_copyright_information"><!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

--><!-- SDC google analytics --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-60917135-3', 'auto');
  ga('send', 'pageview');
</script></div>
</body>
</html>