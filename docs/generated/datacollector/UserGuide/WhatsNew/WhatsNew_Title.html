
<!DOCTYPE html
  PUBLIC "" "about:legacy-compat">
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:whc="http://www.oxygenxml.com/webhelp/components" xml:lang="en-us" lang="en-us">
    <head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><link rel="shortcut icon" href="../../../oxygen-webhelp/template/images/favicon.png"><!----></link><link rel="icon" href="../../../oxygen-webhelp/template/images/favicon.png"><!----></link>        
      <meta name="copyright" content="(C) Copyright 2020" /><meta name="DC.rights.owner" content="(C) Copyright 2020" /><meta name="DC.Type" content="concept" /><meta name="DC.Title" content="What's New" /><meta name="abstract" content="" /><meta name="description" content="Data Collector version 3.19.x includes the following new features and enhancements: StreamSets Accounts StreamSets Accounts enables users without an enterprise account to download the latest version ..." /><meta name="DC.Relation" scheme="URI" content="../../../datacollector/UserGuide/Getting_Started/GettingStarted_Title.html#concept_htw_ghg_jq" /><meta name="DC.Relation" scheme="URI" content="../../../datacollector/UserGuide/Installation/Install_title.html" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="version" content="3" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="release" content="16" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="modification" content="0" /><meta name="DC.Date.Created" content="2014-10-31" /><meta name="DC.Format" content="XHTML" /><meta name="DC.Identifier" content="concept_hz3_5fk_fy" /><title>What's New</title><!--  Generated with Oxygen version 20.0-SNAPSHOT, build number 2018042310.  --><meta name="wh-path2root" content="../../../" /><meta name="wh-toc-id" content="concept_hz3_5fk_fy-d46e1069" />         
        
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <!-- Latest compiled and minified Bootstrap CSS -->
        <link rel="stylesheet" type="text/css" href="../../../oxygen-webhelp/lib/bootstrap/css/bootstrap.min.css" />

        <!-- Bootstrap Optional theme -->
        <link rel="stylesheet" href="../../../oxygen-webhelp/lib/bootstrap/css/bootstrap-theme.min.css" />
        <link rel="stylesheet" href="../../../oxygen-webhelp/lib/jquery-ui/jquery-ui.min.css" />

        <!-- Template default styles  -->
        <link rel="stylesheet" type="text/css" href="../../../oxygen-webhelp/app/topic-page.css?buildId=2018042310" />
        

        <script type="text/javascript" src="../../../oxygen-webhelp/lib/jquery/jquery-3.1.1.min.js"><!----></script>

        <script data-main="../../../oxygen-webhelp/app/topic-page.js" src="../../../oxygen-webhelp/lib/requirejs/require.js"></script>
        
        <!-- Skin resources -->
        <link rel="stylesheet" type="text/css" href="../../../oxygen-webhelp/template/light.css?buildId=2018042310" />
        <!-- EXM-36950 - Expand the args.hdf parameter here -->
        
        
    <link rel="stylesheet" type="text/css" href="../../../skin.css" /></head>

    <body class="wh_topic_page frmBody">
        <!-- EXM-36950 - Expand the args.hdr parameter here -->
        
        
        
<nav class="navbar navbar-default wh_header">
    <div class="container-fluid">
        <div class="wh_header_flex_container">
            <div class="wh_logo_and_publication_title_container">
                <div class="wh_logo_and_publication_title">
                    
                    <!--
                            This component will be generated when the next parameters are specified in the transformation scenario:
                            'webhelp.logo.image' and 'webhelp.logo.image.target.url'.
                            See: http://oxygenxml.com/doc/versions/17.1/ug-editor/#topics/dita_webhelp_output.html.
                    -->
                    <a href="http://streamsets.com" class=" wh_logo hidden-xs "></a>
                    <div class=" wh_publication_title "><a href="../../../index.html"><span class="booktitle">  <span class="ph mainbooktitle"><span class="ph">Data Collector</span> User Guide</span>  </span></a></div>
                    
                </div>
                
                <!-- The menu button for mobile devices is copied in the output only when the 'webhelp.show.top.menu' parameter is set to 'yes' -->
                
            </div>

            <div class="wh_top_menu_and_indexterms_link collapse navbar-collapse">
                
                
                <div class=" wh_indexterms_link "><a href="../../../indexTerms.html" title="Index"><span>Index</span></a></div>
                
            </div>
        </div>
    </div>
</nav>

        <div class=" wh_search_input "><form id="searchForm" method="get" action="../../../search.html"><div><input type="search" placeholder="Search " class="wh_search_textfield" id="textToSearch" name="searchQuery" /><button type="submit" class="wh_search_button"><span>Search</span></button></div><script><!--
                                    $(document).ready(function () {
                                        $('#searchForm').submit(function (e) {
                                            if ($('.wh_search_textfield').val().length < 1) {
                                                e.preventDefault();
                                            }
                                        });
                                    });
                                --></script></form></div>
        
        <div class="container-fluid">
            <div class="row">

                <nav class="wh_tools hidden-print">
                    <div data-tooltip-position="bottom" class=" wh_breadcrumb "><ol xmlns:html="http://www.w3.org/1999/xhtml" class="hidden-print"><li><span class="home"><a href="../../../index.html"><span>Home</span></a></span></li>
   <li class="active"><span class="topicref" data-id="concept_hz3_5fk_fy"><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_hz3_5fk_fy">What's New</a><span class="wh-tooltip">
               
               <p xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="shortdesc"></p>
               </span></span></span></li>
</ol></div>

                    <div class="wh_right_tools hidden-sm hidden-xs">
                        <div class=" wh_navigation_links "><span id="topic_navigation_links" class="navheader">
<span class="navprev"><a class="link" href="../../../datacollector/UserGuide/Getting_Started/GettingStarted_Title.html#concept_htw_ghg_jq" title="Getting Started"></a></span>  
<span class="navnext"><a class="link" href="../../../datacollector/UserGuide/Installation/Install_title.html" title="Installation"></a></span>  </span></div>
                        <button class="wh_hide_highlight" title="Toggle search highlights"></button>
                        <button class="webhelp_expand_collapse_sections" data-next-state="collapsed" title="Collapse sections"></button>
                        <div class=" wh_print_link print "><a href="javascript:window.print();" title="Print this page"></a></div>
                    </div>
                </nav>
            </div>

            <div class="wh_content_area">
                <div class="row">
                    
                        <nav role="navigation" id="wh_publication_toc" class="col-lg-3 col-md-3 col-sm-3 hidden-xs navbar hidden-print">
                            <div class=" wh_publication_toc " data-tooltip-position="right"><ul>
   <li><span data-tocid="concept_htw_ghg_jq-d46e53" class="topicref" data-id="concept_htw_ghg_jq" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Getting_Started/GettingStarted_Title.html#concept_htw_ghg_jq">Getting Started</a></span></span></li>
   <li class="active"><span data-tocid="concept_hz3_5fk_fy-d46e1069" class="topicref" data-id="concept_hz3_5fk_fy" data-state="expanded"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_hz3_5fk_fy">What's New</a><span class="wh-tooltip">
               
               <p xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="shortdesc"></p>
               </span></span></span><ul class="nav nav-list">
         <li><span data-tocid="concept_jsb_14y_lnb-d46e1537" class="topicref" data-id="concept_jsb_14y_lnb" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_jsb_14y_lnb">What's New in 3.19.x</a><span class="wh-tooltip">
                     
                     <p xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="shortdesc"></p>
                     </span></span></span></li>
         <li><span data-tocid="concept_ky1_flc_smb-d46e1561" class="topicref" data-id="concept_ky1_flc_smb" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_ky1_flc_smb">What's New in 3.18.x</a><span class="wh-tooltip">
                     
                     <p xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="shortdesc"></p>
                     </span></span></span></li>
         <li><span data-tocid="concept_hjn_xx4_kmb-d46e1595" class="topicref" data-id="concept_hjn_xx4_kmb" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_hjn_xx4_kmb">What's New in 3.17.x</a><span class="wh-tooltip">
                     
                     <p xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="shortdesc"></p>
                     </span></span></span></li>
         <li><span data-tocid="concept_tky_1dt_qlb-d46e1639" class="topicref" data-id="concept_tky_1dt_qlb" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_tky_1dt_qlb">What's New in 3.16.x</a></span></span></li>
         <li><span data-tocid="concept_cls_sgy_glb-d46e1691" class="topicref" data-id="concept_cls_sgy_glb" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_cls_sgy_glb">What's New in 3.15.x</a><span class="wh-tooltip">
                     
                     <p xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="shortdesc"></p>
                     </span></span></span></li>
         <li><span data-tocid="concept_gcq_bkh_blb-d46e1756" class="topicref" data-id="concept_gcq_bkh_blb" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_gcq_bkh_blb">What's New in 3.14.x</a><span class="wh-tooltip">
                     
                     <p xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="shortdesc"></p>
                     </span></span></span></li>
         <li><span data-tocid="concept_rs2_mln_hkb-d46e1918" class="topicref" data-id="concept_rs2_mln_hkb" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_rs2_mln_hkb">What's New in 3.13.x</a><span class="wh-tooltip">
                     
                     <p xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="shortdesc"></p>
                     </span></span></span></li>
         <li><span data-tocid="concept_oms_dtd_qjb-d46e2013" class="topicref" data-id="concept_oms_dtd_qjb" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_oms_dtd_qjb">What's New in 3.12.x</a><span class="wh-tooltip">
                     
                     <p xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="shortdesc"></p>
                     </span></span></span></li>
         <li><span data-tocid="concept_rjw_bh2_hjb-d46e2118" class="topicref" data-id="concept_rjw_bh2_hjb" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_rjw_bh2_hjb">What's New in 3.11.x</a><span class="wh-tooltip">
                     
                     <p xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="shortdesc"></p>
                     </span></span></span></li>
         <li><span data-tocid="concept_bfw_mrk_43b-d46e2233" class="topicref" data-id="concept_bfw_mrk_43b" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_bfw_mrk_43b">What's New in 3.10.x</a><span class="wh-tooltip">
                     
                     <p xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="shortdesc"></p>
                     </span></span></span></li>
         <li><span data-tocid="concept_sjn_dcb_zhb-d46e2359" class="topicref" data-id="concept_sjn_dcb_zhb" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_sjn_dcb_zhb">What's New in 3.9.x</a><span class="wh-tooltip">
                     
                     <p xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="shortdesc"></p>
                     </span></span></span></li>
         <li><span data-tocid="concept_lvk_lsk_1hb-d46e2641" class="topicref" data-id="concept_lvk_lsk_1hb" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_lvk_lsk_1hb">What's New in 3.8.0</a></span></span></li>
         <li><span data-tocid="concept_fnl_4lt_dgb-d46e2959" class="topicref" data-id="concept_fnl_4lt_dgb" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_fnl_4lt_dgb">What's New in 3.7.0</a></span></span></li>
         <li><span data-tocid="concept_fmr_1sz_tfb-d46e3317" class="topicref" data-id="concept_fmr_1sz_tfb" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_fmr_1sz_tfb">What's New in 3.6.0</a></span></span></li>
         <li><span data-tocid="concept_rs3_c31_2fb-d46e3511" class="topicref" data-id="concept_rs3_c31_2fb" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_rs3_c31_2fb">What's New in 3.5.0</a></span></span></li>
         <li><span data-tocid="concept_ynr_5lt_m2b-d46e3715" class="topicref" data-id="concept_ynr_5lt_m2b" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_ynr_5lt_m2b">What's New in 3.4.0</a></span></span></li>
         <li><span data-tocid="concept_gbv_rcr_h2b-d46e3929" class="topicref" data-id="concept_gbv_rcr_h2b" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_gbv_rcr_h2b">What's New in 3.3.1</a></span></span></li>
         <li><span data-tocid="concept_k42_pbc_xdb-d46e4153" class="topicref" data-id="concept_k42_pbc_xdb" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_k42_pbc_xdb">What's New in 3.3.0</a></span></span></li>
         <li><span data-tocid="concept_yv1_cm2_pdb-d46e4387" class="topicref" data-id="concept_yv1_cm2_pdb" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_yv1_cm2_pdb">What's New in 3.2.0.0</a></span></span></li>
         <li><span data-tocid="concept_ozm_kxk_fdb-d46e4631" class="topicref" data-id="concept_ozm_kxk_fdb" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_ozm_kxk_fdb">What's New in 3.1.2.0</a></span></span></li>
         <li><span data-tocid="concept_g3c_fn2_ycb-d46e4885" class="topicref" data-id="concept_g3c_fn2_ycb" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_g3c_fn2_ycb">What's New in 3.1.1.0</a></span></span></li>
         <li><span data-tocid="concept_agl_4tw_scb-d46e5150" class="topicref" data-id="concept_agl_4tw_scb" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_agl_4tw_scb">What's New in 3.1.0.0</a></span></span></li>
         <li><span data-tocid="concept_z4b_qrb_4cb-d46e5424" class="topicref" data-id="concept_z4b_qrb_4cb" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_z4b_qrb_4cb">What's New in 3.0.3.0</a></span></span></li>
         <li><span data-tocid="concept_lhn_cvv_lcb-d46e5708" class="topicref" data-id="concept_lhn_cvv_lcb" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_lhn_cvv_lcb">What's New in 3.0.2.0</a></span></span></li>
         <li><span data-tocid="concept_lgz_yp4_gcb-d46e6002" class="topicref" data-id="concept_lgz_yp4_gcb" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_lgz_yp4_gcb">What's New in 3.0.1.0</a></span></span></li>
         <li><span data-tocid="concept_cjx_y4k_wbb-d46e6307" class="topicref" data-id="concept_cjx_y4k_wbb" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_cjx_y4k_wbb">What's New in 3.0.0.0</a></span></span></li>
         <li><span data-tocid="concept_rrq_v3k_kbb-d46e6622" class="topicref" data-id="concept_rrq_v3k_kbb" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_rrq_v3k_kbb">What's New in 2.7.2.0</a></span></span></li>
         <li><span data-tocid="concept_rr2_mbz_w1b-d46e6947" class="topicref" data-id="concept_rr2_mbz_w1b" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_rr2_mbz_w1b">What's New in 2.7.1.1</a></span></span></li>
         <li><span data-tocid="unique_1138101209-d46e7282" class="topicref" data-id="unique_1138101209" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#unique_1138101209">What's New in 2.7.1.0</a></span></span></li>
         <li><span data-tocid="unique_184236386-d46e7627" class="topicref" data-id="unique_184236386" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#unique_184236386">What's New in 2.7.0.0</a></span></span></li>
         <li><span data-tocid="concept_avm_x1y_h1b-d46e7982" class="topicref" data-id="concept_avm_x1y_h1b" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_avm_x1y_h1b">What's New in 2.6.0.1</a></span></span></li>
         <li><span data-tocid="concept_bsw_cky_11b-d46e8347" class="topicref" data-id="concept_bsw_cky_11b" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_bsw_cky_11b">What's New in 2.6.0.0</a></span></span></li>
         <li><span data-tocid="concept_afr_hly_tz-d46e8723" class="topicref" data-id="concept_afr_hly_tz" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_afr_hly_tz">What's New in 2.5.1.0</a></span></span></li>
         <li><span data-tocid="concept_ddx_bpm_pz-d46e9108" class="topicref" data-id="concept_ddx_bpm_pz" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_ddx_bpm_pz">What's New in 2.5.0.0</a></span></span></li>
         <li><span data-tocid="concept_cf2_sdz_fz-d46e9503" class="topicref" data-id="concept_cf2_sdz_fz" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_cf2_sdz_fz">What's New in 2.4.1.0</a></span></span></li>
         <li><span data-tocid="concept_kzc_4sd_yy-d46e9908" class="topicref" data-id="concept_kzc_4sd_yy" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_kzc_4sd_yy">What's New in 2.4.0.0</a></span></span></li>
         <li><span data-tocid="concept_bml_dbt_wy-d46e10324" class="topicref" data-id="concept_bml_dbt_wy" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_bml_dbt_wy">What's New in 2.3.0.1</a></span></span></li>
         <li><span data-tocid="concept_yym_xqt_5y-d46e10750" class="topicref" data-id="concept_yym_xqt_5y" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_yym_xqt_5y">What's New in 2.3.0.0</a></span></span></li>
         <li><span data-tocid="concept_wbf_dgk_fy-d46e11186" class="topicref" data-id="concept_wbf_dgk_fy" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_wbf_dgk_fy">What's New in 2.2.1.0</a></span></span></li>
         <li><span data-tocid="concept_oyv_zfk_fy-d46e11632" class="topicref" data-id="concept_oyv_zfk_fy" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_oyv_zfk_fy">What's New in 2.2.0.0</a></span></span></li>
      </ul>
   </li>
   <li><span data-tocid="concept_l4q_flb_kr-d46e12090" class="topicref" data-id="concept_l4q_flb_kr" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Installation/Install_title.html">Installation</a></span></span></li>
   <li><span data-tocid="concept_ylh_yyz_ky-d46e14866" class="topicref" data-id="concept_ylh_yyz_ky" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Configuration/Config_title.html">Configuration</a></span></span></li>
   <li><span data-tocid="concept_ejk_f1f_5v-d46e25044" class="topicref" data-id="concept_ejk_f1f_5v" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Upgrade/Upgrade_title.html">Upgrade</a></span></span></li>
   <li><span data-tocid="concept_qsw_cjy_bt-d46e33436" class="topicref" data-id="concept_qsw_cjy_bt" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Pipeline_Design/PipelineDesign_title.html">Pipeline Concepts and Design</a></span></span></li>
   <li><span data-tocid="concept_qn1_wn4_kq-d46e35308" class="topicref" data-id="concept_qn1_wn4_kq" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Pipeline_Configuration/PipelineConfiguration_title.html">Pipeline Configuration</a></span></span></li>
   <li><span data-tocid="concept_hdr_gyw_41b-d46e39288" class="topicref" data-id="concept_hdr_gyw_41b" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Data_Formats/DataFormats-Title.html">Data Formats</a><span class="wh-tooltip">
               
               <p xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="shortdesc"></p>
               </span></span></span></li>
   <li><span data-tocid="concept_yjl_nc5_jq-d46e41548" class="topicref" data-id="concept_yjl_nc5_jq" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Origins/Origins_title.html">Origins</a></span></span></li>
   <li><span data-tocid="concept_yjl_nc5_jq-d46e112031" class="topicref" data-id="concept_yjl_nc5_jq" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Processors/Processors_title.html">Processors</a></span></span></li>
   <li><span data-tocid="concept_agj_cfj_br-d46e132720" class="topicref" data-id="concept_agj_cfj_br" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Destinations/Destinations-title.html">Destinations</a></span></span></li>
   <li><span data-tocid="concept_umc_1lk_fx-d46e161471" class="topicref" data-id="concept_umc_1lk_fx" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Executors/Executors-title.html">Executors</a></span></span></li>
   <li><span data-tocid="concept_fyf_gkq_4bb-d46e171123" class="topicref" data-id="concept_fyf_gkq_4bb" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Edge_Mode/EdgePipelines_title.html"><span xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="ph">StreamSets Data Collector Edge</span></a></span></span></li>
   <li><span data-tocid="concept_ugp_kwf_xw-d46e173726" class="topicref" data-id="concept_ugp_kwf_xw" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/DPM/DPM_title.html">StreamSets Control Hub</a></span></span></li>
   <li><span data-tocid="concept_xxd_f5r_kx-d46e177260" class="topicref" data-id="concept_xxd_f5r_kx" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Event_Handling/EventFramework-Title.html#concept_xxd_f5r_kx">Dataflow Triggers</a><span class="wh-tooltip">
               
               <p xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="shortdesc"></p>
               </span></span></span></li>
   <li><span data-tocid="concept_zq5_pb4_flb-d46e179451" class="topicref" data-id="concept_zq5_pb4_flb" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Solutions/Solutions-title.html">Solutions</a></span></span></li>
   <li><span data-tocid="concept_wwq_gxc_py-d46e183788" class="topicref" data-id="concept_wwq_gxc_py" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Multithreaded_Pipelines/MultithreadedPipelines.html#concept_wwq_gxc_py">Multithreaded Pipelines</a><span class="wh-tooltip">
               
               <p xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="shortdesc"></p>
               </span></span></span></li>
   <li><span data-tocid="concept_gzw_tdm_p2b-d46e184370" class="topicref" data-id="concept_gzw_tdm_p2b" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Microservice/Microservice_Title.html#concept_gzw_tdm_p2b">Microservice Pipelines</a></span></span></li>
   <li><span data-tocid="Orchestrators_Title-d46e184742" class="topicref" data-id="Orchestrators_Title" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Orchestration_Pipelines/OrchestrationPipelines_Title.html#Orchestrators_Title">Orchestration Pipelines</a></span></span></li>
   <li><span data-tocid="concept_wr1_ktz_bt-d46e185034" class="topicref" data-id="concept_wr1_ktz_bt" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/RPC_Pipelines/SDC_RPCpipelines_title.html#concept_wr1_ktz_bt">SDC RPC Pipelines</a></span></span></li>
   <li><span data-tocid="concept_fpz_5r4_vs-d46e185516" class="topicref" data-id="concept_fpz_5r4_vs" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Cluster_Mode/ClusterPipelines_title.html">Cluster Pipelines</a></span></span></li>
   <li><span data-tocid="concept_jjk_23z_sq-d46e186661" class="topicref" data-id="concept_jjk_23z_sq" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Data_Preview/DataPreview_Title.html#concept_jjk_23z_sq">Data Preview</a></span></span></li>
   <li><span data-tocid="concept_pgk_brx_rr-d46e187617" class="topicref" data-id="concept_pgk_brx_rr" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Alerts/RulesAlerts_title.html#concept_pgk_brx_rr">Rules and Alerts</a><span class="wh-tooltip">
               
               <p xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="shortdesc"></p>
               </span></span></span></li>
   <li><span data-tocid="concept_asx_fdz_sq-d46e190245" class="topicref" data-id="concept_asx_fdz_sq" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Pipeline_Monitoring/PipelineMonitoring_title.html#concept_asx_fdz_sq">Pipeline Monitoring</a><span class="wh-tooltip">
               
               <p xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="shortdesc"></p>
               </span></span></span></li>
   <li><span data-tocid="concept_o3l_dtr_5q-d46e191502" class="topicref" data-id="concept_o3l_dtr_5q" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Pipeline_Maintenance/PipelineMaintenance_title.html#concept_o3l_dtr_5q">Pipeline Maintenance</a></span></span></li>
   <li><span data-tocid="concept_yms_ftm_sq-d46e193332" class="topicref" data-id="concept_yms_ftm_sq" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Administration/Administration_title.html#concept_yms_ftm_sq">Administration</a></span></span></li>
   <li><span data-tocid="concept_nls_w1r_ks-d46e198182" class="topicref" data-id="concept_nls_w1r_ks" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Tutorial/Tutorial-title.html">Tutorial</a></span></span></li>
   <li><span data-tocid="concept_sh3_frm_tq-d46e199387" class="topicref" data-id="concept_sh3_frm_tq" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Troubleshooting/Troubleshooting_title.html#concept_sh3_frm_tq">Troubleshooting</a><span class="wh-tooltip">
               
               <p xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="shortdesc"></p>
               </span></span></span></li>
   <li><span data-tocid="concept_xbx_rs1_tq-d46e204830" class="topicref" data-id="concept_xbx_rs1_tq" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Glossary/Glossary_title.html#concept_xbx_rs1_tq">Glossary</a></span></span></li>
   <li><span data-tocid="concept_jn1_nzb_kv-d46e204885" class="topicref" data-id="concept_jn1_nzb_kv" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Apx-DataFormats/DataFormat_Title.html#concept_jn1_nzb_kv">Data Formats by Stage</a><span class="wh-tooltip">
               
               <p xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="shortdesc"></p>
               </span></span></span></li>
   <li><span data-tocid="concept_pvm_yt3_wq-d46e205107" class="topicref" data-id="concept_pvm_yt3_wq" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Expression_Language/ExpressionLanguage_title.html">Expression Language</a></span></span></li>
   <li><span data-tocid="concept_vcj_1ws_js-d46e208778" class="topicref" data-id="concept_vcj_1ws_js" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Apx-RegEx/RegEx-Title.html#concept_vcj_1ws_js">Regular Expressions</a><span class="wh-tooltip">
               
               <p xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="shortdesc"></p>
               </span></span></span></li>
   <li><span data-tocid="concept_chv_vmj_wr-d46e209001" class="topicref" data-id="concept_chv_vmj_wr" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Apx-GrokPatterns/GrokPatterns_title.html#concept_chv_vmj_wr">Grok Patterns</a><span class="wh-tooltip">
               
               <p xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="shortdesc"></p>
               </span></span></span></li>
</ul></div>
                        </nav>
                    
                    
                    <div class="col-lg-9 col-md-9 col-sm-9 col-xs-12" id="wh_topic_body">
                        <div class=" wh_topic_content body "><main role="main"><article role="article" aria-labelledby="ariaid-title1"><article class="nested0" aria-labelledby="ariaid-title1" id="concept_hz3_5fk_fy">
 <h1 class="title topictitle1" id="ariaid-title1">What's New</h1>

 
 <div class="body conbody"><p class="shortdesc"></p>

  <p class="p"></p>

 </div>

<article class="topic concept nested1" aria-labelledby="ariaid-title2" id="concept_jsb_14y_lnb">
    <h2 class="title topictitle2" id="ariaid-title2">What's New in 3.19.x</h2>

    
    <div class="body conbody"><p class="shortdesc"></p>

        <div class="p"><span class="ph">Data Collector</span>
            version 3.19.x includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">StreamSets Accounts</dt>

                    <dd class="dd">
                        <p dir="ltr" class="p" id="concept_jsb_14y_lnb__docs-internal-guid-53828bb3-7fff-fdfc-2adc-ae433a675957"><a class="xref" href="../Administration/Administration_title.html#concept_nqf_ls3_glb">StreamSets Accounts</a> enables users without an enterprise
                            account to download the latest version of <span class="ph">Data Collector</span> and Transformer and to log into linked <span class="ph">Data Collector</span>s and Transformers. Users with a StreamSets enterprise account use the
                            StreamSets Support portal for downloads. </p>

                        <p dir="ltr" class="p">You can use an existing Google or Microsoft account to register
                            with StreamSets Accounts. Or, you can enter an email address and
                            password. After you're registered, you can download the latest <span class="ph">Data Collector</span> tarball or access Docker for the latest <span class="ph">Data Collector</span> image. StreamSets Accounts provides downloads for the common
                            installation tarball.</p>

                        <p dir="ltr" class="p">After you install <span class="ph">Data Collector</span>, you link it to StreamSets Accounts. StreamSets Accounts then
                            provides single sign on authentication for <span class="ph">Data Collector</span>. You can change the authentication method as needed.</p>

                        <div class="note note"><span class="notetitle">Note:</span> <span class="ph" id="concept_jsb_14y_lnb__d245e507">Users with an enterprise account
                              should not use StreamSets Accounts. Also, users who installed Data
                              Collector through a cloud service provider marketplace do not need to
                              use StreamSets Accounts.</span>
                  </div>

                    </dd>

                
                
                    <dt class="dt dlterm">Common Installation</dt>

                    <dd class="dd">
                        <p dir="ltr" class="p" id="concept_jsb_14y_lnb__docs-internal-guid-9eccb99b-7fff-97cc-ce6e-fc19d35cd6cc">StreamSets now provides a <a class="xref" href="../Installation/Commoninstall.html#concept_wzr_gs5_hnb" title="All users can install the Data Collector common installation. The common installation includes all core Data Collector functionality and commonly-used stage libraries in a tarball installation."><span class="ph">Data Collector</span> common installation</a>. The common installation is a tarball
                            that contains all core <span class="ph">Data Collector</span> functionality as well as commonly-used stage libraries.</p>

                        <p dir="ltr" class="p">This installation allows you to create pipelines easily while
                            using less disk space than the full installation. After you perform the
                            common installation, you can install individual stage libraries as
                            needed.</p>

                        <p dir="ltr" class="p">The common installation is available to users with an
                            enterprise account through the StreamSets Support portal. The common
                            installation is available to all other users through StreamSets
                            Accounts.</p>

                    </dd>

                
                
                    <dt class="dt dlterm">Stage Enhancements</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_jsb_14y_lnb__ul_vlp_s4y_lnb">
                            <li class="li"><a class="xref" href="../Pipeline_Configuration/AmazonSecurity.html#concept_ypw_hbj_3nb">Amazon stages</a> - When Amazon stages use an instance
                                profile to authenticate with Amazon Web Services (AWS), you can
                                configure the stages to assume another role. For example, you might
                                need to assume another role when the IAM policies attached to the
                                instance profile do not grant the required access.<p dir="ltr" class="p">To
                                    assume another role, you first must define the IAM policy in AWS
                                    that allows the instance profile to assume a role. Then, you
                                    configure the required stage properties in <span class="ph">Data Collector</span>.</p>
<div class="p">
                                    <div class="note note"><span class="notetitle">Note:</span> <span class="ph">Data Collector</span> will provide support to assume another role when using
                                        AWS access keys in a future release.</div>

                                </div>
</li>

                            <li class="li"><a class="xref" href="../Pipeline_Configuration/KafkaSecurity.html#concept_jpy_ln5_klb">Kafka stages</a> - The Kafka Consumer, Kafka Multitopic
                                Consumer, and Kafka Producer stages include a Security tab used to
                                configure one of the following options to connect securely to
                                    Kafka:<ul class="ul" id="concept_jsb_14y_lnb__ul_vpv_bpy_lnb">
                                    <li class="li">SSL/TLS encryption</li>

                                    <li class="li">SSL/TLS encryption and authentication</li>

                                    <li class="li">Kerberos authentication</li>

                                    <li class="li">Kerberos authentication on SSL/TLS</li>

                                </ul>
<p dir="ltr" class="p">The keytab properties where you can provide
                                    Kerberos credentials have been moved from the Kafka tab to the
                                    Security tab.</p>
Previously, you configured Kafka security by
                                configuring additional Kafka configuration properties. </li>

                            <li class="li">Amazon SQS Consumer origin - You can specify the names of queues to
                                process instead of specifying queue name prefixes.</li>

                            <li class="li">Google Pub/Sub Subscriber origin:<ul class="ul" id="concept_jsb_14y_lnb__ul_ihk_3py_lnb">
                                    <li class="li">The default value for the Subscriber Thread Pool Size
                                        property is now 1.</li>

                                    <li class="li">The default value for the Num Pipeline Runners property is
                                        also 1.</li>

                                </ul>
<p dir="ltr" class="p">These changes do not affect existing
                                    pipelines.</p>
</li>

                            <li class="li"><a class="xref" href="../Origins/KinConsumer.html#concept_tfj_ptx_gnb">Kinesis Consumer origin</a> - You can configure additional
                                Kinesis configuration properties that you require.</li>

                            <li class="li">JMS Consumer origin - The origin includes JMS headers as record
                                header attributes. JMS record header attributes are named as
                                follows: <code class="ph codeph">jms.header.&lt;header name&gt;</code></li>

                            <li class="li">Oracle CDC Client origin:<ul class="ul" id="concept_jsb_14y_lnb__ul_b1m_4py_lnb">
                                    <li class="li">You can use the origin to process data from <a class="xref" href="../Origins/OracleCDC.html#concept_rs5_hjj_tw">Oracle Real Application Clusters (RAC) 12c, 18c and
                                            19c</a>.</li>

                                    <li class="li">You can configure the origin to disable the LogMiner <a class="xref" href="../Origins/OracleCDC.html#task_ehh_mjj_tw">CONTINUOUS_MINE option</a>. </li>

                                    <li class="li">When parsing SQL query statements, the origin no longer
                                        processes <a class="xref" href="../Origins/OracleCDC.html#concept_vfw_bjz_ty">SELECT_FOR_UPDATE statements</a>. This can affect
                                        existing pipelines. <p class="p">For information about possible
                                            upgrade impact, see <a class="xref" href="../Upgrade/PostUpgrade.html#concept_k53_vhk_cgb">Update Oracle CDC Client Pipelines</a>.</p>
</li>

                                </ul>
</li>

                            <li class="li">Delay processor - You can configure the processor to skip the delay
                                for empty batches.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Connections when Registered with Control Hub</dt>

                    <dd class="dd">
                        <p dir="ltr" class="p" id="concept_jsb_14y_lnb__docs-internal-guid-b95419a3-7fff-afcc-c451-58eac83d13e6">When <span class="ph">Data Collector</span> version 3.19.0 is registered with the Control Hub November release,
                            the following stages will support using Control Hub connections:</p>

                        <ul class="ul" id="concept_jsb_14y_lnb__ul_y51_dqy_lnb">
                            <li dir="ltr" class="li">Amazon stages</li>

                            <li dir="ltr" class="li">Google Cloud stages</li>

                            <li dir="ltr" class="li">JDBC stages</li>

                            <li dir="ltr" class="li">Kafka stages</li>

                            <li dir="ltr" class="li">Kudu stages</li>

                            <li dir="ltr" class="li">Salesforce stages</li>

                        </ul>

                        <div class="p" dir="ltr">Connections define the information required to access data in
                            external systems. You create connections in Control Hub and then use
                            those connections when configuring pipelines in Control Hub Pipeline
                            Designer. You cannot use Control Hub connections in the <span class="ph">Data Collector</span> pipeline canvas.<div class="note important"><span class="importanttitle">Important:</span> If you register <span class="ph">Data Collector</span> version 3.19.0 with Control Hub cloud before the November release
                                or with Control Hub on-premises version 3.18.x or earlier, then the
                                Connection property that displays for these stages in the Control
                                Hub Pipeline Designer is not supported. Do not change the property
                                from the default value of None. If you select Choose Value or use a
                                parameter to define the property, Pipeline Designer hides the
                                remaining connection properties and the pipeline fails to
                                run.</div>
</div>

                    </dd>

                
                
                    <dt class="dt dlterm">Stage Libraries</dt>

                    <dd class="dd">This release includes the following <a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">new stage libraries</a>:
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="concept_jsb_14y_lnb__table_bkb_lqy_lnb" class="table" frame="border" border="1" rules="all"><colgroup><col style="width:59.01639344262295%" /><col style="width:40.98360655737705%" /></colgroup><thead class="thead" style="text-align:left;">
                                    <tr>
                                        <th dir="ltr" class="entry cellrowborder" style="text-align:left;" id="d717115e788">Stage Library Name</th>

                                        <th dir="ltr" class="entry cellrowborder" style="text-align:left;" id="d717115e791">Description</th>

                                    </tr>

                                </thead>
<tbody class="tbody">
                                    <tr>
                                        <td dir="ltr" class="entry cellrowborder" style="text-align:left;" headers="d717115e788 ">streamsets-datacollector-elasticsearch_6-lib</td>

                                        <td dir="ltr" class="entry cellrowborder" style="text-align:left;" headers="d717115e791 ">For Elasticsearch 6.x. </td>

                                    </tr>

                                    <tr>
                                        <td dir="ltr" class="entry cellrowborder" style="text-align:left;" headers="d717115e788 ">streamsets-datacollector-elasticsearch_7-lib</td>

                                        <td dir="ltr" class="entry cellrowborder" style="text-align:left;" headers="d717115e791 ">For Elasticsearch 7.x. </td>

                                    </tr>

                                </tbody>
</table>
</div>
</dd>

                
            </dl>
<dl class="dl">
                
                    <dt class="dt dlterm">Additional Enhancements</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_jsb_14y_lnb__ul_d4s_pqy_lnb">
                            <li class="li"><a class="xref" href="../Configuration/DCConfig.html#task_lxk_kjw_1r" title="You can customize Data Collector by editing the Data Collector configuration file, sdc.properties.">New Data Collector configuration property</a> - The
                                    <code class="ph codeph">http.access.control.exposed.headers</code> property
                                enables listing a set of headers that can be exposed as part of a
                                cross-domain response.</li>

                        </ul>

                    </dd>

                
            </dl>
</div>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title3" id="concept_ky1_flc_smb">
    <h2 class="title topictitle2" id="ariaid-title3">What's New in 3.18.x</h2>

    
    <div class="body conbody"><p class="shortdesc"></p>

        <div class="p"><span class="ph">Data Collector</span>
            version 3.18.x includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Pipeline Advanced Options</dt>

                    <dd class="dd"><p dir="ltr" class="p" id="concept_ky1_flc_smb__docs-internal-guid-4954ed3c-7fff-953c-c442-555cc30d6de1">Pipelines and most pipeline stages include <a class="xref" href="../Pipeline_Configuration/AdvancedOptions.html#concept_ddf_hr1_rmb" title="Pipelines and most pipeline stages include advanced options with default values that should work in most cases. By default, each pipeline and stage hides the advanced options. Advanced options can include individual properties or complete tabs.">advanced options</a> with default values that should work in
                            most cases. By default, new and upgraded pipelines hide the advanced
                            options. Advanced options can include individual properties or complete
                            tabs.</p>
As you start designing pipelines, StreamSets recommends
                        configuring the basic properties and using the default values for the
                        advanced options. Then as you continue working with pipelines, explore the
                        advanced options to find ways to fine tune processing.</dd>

                
                
                    <dt class="dt dlterm">Sample Pipelines</dt>

                    <dd class="dd">Data Collector provides several <a class="xref" href="../Pipeline_Design/SamplePipelines.html#concept_jbj_4yg_rmb" title="Data Collector provides sample pipelines that you can use to learn about Data Collector features or as a basis for building your own pipelines.">sample pipelines</a> that you can use to learn about <span class="ph">Data Collector</span> features or as a basis for building your own pipelines.</dd>

                
                
                    <dt class="dt dlterm">Enterprise Stage Libraries</dt>

                    <dd class="dd">In September 2020, StreamSets released updated Enterprise stage libraries
                        for Databricks and Snowflake. </dd>

                    <dd class="dd ddexpand">
                        <p class="p">For a list of available Enterprise libraries, see <a class="xref" href="../Installation/AddtionalStageLibs.html#concept_s1r_1gg_dhb">Enterprise Stage Libraries</a>. For more information about the
                            new features, fixed issues, and known issues in an Enterprise stage
                            library, see the release notes for the Enterprise stage library,
                            available under <a class="xref" href="https://streamsets.com/documentation-page/#enterprise" target="_blank">Enterprise Libraries</a> on the
                            StreamSets Documentation page.</p>

                    </dd>

                    <dd class="dd ddexpand">Enterprise stage libraries are free for use in both development and
                        production.</dd>

                
                
                    <dt class="dt dlterm">Stage Enhancements</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_ky1_flc_smb__ul_x44_rlc_smb">
                            <li class="li">HTTP Client processor - The Missing Values Behavior property
                                determines the action that occurs when a request generates no
                                response. </li>

                            <li class="li">Amazon S3 stages - "IAM roles" are now known as "instance profiles"
                                in all Amazon S3 stages. This does not affect existing
                                pipelines.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Additional Enhancements</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_ky1_flc_smb__ul_t5v_vlc_smb">
                            <li class="li">Error records - Error records include Control Hub job information in
                                    <code class="ph codeph">jobName</code> and <code class="ph codeph">jobId</code> record
                                header attributes.</li>

                            <li class="li"><a class="xref" href="../Installation/CloudInstall.html#task_nc1_d3b_5kb" title="You can install the full Data Collector on Google Cloud Platform.">Default password for Google Cloud Platform marketplace
                                    installation</a> - The default password for <span class="ph">Data Collector</span> installed through the Google Cloud Platform marketplace is now
                                the VM Instance Id. Previously, the default password was
                                    <code class="ph codeph">admin</code>.</li>

                        </ul>

                    </dd>

                
            </dl>
</div>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title4" id="concept_hjn_xx4_kmb">
    <h2 class="title topictitle2" id="ariaid-title4">What's New in 3.17.x</h2>

    
    <div class="body conbody"><p class="shortdesc"></p>

        <div class="p"><span class="ph">Data Collector</span>
            version 3.17.x includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">New Stage</dt>

                    <dd class="dd">This release includes the following new stage:</dd>

                    <dd class="dd ddexpand">
                        <ul class="ul" id="concept_hjn_xx4_kmb__ul_cz2_sy4_kmb">
                            <li class="li"><a class="xref" href="../Origins/SAPHana.html#concept_pmt_ml3_3mb">SAP HANA
                                    Query Consumer</a> - Use this origin to read data from an SAP
                                HANA database with a user-defined query. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Stage Enhancements</dt>

                    <dd class="dd">This release includes the following stage enhancements:</dd>

                    <dd class="dd ddexpand">
                        <ul class="ul" id="concept_hjn_xx4_kmb__ul_qyt_tz4_kmb">
                            <li class="li">Amazon S3 stages - When you configure the <a class="xref" href="../Origins/AmazonS3.html#task_gfj_ssv_yq">Amazon S3 origin</a>, <a class="xref" href="../Destinations/AmazonS3.html#task_pxb_j3r_rt">destination</a>, or <a class="xref" href="../Executors/AmazonS3.html#task_nky_cnm_f1b">executor</a> or the <a class="xref" href="../Origins/AmazonSQS.html#task_jxn_nnm_5bb">Amazon SQS Consumer origin</a> to use a proxy to connect,
                                you can optionally configure a domain name and workstation for the
                                proxy server.</li>

                            <li class="li">Control Hub API processor - The processor can process responses of
                                any size. Previously the maximum response size was 50,000
                                characters. </li>

                            <li class="li"><a class="xref" href="../Destinations/Elasticsearch.html#task_uns_gtv_4r">Elasticsearch destination</a> - You can use record functions
                                and delimited data record functions in the Additional Properties
                                field.</li>

                            <li class="li">Elasticsearch stages - The <a class="xref" href="../Origins/Elasticsearch.html#task_pmh_xpm_2z">Elasticsearch origin</a> and <a class="xref" href="../Destinations/Elasticsearch.html#task_uns_gtv_4r">destination</a> include a User Name property and a Password
                                property instead of a single Security Username/Password
                                    property.<p class="p">This change does not affect existing pipelines.
                                    During an upgrade, existing configurations for the Security
                                    Username/Password property are placed into the User Name
                                    property, which supports the <code class="ph codeph">username:password</code>
                                    format. </p>
</li>

                            <li class="li">HTTP Client processor - You can configure the processor to use the
                                following enhancements:<ul class="ul" id="concept_hjn_xx4_kmb__ul_vqj_kdp_kmb">
                                    <li class="li"><a class="xref" href="../Processors/HTTPClient.html#concept_zpw_gl4_kmb" title="By default, the HTTP Client processor processes only responses that include a 2xx success status code. When the response includes any other status code, such as a 4xx or 5xx status code, the processor generates an error and handles the record based on the error record handling configured for the stage.">Actions</a> to take based on the response
                                        status.</li>

                                    <li class="li"><a class="xref" href="../Processors/HTTPClient.html#concept_yx4_w5h_kmb">Pagination properties</a> to enable processing large
                                        volumes of data from paginated APIs.</li>

                                    <li class="li">Action to take when the request times out because the HTTP
                                        service did not respond within the read timeout period.</li>

                                </ul>
</li>

                            <li class="li">JDBC MySQL data type conversions - The JDBC origins and JDBC
                                processors convert MySQL unsigned integers as follows:<ul class="ul" id="concept_hjn_xx4_kmb__ul_kw3_mdp_kmb">
                                    <li class="li">Bigint Unsigned converts to Decimal.</li>

                                    <li class="li">Int Unsigned and Mediumint Unsigned convert to Long.</li>

                                    <li class="li">Smallint Unsigned converts to Integer.</li>

                                </ul>
<p class="p">This change can require performing a <a class="xref" href="../Upgrade/PostUpgrade.html#concept_q1s_vcg_kmb">post-upgrade task</a>.</p>
</li>

                            <li class="li">Kinesis stages - The Kinesis Consumer origin, Kinesis Firehose
                                destination, and Kinesis Producer destination provide an
                                Authentication Method property that allows selecting either IAM
                                Roles or AWS Keys. <p class="p">Previously, you used IAM roles by omitting AWS
                                    keys when configuring the stages. This change does not affect
                                    existing pipelines. </p>
</li>

                            <li class="li">Orchestration stages: <ul class="ul" id="concept_hjn_xx4_kmb__ul_ecx_wdp_kmb">
                                    <li class="li">Some orchestration stages and properties have been renamed.
                                        These changes do not affect existing pipelines. This
                                        includes, but is not limited to, the following: <ul class="ul" id="concept_hjn_xx4_kmb__ul_tbx_ydp_kmb">
                                            <li class="li">The Start Job origin and processor are now the <a class="xref" href="../Origins/StartJob.html#concept_ufc_53w_wlb">Start Jobs origin</a> and <a class="xref" href="../Processors/StartJob-P.html#concept_irv_l5r_2jb">processor</a>.</li>

                                            <li class="li">The Start Pipeline origin and processor are now the
                                                  <a class="xref" href="../Origins/StartPipe.html#concept_h1l_xpr_2jb">Start Pipelines origin</a> and <a class="xref" href="../Processors/StartPipe-P.html#concept_bbc_cxr_2jb">processor</a>.</li>

                                            <li class="li">The Wait for Job Completion processor is now the
                                                  <a class="xref" href="../Processors/WaitJob.html#concept_xv5_xbd_zlb">Wait for Jobs processor</a>.</li>

                                            <li class="li">The Wait for Pipeline Completion processor is now
                                                the <a class="xref" href="../Processors/WaitPipe.html#concept_kd3_qld_zlb">Wait for Pipelines processor</a>. </li>

                                        </ul>
</li>

                                    <li class="li">Records generated by the Start Jobs and Start Pipelines
                                        stages, and updated by the Wait for Jobs and Wait for
                                        Pipelines stages, include pipeline and stage metrics when
                                        available. This includes input record, output record, error
                                        record, and error message counts.</li>

                                </ul>
</li>

                            <li class="li">Scripting origins - You can <a class="xref" href="../Pipeline_Maintenance/PipelineMaintenance_title.html#task_hdg_j1s_5q" title="You can reset the origin when you want the Data Collector to process all available data instead of processing data from the last-saved offset. Reset the origin when the pipeline is not running.">reset the origin</a> for pipelines that include the Groovy
                                Scripting, JavaScript Scripting, or Jython Scripting origin. </li>

                            <li class="li">SFTP/FTP/FTPS origin - The origin generates an error when it
                                encounters a file that it does not have permission to read instead
                                of stopping the pipeline.</li>

                            <li class="li">TensorFlow Evaluator processor:<ul class="ul" id="concept_hjn_xx4_kmb__ul_lhb_k2p_kmb">
                                    <li class="li">The processor uses the <a class="xref" href="../Processors/TensorFlow.html#concept_qlq_jlh_bfb">1.15 TensorFlow client library and supports all 1.x
                                            TensorFlow versions</a>.</li>

                                    <li class="li">In the <a class="xref" href="../Processors/TensorFlow.html#task_fr5_gsh_z2b">Fields to Convert property</a> for each input
                                        configuration, you can configure a field type expression
                                        that defines a set of fields.</li>

                                </ul>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Pipeline Enhancements</dt>

                    <dd class="dd">This release includes the following pipeline enhancements:<ul class="ul" id="concept_hjn_xx4_kmb__ul_q4y_2fp_kmb">
                            <li class="li">Pipeline run history - The pipeline run history displays the input,
                                output, and error record count for each pipeline run. </li>

                            <li class="li">Pipeline run summary - Information about the most recent pipeline
                                run remains available on the Summary tab of the pipeline after the
                                pipeline stops. The summary includes run details such as the start
                                time and duration.</li>

                            <li class="li"><a class="xref" href="../Pipeline_Configuration/EventGeneration.html#concept_cv3_nqt_51b">Pipeline start and stop events</a> - The event records
                                generated when a pipeline starts and stops include fields for the
                                related Control Hub job ID and job name. </li>

                            <li class="li">Stage library panel display and <a class="xref" href="../Installation/AddtionalStageLibs.html#concept_fb2_qmn_bz">stage installation</a>:<ul class="ul" id="concept_hjn_xx4_kmb__ul_bdr_jfp_kmb">
                                    <li class="li">The stage library panel displays all Data Collector stages,
                                        instead of only the installed stages. Stages that are not
                                        installed appear disabled, or greyed out.</li>

                                    <li class="li">When you click on a disabled stage, you can install the
                                        stage library that includes the stage. </li>

                                </ul>
</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Security Enhancements</dt>

                    <dd class="dd">This release includes the following security-related enhancements:</dd>

                    <dd class="dd ddexpand">
                        <ul class="ul" id="concept_hjn_xx4_kmb__ul_r2r_nfp_kmb">
                            <li class="li"><a class="xref" href="../Configuration/Authentication.html#task_lp4_d3p_3mb" title="When Data Collector is configured for file-based authentication, you can use the Data Collector UI to change your password.">File-based user authentication</a> - You can use the Data
                                Collector UI to change your password when Data Collector is
                                configured for file-based authentication.</li>

                            <li class="li">Hashicorp Vault credential store - You can enable the use of a
                                namespace in Hashicorp Vault by configuring a namespace path for the
                                credentialStore.vault.config.namespace property in the
                                    <code class="ph codeph">$SDC_CONF/credential-stores.properties</code> file.
                                    <p class="p">For example,
                                        <code class="ph codeph">credentialStore.vault.config.namespace=nspace1/nspace2/</code>.</p>
</li>

                            <li class="li">Runtime:resourcesDirPath() function - Returns the full path to the
                                directory for runtime resource files.</li>

                            <li class="li"><a class="xref" href="../Pipeline_Configuration/SSL-TLS.html#concept_q2c_hj2_1nb" title="You can configure stages to load the contents of the keystore or truststore from a remote credential store or from values entered in the stage properties. The stage builds the keystore or truststore from the private key and certificates retrieved from the credential store or entered in the stage properties.">SSL/TLS enhancement</a> - Stages that use SSL/TLS can load
                                the contents of the keystore and truststore from a remote credential
                                store.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Additional Enhancement</dt>

                    <dd class="dd">This release includes the following additional enhancement:<ul class="ul" id="concept_hjn_xx4_kmb__ul_a3c_sgp_kmb">
                            <li class="li"><a class="xref" href="../Configuration/DCConfig.html#task_lxk_kjw_1r" title="You can customize Data Collector by editing the Data Collector configuration file, sdc.properties.">Data Collector production batch size</a> - The default value
                                for the production.maxBatchSize property in the Data Collector
                                configuration file has increased to 50,000 records. This change does
                                not affect existing pipelines. </li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Deprecated Feature</dt>

                    <dd class="dd">This release includes the following deprecated feature:<ul class="ul" id="concept_hjn_xx4_kmb__ul_ims_yy3_lmb">
                            <li class="li"><a class="xref" href="../Processors/DatabricksML.html#concept_nlz_k3v_gfb">Databricks ML Evaluator processor</a> - This processor is
                                deprecated and will be removed in a future release. Do not use the
                                processor in new pipelines.</li>

                        </ul>
</dd>

                
            </dl>
</div>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title5" id="concept_tky_1dt_qlb">
    <h2 class="title topictitle2" id="ariaid-title5">What's New in 3.16.x</h2>

    <div class="body conbody">
        <div class="p"><span class="ph">Data Collector</span>
            version 3.16.x includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">New Stages</dt>

                    <dd class="dd">This release includes the following new stages:<ul class="ul" id="concept_tky_1dt_qlb__ul_cgm_bty_qlb">
                            <li class="li"><a class="xref" href="../Executors/SFTP.html#concept_kcf_sn2_jlb">SFTP/FTP/FTPS executor</a> - Use the executor to move or
                                remove a file from an SFTP, FTP, or FTPS server upon receiving an
                                event.</li>

                            <li class="li">New orchestration stages:<ul class="ul" id="concept_tky_1dt_qlb__ul_cwd_2ty_qlb">
                                    <li class="li"><a class="xref" href="../Origins/StartJob.html#concept_ufc_53w_wlb">Start Job origin</a> - Use this origin to start a
                                        Control Hub job. </li>

                                    <li class="li"><a class="xref" href="../Processors/WaitJob.html#concept_xv5_xbd_zlb">Wait for Job Completion processor</a> - Use this
                                        processor to wait for a Control Hub job to complete. </li>

                                    <li class="li"><a class="xref" href="../Processors/WaitPipe.html#concept_kd3_qld_zlb">Wait for Pipeline Completion processor</a> - Use
                                        this processor to wait for a Data Collector, Transformer, or
                                        Edge pipeline to complete. </li>

                                </ul>
</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Stage Enhancements</dt>

                    <dd class="dd">
                        <p dir="ltr" class="p" id="concept_tky_1dt_qlb__docs-internal-guid-5e3e2e68-7fff-0abb-35ed-4e5f63ce4bc0">This release includes the following stage enhancements:</p>

                        <ul class="ul" id="concept_tky_1dt_qlb__ul_q52_kkz_qlb">
                            <li class="li">Amazon stages - You can now configure an Authentication Method
                                property to specify whether to connect with an IAM role, with AWS
                                keys, or without authentication. Previously, you could not connect
                                without authentication to a public bucket.</li>

                            <li class="li">Amazon S3 stages - The Amazon S3 origin, destination, and executor
                                can use a virtual address model to access objects. Previously, the
                                stages used a path address model. </li>

                            <li class="li">HTTP Client stages:<ul class="ul" id="concept_tky_1dt_qlb__ul_ybt_4kz_qlb">
                                    <li class="li">HTTP status codes - Error logging details generated by HTTP
                                        Client stages now include HTTP status codes. </li>

                                    <li class="li">HTTP status header attribute - The HTTP Client stages
                                        include an <code class="ph codeph">HTTP-Status</code> header attribute
                                        that stores the HTTP status for each record. </li>

                                    <li class="li">Timeout defaults - The HTTP Client stages no longer allow 0
                                        for the Connection Timeout or Read Timeout properties. The
                                        default value for Connect Timeout is now 250000
                                        milliseconds. The default value for Read Timeout is 30000
                                        milliseconds. </li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Origins/HTTPClient.html#concept_hqz_pvn_kmb" title="By default, the HTTP Client origin processes only responses that include a 2xx success status code. When the response includes any other status code, such as a 4xx or 5xx status code, the origin generates an error and handles the record based on the error record handling configured for the stage.">HTTP Client origin record generation</a> - You can configure
                                the origin to generate records for all statuses that are not added
                                to the Per-Status Actions list. You can also specify a field to
                                write the error response body for those records.</li>

                            <li class="li">HTTP Server origin authentication enhancements - The origin can
                                create more secure connections using Kerberos authentication. It can
                                also use basic authentication when you enable TLS/SSL.</li>

                            <li class="li"><a class="xref" href="../Origins/MultiTableJDBCConsumer.html#task_kst_m4w_4y">JDBC Multitable Consumer origin header attributes</a> - You
                                can enable the origin to create JDBC header attributes.</li>

                            <li class="li"><a class="xref" href="../Pipeline_Configuration/KafkaSecurity.html#concept_vwr_v2v_klb__stageprops">Kafka stages allow Kerberos credentials</a> - You can
                                specify Kerberos keytabs and principals in Kafka stages, including
                                the Kafka Consumer, Kafka Multitopic Consumer, and Kafka
                                Producer.</li>

                            <li class="li"><a class="xref" href="../Origins/KafkaMultiConsumer.html#task_ost_3n4_x1b">Kafka Multitopic Consumer origin</a> - You can include Kafka
                                timestamps in the record header.</li>

                            <li class="li">Oracle CDC Client origin: <ul class="ul" id="concept_tky_1dt_qlb__ul_hr4_glz_qlb">
                                    <li dir="ltr" class="li"><a class="xref" href="../Origins/OracleCDC.html#concept_rs5_hjj_tw">19c
                                            support</a> - You can use the Oracle CDC Client
                                        origin to read changed data from Oracle 19c in addition to
                                        11g, 12c, and 18c.</li>

                                    <li dir="ltr" class="li">
                                        <p dir="ltr" class="p">Property removal - With this release,
                                            initialization when Dictionary Source is set to Redo
                                            Logs has been improved. As a result, the Duration of
                                            Directory Extraction property is no longer needed and
                                            has been removed.</p>

                                    </li>

                                </ul>
</li>

                            <li class="li">REST Service origin:<ul class="ul" id="concept_tky_1dt_qlb__ul_oxv_t4z_qlb">
                                    <li dir="ltr" class="li">
                                        <p dir="ltr" class="p"><a class="xref" href="../Origins/RESTService.html#concept_xls_51z_cmb" title="You can configure the origin to use Data Collector as an API gateway instead of listening for messages at an HTTP endpoint.">API Gateway</a> - You can configure the origin
                                            to use Data Collector as an API Gateway.</p>

                                    </li>

                                    <li dir="ltr" class="li">
                                        <p dir="ltr" class="p"><a class="xref" href="../Origins/RESTService.html#concept_i1w_dk2_fmb">Authentication</a> - The origin connect securely
                                            using Kerberos authentication. It can also use basic
                                            authentication when you enable TLS/SSL.</p>

                                    </li>

                                    <li dir="ltr" class="li">
                                        <p dir="ltr" class="p">Endpoint URL - The microservice endpoint URL
                                            now displays in monitor mode. </p>

                                    </li>

                                </ul>
</li>

                            <li class="li">Salesforce origin event enhancement - The no-more-data event record
                                generated by the origin now includes a <code class="ph codeph">record-count</code>
                                field that specifies the number of records that were successfully
                                processed. </li>

                            <li class="li">Salesforce Lookup processor:<ul class="ul" id="concept_tky_1dt_qlb__ul_bb2_dqz_qlb">
                                    <li class="li">Multiple results - You can configure the processor to write
                                        multiple return values as a list in a single record instead
                                        of returning only the first value or creating a record for
                                        each value.</li>

                                    <li class="li">Bulk API - You can use the Salesforce Bulk API to look up
                                        records in Salesforce based on a SOQL query. </li>

                                </ul>
</li>

                            <li class="li">SFTP/FTP/FTPS Client origin processing delay - You can configure a
                                File Processing Delay property when you want to allow time for a
                                file to be completely written before processing it. </li>

                            <li class="li">Start Pipeline processor: <ul class="ul" id="concept_tky_1dt_qlb__ul_f2p_fqz_qlb">
                                    <li dir="ltr" class="li"> Pipeline name support - You can configure the
                                        Pipeline ID Type property to enable specifying the pipeline
                                        to start based on the pipeline name instead of the pipeline
                                        ID. </li>

                                    <li dir="ltr" class="li">
                                        <p dir="ltr" class="p">Unique Task Name property - You can specify a
                                            unique name for the processor that is included in the
                                            output record. </p>

                                    </li>

                                </ul>
</li>

                            <li class="li">Start Job processor: <ul class="ul" id="concept_tky_1dt_qlb__ul_q4z_lqz_qlb">
                                    <li class="li">Job name support - You can configure the Job ID Type
                                        property to enable specifying the Control Hub job to start
                                        based on the job name instead of the job ID. </li>

                                    <li class="li">Unique Task Name property - You can specify a unique name
                                        for the processor that is included in the output
                                        record.</li>

                                </ul>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Enterprise Stage Libraries</dt>

                    <dd class="dd"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_s1r_1gg_dhb">Enterprise stage libraries</a> are free for use in both development
                        and production.</dd>

                    <dd class="dd ddexpand">In June 2020, StreamSets released an updated Enterprise stage library for
                        SQL Server 2019 Big Data Cluster.</dd>

                    <dd class="dd ddexpand">
                        <p class="p">For a list of available Enterprise libraries, see <a class="xref" href="../Installation/AddtionalStageLibs.html#concept_s1r_1gg_dhb">Enterprise Stage Libraries</a>. For more information about the new features, fixed issues, and known
                            issues in an Enterprise stage library, see the release notes for the
                            Enterprise stage library, available under <a class="xref" href="https://streamsets.com/documentation-page/#enterprise" target="_blank">Enterprise Libraries</a> on the
                            StreamSets Documentation page.</p>

                    </dd>

                
                
                    <dt class="dt dlterm">Additional Enhancements</dt>

                    <dd class="dd">
                        <p dir="ltr" class="p" id="concept_tky_1dt_qlb__docs-internal-guid-4194fa6e-7fff-64fc-44ed-4ba328ca834c">This release includes the following additional enhancements:</p>

                        <ul class="ul" id="concept_tky_1dt_qlb__ul_tgq_tqz_qlb">
                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Configuration/Authentication.html#task_nsz_lp4_1r">Default users and groups for cloud service provider
                                        installations</a> - <span class="ph">Data Collector</span> installed through a cloud service provider marketplace now
                                    includes only a default <code class="ph codeph">admin</code> user account and
                                    no default groups.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Configuration/CredentialStores.html#concept_g53_vdn_r1b" title="As an additional layer of security, you can employ user groups to further limit access to the secrets defined in credential stores.">Group secrets in credential stores</a> - You can
                                    configure <span class="ph">Data Collector</span> to validate a users group against a comma-separated list of
                                    groups allowed to access each secret.</p>

                            </li>

                            <li class="li"><a class="xref" href="../Configuration/CredentialStores.html#concept_g53_vdn_r1b" title="As an additional layer of security, you can employ user groups to further limit access to the secrets defined in credential stores.">Required group argument in credential functions</a> - If you
                                do not want to use the group argument in credential functions when
                                working with <span class="ph">Data Collector</span> and <span class="ph">Control Hub</span>, you can specify the default group using <code class="ph codeph">all</code> or
                                    <code class="ph codeph">all@&lt;organization ID&gt;</code>. <span class="ph">StreamSets</span> recommends using <code class="ph codeph">all</code> so that you do not need to
                                modify credential functions when migrating pipelines from <span class="ph">Data Collector</span> to <span class="ph">Control Hub</span>. Previously, you had to use <code class="ph codeph">all@&lt;organization
                                    ID&gt;</code>.</li>

                            <li dir="ltr" class="li">
                                <p class="p">MapR 5.x no longer supported - With this release, MapR 5.x stage
                                    libraries are no longer supported and no longer available. </p>

                            </li>

                            <li dir="ltr" class="li">Monitor mode indicator - Monitor mode now displays the
                                following real-time Running icon on the stage that is processing
                                data: <img class="image" id="concept_tky_1dt_qlb__image_ugq_tqz_qlb" height="20" src="https://lh5.googleusercontent.com/4yOP1Hsv4If0FTfgEqqNOiEFC-QlQcL1g9bqqj5BZw6dCmlajHDyGTdU2AbJ7XGISrbVJ76rI4dx04GEz4nqtbB2x_nPJbJ-LKYqK-0SIalrpr8PyJl3qoVL4yQxoA7rztCjAzTj" width="21" /></li>

                        </ul>

                    </dd>

                
            </dl>
</div>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title6" id="concept_cls_sgy_glb">
    <h2 class="title topictitle2" id="ariaid-title6">What's New in 3.15.x</h2>

    
    <div class="body conbody"><p class="shortdesc"></p>

        <div class="p"><span class="ph">Data Collector</span>
            version 3.15.x includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Installation</dt>

                    <dd class="dd">This release includes the following installation enhancements:<ul class="ul" id="concept_cls_sgy_glb__ul_dmw_xgy_glb">
                            <li class="li"><a class="xref" href="../Administration/Administration_title.html#concept_nqf_ls3_glb"><span class="ph">Data Collector</span> downloads require registration</a> - <span class="ph">Data Collector</span> installation packages downloaded from the <span class="ph">StreamSets</span> website now require that you register the <span class="ph">Data Collector</span> instance with <span class="ph">StreamSets</span>. <span class="ph">Data Collector</span> enforces the registration at different times, based on the
                                following installation types:<ul class="ul" id="concept_cls_sgy_glb__ul_pyt_2hy_glb">
                                    <li class="li">A core installation of <span class="ph">Data Collector</span> requires registration when you install an additional
                                        stage library.</li>

                                    <li class="li">A full installation of <span class="ph">Data Collector</span> requires registration during your initial login.</li>

                                </ul>
<p class="p">Registration involves submitting your contact information to
                                    receive a unique activation code by email and then entering the
                                    code in your <span class="ph">Data Collector</span> instance to activate the instance and use all
                                    functionality.</p>
</li>

                            <li class="li">Download access for enterprise customers - Customers with an
                                enterprise account can now download all <span class="ph">Data Collector</span> installation packages from the <a class="xref" href="https://support.streamsets.com/" target="_blank"><u class="ph u">StreamSets Support portal</u></a>.
                                Installation packages downloaded from the Support portal do not
                                require that you register the <span class="ph">Data Collector</span> instance.</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Enterprise Stage Libraries</dt>

                    <dd class="dd"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_s1r_1gg_dhb">Enterprise stage libraries</a> are now free for use in both
                        development and production. You can now use Enterprise stage libraries
                        without purchasing an enterprise license.</dd>

                    <dd class="dd ddexpand">In April 2020, <span class="ph">StreamSets</span> released updated Enterprise stage libraries for Protector, Oracle, and
                        Snowflake. <span class="ph">StreamSets</span> also released a new Enterprise stage library for Databricks. </dd>

                    <dd class="dd ddexpand">For a list of available Enterprise stage libraries, see <a class="xref" href="../Installation/AddtionalStageLibs.html#concept_s1r_1gg_dhb">Enterprise Stage Libraries</a>. For more information about the new
                        features, fixed issues, and known issues in an Enterprise stage library, see
                        the release notes for the Enterprise stage library, available under <a class="xref" href="https://streamsets.com/documentation-page/#enterprise" target="_blank">Enterprise Libraries</a> on the
                        StreamSets Documentation page.</dd>

                
                
                    <dt class="dt dlterm">Data Collector Configuration</dt>

                    <dd class="dd">This release includes the following <span class="ph">Data Collector</span> configuration enhancement:<ul class="ul" id="concept_cls_sgy_glb__ul_kf1_rhy_glb">
                            <li class="li"><a class="xref" href="../Configuration/DCEnvironmentConfig.html#concept_mdc_shg_qr" title="Increase or decrease the Data Collector Java heap size as necessary, based on the resources available on the host machine. By default, the Java heap size is 1024 MB.">Java heap size for cloud service provider installations</a>
                                - <span class="ph">Data Collector</span> installed through a cloud service provider marketplace now has a
                                default Java heap size set to 50% of the available memory on the
                                virtual machine.</li>

                        </ul>
</dd>

                
            </dl>
</div>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title7" id="concept_gcq_bkh_blb">
    <h2 class="title topictitle2" id="ariaid-title7">What's New in 3.14.x</h2>

    
    <div class="body conbody"><p class="shortdesc"></p>

        <div class="p"><span class="ph">Data Collector</span>
            version 3.14.x includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">This release includes enhancements to the following origins:<ul class="ul" id="concept_gcq_bkh_blb__ul_qln_skh_blb">
                            <li dir="ltr" class="li"><a class="xref" href="../Origins/CronScheduler.html#concept_nsz_mnr_2jb">Cron
                                    Scheduler origin</a> - The origin is no longer considered a
                                Technology Preview feature and is approved for use in
                                production.</li>

                            <li dir="ltr" class="li"><a class="xref" href="../Origins/HTTPServer.html#concept_s2p_5hb_4y">HTTP Server origin</a>- You can now configure the origin
                                either to not require an application ID or to accept requests from
                                one of many application IDs. </li>

                            <li dir="ltr" class="li"><a class="xref" href="../Origins/MultiTableJDBCConsumer.html#concept_kbl_z4h_blb">JDBC Multitable Consumer origin</a> - The origin now
                                converts SQL Server columns of the Datetimeoffset data type to the
                                    <span class="ph">Data Collector</span> Zoned_datetime data type.</li>

                            <li dir="ltr" class="li"><a class="xref" href="../Origins/JDBCConsumer.html#concept_xzw_lph_blb">JDBC Query Consumer origin</a> - The origin now converts SQL
                                Server columns of the Datetimeoffset data type to the <span class="ph">Data Collector</span> Zoned_datetime data type.</li>

                            <li dir="ltr" class="li"><a class="xref" href="../Origins/StartPipe.html#concept_h1l_xpr_2jb">Start Pipeline origin</a> - The origin is no longer
                                considered a Technology Preview feature and is approved for use in
                                production.</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Processors</dt>

                    <dd class="dd">This release includes enhancements to the following processors:<ul class="ul" id="concept_gcq_bkh_blb__ul_asx_bqh_blb">
                            <li dir="ltr" class="li"><a class="xref" href="../Processors/ControlHubAPI.html#concept_akz_zsr_2jb">Control Hub API processor</a> - The processor is no longer
                                considered a Technology Preview feature and is approved for use in
                                production.</li>

                            <li dir="ltr" class="li"><a class="xref" href="../Processors/StartJob-P.html#concept_irv_l5r_2jb">Start
                                    Job processor</a> - The processor is no longer considered a
                                Technology Preview feature and is approved for use in
                                production.</li>

                            <li class="li"><a class="xref" href="../Processors/StartPipe-P.html#concept_bbc_cxr_2jb">Start Pipeline processor</a> - The processor is no longer
                                considered a Technology Preview feature and is approved for use in
                                production.</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Executors</dt>

                    <dd class="dd">This release includes enhancements to the following executor:<ul class="ul" id="concept_gcq_bkh_blb__ul_zj4_1rh_blb">
                            <li class="li"><a class="xref" href="../Executors/JDBCQuery.html#concept_i2r_pxr_vkb" title="You define one or more SQL queries that the JDBC Query executor runs on the database each time it receives an event record.">JDBC Query executor</a> - The executor can now run one or
                                more user-defined SQL queries each time it receives an event record.
                                Previously, the executor could only run a single SQL query.</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Data Formats</dt>

                    <dd class="dd">This release includes the following data formats enhancement:<ul class="ul" id="concept_gcq_bkh_blb__ul_l12_5rh_blb">
                            <li dir="ltr" class="li"><a class="xref" href="../Data_Formats/XMLDFormat.html#concept_qls_yfs_vkb" title="You can include the root element in the generated record by enabling the Preserve Root Element property.">XML</a> - When an origin reads XML data, you can now
                                configure the origin to include the root element in the generated
                                records. Previously, origins did not include the root element in the
                                generated records.</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Microservice Pipelines</dt>

                    <dd class="dd">This release includes the following microservice pipeline enhancement:<ul class="ul" id="concept_gcq_bkh_blb__ul_uz2_ksh_blb">
                            <li class="li">Send Response to Origin destination - You can now specify response
                                headers that the destination passes to the REST Service origin in a
                                microservice pipeline. </li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Working with <span class="ph">Control Hub</span></dt>

                    <dd class="dd">This release includes the following enhancement when working with <span class="ph">Control Hub</span>:<ul class="ul" id="concept_gcq_bkh_blb__ul_wfm_qsh_blb">
                            <li class="li">Register <span class="ph">Data Collector</span> with <span class="ph">Control Hub</span> - <span class="ph">Data Collector</span> instances that are deployed behind a reverse proxy using
                                user-defined path-based routes can now successfully be registered
                                with <span class="ph">Control Hub</span>.</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Stage Libraries</dt>

                    <dd class="dd">This release includes the following stage library enhancements:<ul class="ul" id="concept_gcq_bkh_blb__ul_nnf_zxk_1hb">
                            <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">New stage libraries</a> - This release includes the
                                following new stage library:
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="concept_gcq_bkh_blb__table_edq_2yk_1hb" class="table" frame="border" border="1" rules="all"><colgroup><col style="width:50%" /><col style="width:50%" /></colgroup><thead class="thead" style="text-align:left;">
                                            <tr>
                                                <th class="entry cellrowborder" id="d717115e2107">Stage Library Name</th>

                                                <th class="entry cellrowborder" id="d717115e2110">Description</th>

                                            </tr>

                                        </thead>
<tbody class="tbody">
                                            <tr>
                                                <td class="entry cellrowborder" headers="d717115e2107 ">streamsets-datacollector-cdh_kafka_4_1-lib</td>

                                                <td class="entry cellrowborder" headers="d717115e2110 ">For the Cloudera distribution of Apache Kafka
                                                  - CDK 4.1.0 (based on Apache Kafka version
                                                  2.2.1).</td>

                                            </tr>

                                        </tbody>
</table>
</div>
</li>

                        </ul>

                    </dd>

                
            </dl>
</div>

    </div>

<article class="topic concept nested2" aria-labelledby="ariaid-title8" id="concept_bvd_sth_blb">
    <h3 class="title topictitle3" id="ariaid-title8"><span class="ph">Data Collector Edge</span> New Features and Enhancements</h3>

    
    <div class="body conbody"><p class="shortdesc"></p>

        <div class="p">This <span class="ph">Data Collector Edge</span> (<span class="ph">SDC Edge</span>) version includes new features and enhancements in the following areas: <dl class="dl">
                
                    <dt class="dt dlterm">Destinations in Edge Pipelines</dt>

                    <dd class="dd"><span class="ph">Data Collector Edge</span> pipelines now support the <a class="xref" href="../Destinations/InfluxDB.html#concept_inf_db_sr">InfluxDB
                            destination</a>.</dd>

                
            </dl>
</div>

    </div>

</article>
</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title9" id="concept_rs2_mln_hkb">
    <h2 class="title topictitle2" id="ariaid-title9">What's New in 3.13.x</h2>

    
    <div class="body conbody"><p class="shortdesc"></p>

        <p class="p"><span class="ph">Data Collector</span>
            version 3.13.x improves performance when running large numbers of pipelines and changes
            the API for communication between <span class="ph">Data Collector</span> and
                <span class="ph">Control Hub</span>.</p>

        <p class="p">In addition, this version updates the process for generating and uploading support
            bundles. From <span class="ph">Data Collector</span>,
            you now download the support bundle to your machine. You can then upload the support
            bundle from your machine to your <span class="ph">StreamSets</span>
            Support ticket.</p>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title10" id="concept_oms_dtd_qjb">
    <h2 class="title topictitle2" id="ariaid-title10">What's New in 3.12.x</h2>

    
    <div class="body conbody"><p class="shortdesc"></p>

        <div class="p"><span class="ph">Data Collector</span>
            version 3.12.x includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Enterprise Stage Libraries</dt>

                    <dd class="dd">Enterprise stage libraries are free for development purposes only. For
                        information about purchasing an Enterprise stage library for use in
                        production, <a class="xref" href="https://streamsets.com/contact-us/" target="_blank">contact StreamSets</a>.</dd>

                    <dd class="dd ddexpand">In December 2019, StreamSets released a new Enterprise stage library for SQL
                        Server 2019 Big Data Cluster.</dd>

                    <dd class="dd ddexpand">In January 2020, StreamSets released an updated Enterprise stage library for
                        Snowflake. <p class="p">For a list of available Enterprise libraries, see <a class="xref" href="../Installation/AddtionalStageLibs.html#concept_s1r_1gg_dhb">Enterprise Stage Libraries</a>. For more information about the new features, fixed issues, and known
                            issues in an Enterprise stage library, see the release notes for the
                            Enterprise stage library, available under <a class="xref" href="https://streamsets.com/documentation-page/#enterprise" target="_blank">Enterprise Libraries</a> on the
                            StreamSets Documentation page.</p>
</dd>

                
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">This release includes enhancements to the following origins:<ul class="ul" id="concept_oms_dtd_qjb__ul_ubp_ttd_qjb">
                            <li class="li"><a class="xref" href="../Origins/GroovyScripting.html#concept_f3t_hzq_l3b">Groovy Scripting</a>, <a class="xref" href="../Origins/JavaScriptScripting.html#concept_nqx_r3p_p3b">JavaScript Scripting</a>, and <a class="xref" href="../Origins/JythonScripting.html#concept_nwp_g3p_p3b">Jython Scripting</a> - These origins now include two new
                                methods in the <code class="ph codeph">batch</code> object:<ul class="ul" id="concept_oms_dtd_qjb__ul_eqh_v1p_5jb">
                                    <li dir="ltr" class="li"><code class="ph codeph">addError(&lt;record&gt;,&lt;String
                                            message&gt;</code> - Appends an error record to the
                                        batch. The appended error record contains the associated
                                        error message. This method replaces the
                                            <code class="ph codeph">sdc.error.write(&lt;record&gt;, &lt;String
                                            message&gt;)</code> method.</li>

                                    <li class="li"><code class="ph codeph">addEvent(&lt;event record&gt;)</code> - Appends an
                                        event to the batch. This method replaces the
                                            <code class="ph codeph">toEvent(&lt;event record&gt;)</code> method in
                                        the <code class="ph codeph">sdc</code> object.</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Origins/RabbitMQ.html#task_hrz_mq1_h5">RabbitMQ Consumer</a> - The origin now supports transport
                                layer security (TLS) for connections with a RabbitMQ server. You can
                                configure the required properties on the TLS tab.</li>

                            <li class="li"><a class="xref" href="../Origins/Salesforce.html#task_h1n_bs3_rx">Salesforce</a> - The origin has a new property to configure
                                the streaming buffer size when subscribed to notifications.
                                Configure this property to eliminate buffering capacity errors.</li>

                            <li class="li"><a class="xref" href="../Origins/SQLServerCDC.html#task_nsg_fxc_v1b">SQL Server CDC Client</a> - The origin can now be configured
                                to convert unsupported data types into strings and continue
                                processing data. </li>

                            <li class="li"><a class="xref" href="../Origins/SQLServerChange.html#task_vsh_22s_r1b">SQL Server Change Tracking Client</a> - The origin includes
                                the following enhancements: <ul class="ul" id="concept_oms_dtd_qjb__ul_uf4_1c2_qjb">
                                    <li dir="ltr" class="li">The origin now interprets a zero in the Fetch Size
                                        property as an indication to use the database default fetch
                                        size in JDBC statements. </li>

                                    <li dir="ltr" class="li">The origin can now be configured to convert
                                        unsupported data types into strings and continue processing
                                        data.</li>

                                </ul>
</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Destinations</dt>

                    <dd class="dd">This release includes enhancements to the following destinations: <ul class="ul" id="concept_oms_dtd_qjb__ul_ok3_xc2_qjb">
                            <li dir="ltr" class="li"><a class="xref" href="../Destinations/AzureEventHubProducer.html#task_in4_f5q_1bb">Azure Event Hub Producer</a> - The destination can now write
                                records as XML data. </li>

                            <li dir="ltr" class="li"><a class="xref" href="../Destinations/Cassandra.html#task_t1d_z3l_sr">Cassandra</a> - The destination includes the following
                                enhancements: <ul class="ul" id="concept_oms_dtd_qjb__ul_pk3_xc2_qjb">
                                    <li dir="ltr" class="li">The Disable Batch Insert property has been renamed
                                        Enable Batches, and the renamed property is enabled by
                                        default.</li>

                                    <li dir="ltr" class="li">The Request Timeout property has been renamed
                                        Write Timeout. </li>

                                </ul>
</li>

                            <li dir="ltr" class="li"><a class="xref" href="../Destinations/RabbitMQ.html#task_rwy_wn5_2v">RabbitMQ Producer</a> - The destination now supports
                                transport layer security (TLS) for connections with a RabbitMQ
                                server. You can configure the required properties on the TLS tab.
                            </li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Data Formats</dt>

                    <dd class="dd">This release includes the following data formats enhancement:<ul class="ul" id="concept_oms_dtd_qjb__ul_ipw_hd2_qjb">
                            <li class="li">Avro - For schemas located in Confluent Schema Registry, <span class="ph">Data Collector</span> now includes a property for you to specify the user information
                                needed to connect to Schema Registry through basic authentication.
                            </li>

                        </ul>
</dd>

                
            </dl>
</div>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title11" id="concept_rjw_bh2_hjb">
    <h2 class="title topictitle2" id="ariaid-title11">What's New in 3.11.x</h2>

    
    <div class="body conbody"><p class="shortdesc"></p>

        <div class="p"><span class="ph">Data Collector</span>
            version 3.11.x includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">This release includes enhancements to the following origins:<ul class="ul" id="concept_rjw_bh2_hjb__ul_uk2_nh2_hjb">
                            <li dir="ltr" class="li"><a class="xref" href="../Origins/AmazonS3.html#concept_vtn_ty4_jbb">Amazon S3</a> - The origin now generates event records when
                                it starts processing a new object and when it finishes processing an
                                object. </li>

                            <li dir="ltr" class="li"><a class="xref" href="../Origins/ADLS-G1.html#concept_osx_qgz_xhb">Azure Data Lake Storage Gen1</a> - The origin is no longer
                                considered a Technology Preview feature and is approved for use in
                                production.</li>

                            <li dir="ltr" class="li"><a class="xref" href="../Origins/ADLS-G2.html#concept_osx_qgz_xhb">Azure Data Lake Storage Gen2</a> - The origin is no longer
                                considered a Technology Preview feature and is approved for use in
                                production.</li>

                            <li dir="ltr" class="li">Google Big Query - The origin now supports JSON
                                service-account credentials pasted directly into the UI. </li>

                            <li dir="ltr" class="li">Google Cloud Storage - The origin now supports JSON
                                service-account credentials pasted directly into the UI. </li>

                            <li dir="ltr" class="li">Google Pub/Sub Subscriber - The origin now supports JSON
                                service-account credentials pasted directly into the UI. </li>

                            <li dir="ltr" class="li">HTTP Client - The origin now supports <a class="xref" href="../Expression_Language/Functions.html#concept_qkr_trf_sw" title="Use time functions to return the current time or to transform datetime data.">time functions</a> in the Resource URL property. </li>

                            <li dir="ltr" class="li"><a class="xref" href="../Pipeline_Configuration/KMessageKey.html#concept_dyb_s4b_5mb">Kafka Consumer </a>- The origin can now be configured to
                                save the Kafka message key in the record. The origin can save the
                                key in a record header attribute, a record field, or both. </li>

                            <li dir="ltr" class="li"><a class="xref" href="../Pipeline_Configuration/KMessageKey.html#concept_dyb_s4b_5mb">Kafka Multitopic Consumer</a> - The origin can now be
                                configured to save the Kafka message key in the record. The origin
                                can save the key in a record header attribute, a record field, or
                                both. </li>

                            <li dir="ltr" class="li"><a class="xref" href="../Origins/Salesforce.html#task_h1n_bs3_rx">Salesforce</a> - The origin has a new Mismatched Types
                                Behavior property, which specifies how to handle fields with types
                                that do not match the schema. </li>

                            <li class="li"><a class="xref" href="../Origins/SFTP.html#task_lfx_fzd_5v">SFTP/FTP/FTPS Client</a> - The origin has three new timeout
                                properties: Socket Timeout, Connection Timeout, and Data Timeout.
                            </li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Processors</dt>

                    <dd class="dd">This release includes enhancements to the following processors: <ul class="ul" id="concept_rjw_bh2_hjb__ul_jbr_j32_hjb">
                            <li dir="ltr" class="li"><a class="xref" href="../Processors/FieldTypeConverter.html#concept_ixz_s5q_wq" title="The following table lists the data types that can be converted to another data type. List, Map, and List-Map data types cannot be converted.">Field Type Converter</a> - The processor can now convert to
                                the Zoned Datetime data type from the Datetime data type or the Date
                                data type. </li>

                            <li dir="ltr" class="li"><a class="xref" href="../Processors/Groovy.html#concept_hzt_zjt_gv">Groovy Evaluator</a> - The processor now supports the use of
                                the <code class="ph codeph">sdc</code> wrapper object to access the constants,
                                methods, and objects available to each script type. </li>

                            <li dir="ltr" class="li"><a class="xref" href="../Processors/HTTPClient.html#task_z54_1qr_fw" title="Configure an HTTP Client processor to perform requests against a resource URL.">HTTP Client</a> - When responses to requests contain
                                multiple values, the processor can now return the first value, all
                                values in a list in a single record, or all values in separate
                                records. </li>

                            <li dir="ltr" class="li"><a class="xref" href="../Processors/JavaScript.html#concept_mqn_5vk_sr">JavaScript Evaluator</a> - The processor now supports the
                                use of the <code class="ph codeph">sdc</code> wrapper object to access the
                                constants, methods, and objects available to each script type. </li>

                            <li dir="ltr" class="li">
                                <a class="xref" href="../Processors/Jython.html#concept_mqn_5vk_sr">Jython Evaluator</a> - The processor now supports the use of
                                the <code class="ph codeph">sdc</code> wrapper object to access the constants,
                                methods, and objects available to each script type. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Destinations</dt>

                    <dd class="dd">This release includes enhancements to the following destinations: <ul class="ul" id="concept_rjw_bh2_hjb__ul_syx_p32_hjb">
                            <li dir="ltr" class="li"><a class="xref" href="../Destinations/ADLS-G1-D.html#concept_xzc_wfq_xhb">Azure
                                    Data Lake Storage Gen1</a> - The destination is no longer
                                considered a Technology Preview feature and is approved for use in
                                production.</li>

                            <li dir="ltr" class="li"><a class="xref" href="../Destinations/ADLS-G2-D.html#concept_ajp_1d2_vhb">Azure
                                    Data Lake Storage Gen2</a> - The destination is no longer
                                considered a Technology Preview feature and is approved for use in
                                production.</li>

                            <li dir="ltr" class="li"><a class="xref" href="../Destinations/Cassandra.html#task_t1d_z3l_sr">Cassandra</a> - The destination has new properties to
                                disable batches and to set a timeout for individual write requests. </li>

                            <li dir="ltr" class="li">Google Big Query - The destination now supports JSON
                                service-account credentials pasted directly into the UI. </li>

                            <li dir="ltr" class="li">Google Cloud Storage - The destination now supports JSON
                                service-account credentials pasted directly into the UI. </li>

                            <li dir="ltr" class="li">Google Pub/Sub Subscriber - The destination now supports
                                JSON service-account credentials pasted directly into the UI. </li>

                            <li dir="ltr" class="li">HTTP Client - The destination now supports <a class="xref" href="../Expression_Language/Functions.html#concept_qkr_trf_sw" title="Use time functions to return the current time or to transform datetime data.">time functions</a> in the Resource URL property. </li>

                            <li dir="ltr" class="li"><a class="xref" href="../Pipeline_Configuration/KMessageKey.html#concept_kdj_kzf_tmb">Kafka Producer</a> - The destination can now pass Kafka
                                message key values stored in a record header attribute to Kafka. On
                                the Data Format tab, you configure the expected format of the key. </li>

                            <li dir="ltr" class="li"><a class="xref" href="../Destinations/Salesforce.html#concept_rlb_rt3_rx">Salesforce</a> - The destination now writes data to
                                Salesforce objects by matching case-sensitive field names. You can
                                override the default field mappings by continuing to define specific
                                mappings. </li>

                            <li dir="ltr" class="li"><a class="xref" href="../Destinations/SFTP.html#task_jgs_4fw_pgb">SFTP/FTP/FTPS Client</a> - The destination has three new
                                timeout properties: Socket Timeout, Connection Timeout, and Data
                                Timeout. </li>

                            <li class="li"><a class="xref" href="../Destinations/Solr.html#task_ld1_phr_wr">Solr destination</a> - The destination has two new timeout
                                properties: Connection Timeout and Socket Timeout. </li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Executors</dt>

                    <dd class="dd">This release includes enhancements to the following executors: <ul class="ul" id="concept_rjw_bh2_hjb__ul_lf3_5k2_hjb">
                            <li dir="ltr" class="li"><a class="xref" href="../Executors/ADLS-G1-FileMeta.html#concept_zhp_ldk_xhb">ADLS Gen1 File Metadata</a> - The executor is no longer
                                considered a Technology Preview feature and is approved for use in
                                production.</li>

                            <li dir="ltr" class="li"><a class="xref" href="../Executors/ADLS-G2-FileMeta.html#concept_i22_k2k_xhb">ADLS Gen2 File Metadata</a> - The executor is no longer
                                considered a Technology Preview feature and is approved for use in
                                production.</li>

                            <li dir="ltr" class="li"><a class="xref" href="../Executors/JDBCQuery.html#concept_j2c_hpx_gjb">JDBC Query</a> - The executor can now generate events that
                                you can use in an event stream. You can configure the executor to
                                include the number of rows returned or affected by the query when
                                generating events. </li>

                            <li dir="ltr" class="li">Spark - The executor now includes the following:<ul class="ul" id="concept_rjw_bh2_hjb__ul_mf3_5k2_hjb">
                                    <li dir="ltr" class="li">Additional fields in generated event records to
                                        store the user who submitted the job and the time that the
                                        job started. </li>

                                    <li class="li">Additional JARs property for applications written in Python.
                                    </li>

                                </ul>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Technology Preview Functionality</dt>

                    <dd class="dd"><span class="ph">Data Collector</span> includes certain new features and stages with the Technology Preview
                        designation. <a class="xref" href="../Pipeline_Design/TechPreview.html#concept_prl_qfv_gfb" title="Data Collector includes certain new features and stages with the Technology Preview designation. Technology Preview functionality is available for use in development and testing, but is not meant for use in production.">Technology Preview functionality</a> is available for use in
                        development and testing, but is not meant for use in production.<p dir="ltr" class="p">Technology Preview stages include the following image on the stage
                            icon: <img class="image" id="concept_rjw_bh2_hjb__image_k2c_n3b_zhb" src="../Graphics/TechPreviewImage.png" />.</p>
<p dir="ltr" class="p">When Technology Preview
                            functionality becomes approved for use in production, the release notes
                            and documentation reflect the change, and the Technology Preview icon is
                            removed from the UI.</p>
<div class="p" dir="ltr">The following Technology Preview
                            stages are newly available in this release:<ul class="ul" id="concept_rjw_bh2_hjb__ul_gm3_km2_hjb">
                                <li dir="ltr" class="li"><a class="xref" href="../Origins/CronScheduler.html#concept_nsz_mnr_2jb">Cron Scheduler origin</a> - Generates a record with the
                                    current datetime as scheduled by a cron expression.</li>

                                <li dir="ltr" class="li"><a class="xref" href="../Origins/StartPipe.html#concept_h1l_xpr_2jb">Start
                                        Pipeline origin</a> - Starts a Data Collector, Data
                                    Collector Edge, or Transformer pipeline.</li>

                                <li dir="ltr" class="li"><a class="xref" href="../Processors/ControlHubAPI.html#concept_akz_zsr_2jb">Control Hub API processor</a>- Calls a Control Hub API. </li>

                                <li dir="ltr" class="li"><a class="xref" href="../Processors/StartJob-P.html#concept_irv_l5r_2jb">Start Job processor</a> - Starts a Control Hub job.</li>

                                <li class="li"><a class="xref" href="../Processors/StartPipe-P.html#concept_bbc_cxr_2jb">Start Pipeline processor</a> - Starts a Data Collector,
                                    Data Collector Edge, or Transformer pipeline.</li>

                            </ul>
</div>
</dd>

                
                
                    <dt class="dt dlterm">Pipelines</dt>

                    <dd class="dd">
                        <div class="p" dir="ltr">This release includes the following pipeline enhancement:<ul class="ul" id="concept_rjw_bh2_hjb__ul_em2_nq2_hjb">
                                <li class="li">You can now <a class="xref" href="../Pipeline_Configuration/ConfiguringAPipeline.html#task_xlv_jdw_kq" title="Configure a pipeline to define the stream of data. After you configure the pipeline, you can start the pipeline.">configure pipelines</a> to write error records to Amazon
                                    S3.</li>

                            </ul>
</div>

                    </dd>

                
                
                    <dt class="dt dlterm"><span class="ph">Data Collector</span> Configuration</dt>

                    <dd class="dd">This release includes the following <span class="ph">Data Collector</span> configuration enhancement: <ul class="ul" id="concept_rjw_bh2_hjb__ul_jzq_hrb_zhb">
                            <li class="li">The <span class="ph">Data Collector</span>
                                <a class="xref" href="../Configuration/DCConfig.html#task_lxk_kjw_1r" title="You can customize Data Collector by editing the Data Collector configuration file, sdc.properties.">configuration file</a>
                                <span class="ph filepath">sdc.properties</span> contains a new stage-specific
                                property,
                                    <code class="ph codeph">stage.conf_com.streamsets.pipeline.stage.hive.impersonate.current.user</code>.
                                You can set the property to <code class="ph codeph">true</code> to enable the Hive
                                Metadata processor, the Hive Metastore destination, and the Hive
                                Query executor to impersonate the current user when connecting to
                                Hive.</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Stage Libraries</dt>

                    <dd class="dd">This release includes the following stage library enhancements:<ul class="ul" id="concept_rjw_bh2_hjb__ul_nnf_zxk_1hb">
                            <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">New stage libraries</a> - This release includes the
                                following new stage libraries:
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="concept_rjw_bh2_hjb__table_edq_2yk_1hb" class="table" frame="border" border="1" rules="all"><colgroup><col style="width:50%" /><col style="width:50%" /></colgroup><thead class="thead" style="text-align:left;">
                                            <tr>
                                                <th class="entry cellrowborder" id="d717115e2836">Stage Library Name</th>

                                                <th class="entry cellrowborder" id="d717115e2839">Description</th>

                                            </tr>

                                        </thead>
<tbody class="tbody">
                                            <tr>
                                                <td class="entry cellrowborder" headers="d717115e2836 ">streamsets-datacollector-cdh_6_3-lib</td>

                                                <td class="entry cellrowborder" headers="d717115e2839 ">For the Cloudera CDH version 6.3 distribution
                                                  of Apache Hadoop.</td>

                                            </tr>

                                            <tr>
                                                <td class="entry cellrowborder" headers="d717115e2836 ">streamsets-datacollector-orchestrator-lib</td>

                                                <td class="entry cellrowborder" headers="d717115e2839 ">For the orchestration stages. </td>

                                            </tr>

                                        </tbody>
</table>
</div>
</li>

                            <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">Updated stage libraries</a> - This release includes updates
                                to the following stage library: 
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="concept_rjw_bh2_hjb__table_f3x_ycn_bhb" class="table" frame="border" border="1" rules="all"><colgroup><col style="width:50%" /><col style="width:50%" /></colgroup><thead class="thead" style="text-align:left;">
                                            <tr>
                                                <th class="entry cellrowborder" id="d717115e2884">Stage Library Name</th>

                                                <th class="entry cellrowborder" id="d717115e2887">Description</th>

                                            </tr>

                                        </thead>
<tbody class="tbody">
                                            <tr>
                                                <td class="entry cellrowborder" headers="d717115e2884 ">streamsets-datacollector-hdp_3_1-lib</td>

                                                <td class="entry cellrowborder" headers="d717115e2887 ">For Hortonworks 3.1, the library now includes
                                                  two additional stages:<ul class="ul" id="concept_rjw_bh2_hjb__ul_bwx_yr2_hjb">
                                                  <li dir="ltr" class="li">Spark Evaluator processor</li>

                                                  <li dir="ltr" class="li">Spark executor</li>

                                                  </ul>
</td>

                                            </tr>

                                        </tbody>
</table>
</div>
</li>

                        </ul>

                    </dd>

                
            </dl>
</div>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title12" id="concept_bfw_mrk_43b">
    <h2 class="title topictitle2" id="ariaid-title12">What's New in 3.10.x</h2>

    
    <div class="body conbody"><p class="shortdesc"></p>

        <div class="p"><span class="ph">Data Collector</span>
            version 3.10.x includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Enterprise Stage Libraries</dt>

                    <dd class="dd">Enterprise stage libraries are free for development purposes only. For
                        information about purchasing an Enterprise stage library for use in
                        production, <a class="xref" href="https://streamsets.com/contact-us/" target="_blank">contact StreamSets</a>.</dd>

                    <dd class="dd ddexpand">
                        <p class="p">On August 15, 2019, StreamSets released new and updated Enterprise stage
                            libraries. For a list of available Enterprise libraries, see <a class="xref" href="../Installation/AddtionalStageLibs.html#concept_s1r_1gg_dhb">Enterprise Stage Libraries</a>. For more information about the new features, fixed issues, and known
                            issues in an Enterprise stage library, see the release notes for the
                            Enterprise stage library, available under <a class="xref" href="https://streamsets.com/documentation-page/#enterprise" target="_blank">Enterprise Libraries
                                Documentation</a> on the StreamSets Documentation page.</p>

                    </dd>

                
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">This release includes the following new origins:<ul class="ul" id="concept_bfw_mrk_43b__ul_gbk_wrk_43b">
                            <li class="li"><a class="xref" href="../Origins/GroovyScripting.html#concept_chr_zjj_l3b">Groovy Scripting</a> - Runs a Groovy script to create <span class="ph">Data Collector</span> records.</li>

                            <li class="li"><a class="xref" href="../Origins/JavaScriptScripting.html#concept_kn5_bvt_m3b">JavaScript Scripting</a> - Runs a JavaScript script to
                                create <span class="ph">Data Collector</span> records.</li>

                            <li class="li"><a class="xref" href="../Origins/JythonScripting.html#concept_fxz_35t_m3b">Jython Scripting</a> - Runs a Jython script to create <span class="ph">Data Collector</span> records.</li>

                            <li class="li">NiFi HTTP Server - Listens for requests from a NiFi PutHTTP
                                processor and processes NiFi FlowFiles.</li>

                        </ul>
</dd>

                    <dd class="dd ddexpand">This release includes enhancements to the following origins:<ul class="ul" id="concept_bfw_mrk_43b__ul_yng_gsk_43b">
                            <li class="li">SQL Server CDC Client - The origin now has two new <a class="xref" href="../Origins/SQLServerCDC.html#concept_pc4_xts_r1b">record header attributes</a>:<ul class="ul" id="concept_bfw_mrk_43b__ul_uk4_lsk_43b">
                                    <li dir="ltr" class="li"><code class="ph codeph">jdbc.cdc.source_schema_name</code> -
                                        Stores the source schema.</li>

                                    <li class="li"><code class="ph codeph">jdbc.cdc.source_name</code> - Stores the source
                                        table.</li>

                                </ul>
<p class="p">Also the origin no longer requires you to install a <a class="xref" href="../Origins/SQLServerCDC.html#concept_wt3_12p_53b">JDBC driver</a>. <span class="ph">Data Collector</span> now includes the Microsoft SQL Server JDBC driver. </p>
</li>

                            <li class="li"><a class="xref" href="../Origins/SQLServerChange.html#concept_wcb_hkp_53b">SQL Server Change Tracking </a>- The origin no longer
                                requires you to install a JDBC driver. <span class="ph">Data Collector</span> now includes the Microsoft SQL Server JDBC driver.</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Processors</dt>

                    <dd class="dd">This release includes enhancements to the following processors:<ul class="ul" id="concept_bfw_mrk_43b__ul_cyf_vdb_zhb">
                            <li class="li"><a class="xref" href="../Processors/Groovy.html#task_asl_bpt_gv">Groovy Evaluator</a>, <a class="xref" href="../Processors/JavaScript.html#task_mzc_1by_nr">JavaScript Evaluator</a>, and <a class="xref" href="../Processors/Jython.html#task_fty_jwx_nr">Jython Evaluator</a> - These scripting processors now
                                support the following: <ul class="ul" id="concept_bfw_mrk_43b__ul_d4p_ktk_43b">
                                    <li class="li">User-defined parameters - On the Advanced tab, enter
                                        parameters and values. In the script, access the value with
                                        the <code class="ph codeph">sdc.userParams</code> dictionary. </li>

                                    <li class="li">Full-screen script editing - With your cursor in the script
                                        field, press either F11 or Esc, depending on your operating
                                        system, to toggle full-screen editing. </li>

                                </ul>
</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Destinations</dt>

                    <dd class="dd">This release includes enhancements to the following destinations:<ul class="ul" id="concept_bfw_mrk_43b__ul_bkn_ztk_43b">
                            <li class="li">Cassandra - The destination has four new properties to help you
                                debug issues with the destination: Connection Timeout, Read Timeout,
                                Consistency Level, and Log Slow Queries. </li>

                            <li class="li">RabbitMQ Producer - The destination has a new Set Expiration
                                property, available when setting AMQP message properties on the
                                RabbitMQ tab. Clear the Set Expiration property to disable
                                expiration on messages that the destination sends.</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Executors</dt>

                    <dd class="dd">This release includes enhancements to the following executor:<ul class="ul" id="concept_bfw_mrk_43b__ul_pbr_4gb_zhb">
                            <li class="li"><a class="xref" href="../Executors/JDBCQuery.html#task_ym2_3cv_sx">JDBC Query</a> - The executor can now run queries in
                                parallel to improve throughput. On the Advanced tab, select the
                                Enable Parallel Queries property to have the executor run queries
                                simultaneously on each connection to the database.</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Data Formats</dt>

                    <dd class="dd">This release includes enhancements to the following data formats:<ul class="ul" id="concept_bfw_mrk_43b__ul_dlz_45k_43b">
                            <li class="li">Delimited data format - You can now specify when <span class="ph">Data Collector</span> inserts quotes in generated delimited data. The Data Generator
                                processor and destinations that write delimited data include a new
                                Quote Mode property on the Data Format tab when you select a custom
                                delimiter format for delimited data. Configure the Quote Mode
                                property to generate data that quotes all fields, only fields that
                                contain special characters, or no fields.</li>

                            <li class="li">Excel data format - In origins that read the Excel data format, you
                                can now configure the origin to read either from all sheets in a
                                workbook or from particular sheets in a workbook. Also, you can
                                configure the origin to skip cells that do not have a corresponding
                                header value.</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Data Governance Tools</dt>

                    <dd class="dd">This release includes the following data governance tool enhancement:</dd>

                    <dd class="dd ddexpand">
                        <ul class="ul" id="concept_bfw_mrk_43b__ul_psr_qx5_b3b">
                            <li class="li"><a class="xref" href="../Configuration/PublishMetadata.html#concept_q4y_4mt_p1b" title="You can configure Data Collector to publish metadata about running pipelines to Cloudera Navigator. You then use Cloudera Navigator to explore the pipeline metadata, including viewing lineage diagrams of the metadata.">Cloudera Navigator versions</a> - <span class="ph">Data Collector</span> can now publish metadata to Cloudera Navigator running on
                                Cloudera Manager version 6.1.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Expression Language</dt>

                    <dd class="dd">This release includes the following new <a class="xref" href="../Expression_Language/Functions.html#concept_gfs_w55_3cb" title="You can use field functions in field path expressions that determine the set of fields that a processor uses. Each function is evaluated against a set of matching fields individually.">field functions</a>:<ul class="ul" id="concept_bfw_mrk_43b__ul_v3f_cvk_43b">
                            <li class="li"><code class="ph codeph">f:index()</code> - Returns the index within the parent
                                list field. Returns -1 if the field is not in a list.</li>

                            <li class="li"><code class="ph codeph">f:parent()</code> - Returns the parent field.</li>

                            <li class="li"><code class="ph codeph">f:parentPath() </code>- Returns the path of the parent
                                field.</li>

                            <li class="li"><code class="ph codeph">f:getSiblingWithName(&lt;name&gt;)</code> - Returns the
                                sibling field with the name matching <code class="ph codeph">&lt;name&gt;</code>, if
                                the field exists.</li>

                            <li class="li"><code class="ph codeph">f:hasSiblingWithName(&lt;name&gt;)</code> - Returns
                                    <code class="ph codeph">true</code> if there is a sibling field with a name
                                matching <code class="ph codeph">&lt;name&gt;</code>.</li>

                            <li class="li"><code class="ph codeph">f:hasSiblingWithValue(&lt;name&gt;, &lt;value&gt;)</code> -
                                Returns <code class="ph codeph">true</code> if there is a sibling field with a
                                name matching <code class="ph codeph">&lt;name&gt;</code> that has value matching
                                    <code class="ph codeph">&lt;value&gt;</code>.</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm"><span class="ph">Data Collector</span> Configuration</dt>

                    <dd class="dd">This release includes the following <span class="ph">Data Collector</span> configuration enhancement: <ul class="ul" id="concept_bfw_mrk_43b__ul_jzq_hrb_zhb">
                            <li class="li">The <span class="ph">Data Collector</span>
                                <a class="xref" href="../Configuration/DCConfig.html#task_lxk_kjw_1r" title="You can customize Data Collector by editing the Data Collector configuration file, sdc.properties.">configuration file</a>
                                <span class="ph filepath">sdc.properties</span> contains a new stage-specific
                                property,
                                    <code class="ph codeph">stage.conf_com.streamsets.pipeline.stage.jdbc.drivers.load</code>,
                                where you can list JDBC drivers that <span class="ph">Data Collector</span> automatically loads for all pipelines.</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Stage Libraries</dt>

                    <dd class="dd">This release includes the following stage library enhancements:<ul class="ul" id="concept_bfw_mrk_43b__ul_nnf_zxk_1hb">
                            <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">New stage libraries</a> - This release includes the
                                following new stage libraries:
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="concept_bfw_mrk_43b__table_edq_2yk_1hb" class="table" frame="border" border="1" rules="all"><colgroup><col style="width:50%" /><col style="width:50%" /></colgroup><thead class="thead" style="text-align:left;">
                                            <tr>
                                                <th class="entry cellrowborder" id="d717115e3314">Stage Library Name</th>

                                                <th class="entry cellrowborder" id="d717115e3317">Description</th>

                                            </tr>

                                        </thead>
<tbody class="tbody">
                                            <tr>
                                                <td class="entry cellrowborder" headers="d717115e3314 ">streamsets-datacollector-cdh_6_1-lib</td>

                                                <td class="entry cellrowborder" headers="d717115e3317 ">For the Cloudera CDH version 6.1 distribution
                                                  of Apache Hadoop.</td>

                                            </tr>

                                            <tr>
                                                <td class="entry cellrowborder" headers="d717115e3314 ">streamsets-datacollector-cdh_6_2-lib</td>

                                                <td class="entry cellrowborder" headers="d717115e3317 ">For the Cloudera CDH version 6.2 distribution
                                                  of Apache Hadoop. </td>

                                            </tr>

                                            <tr>
                                                <td class="entry cellrowborder" headers="d717115e3314 ">streamsets-datacollector-cdh_spark_2_3_r3-lib</td>

                                                <td class="entry cellrowborder" headers="d717115e3317 ">For the Cloudera CDH cluster Kafka with CDS
                                                  powered by Spark 2.3 release 3. </td>

                                            </tr>

                                            <tr>
                                                <td class="entry cellrowborder" headers="d717115e3314 ">streamsets-datacollector-cdh_spark_2_3_r4-lib</td>

                                                <td class="entry cellrowborder" headers="d717115e3317 ">For the Cloudera CDH cluster Kafka with CDS
                                                  powered by Spark 2.3 release 4.</td>

                                            </tr>

                                        </tbody>
</table>
</div>
</li>

                            <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_fw3_zt3_tbb">Legacy stage libraries</a> - The following stage libraries
                                are now legacy stage libraries: 
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="concept_bfw_mrk_43b__table_f3x_ycn_bhb" class="table" frame="border" border="1" rules="all"><colgroup><col style="width:50%" /><col style="width:50%" /></colgroup><thead class="thead" style="text-align:left;">
                                            <tr>
                                                <th class="entry cellrowborder" id="d717115e3380">Stage Library Name</th>

                                                <th class="entry cellrowborder" id="d717115e3383">Description</th>

                                            </tr>

                                        </thead>
<tbody class="tbody">
                                            <tr>
                                                <td class="entry cellrowborder" headers="d717115e3380 ">streamsets-datacollector-hdp_2_6-lib</td>

                                                <td class="entry cellrowborder" headers="d717115e3383 ">For the Hortonworks version 2.6.x
                                                  distribution of Apache Hadoop.</td>

                                            </tr>

                                            <tr>
                                                <td class="entry cellrowborder" headers="d717115e3380 ">streamsets-datacollector-hdp_2_6-flume-lib</td>

                                                <td class="entry cellrowborder" headers="d717115e3383 ">For the Hortonworks version 2.6.x
                                                  distribution of Apache Flume.</td>

                                            </tr>

                                            <tr>
                                                <td class="entry cellrowborder" headers="d717115e3380 ">streamsets-datacollector-hdp_2_6-hive2-lib</td>

                                                <td class="entry cellrowborder" headers="d717115e3383 ">For the Hortonworks version 2.6.x
                                                  distribution of Apache Hive version 2.1.</td>

                                            </tr>

                                            <tr>
                                                <td class="entry cellrowborder" headers="d717115e3380 ">streamsets-datacollector-hdp_2_6_1-hive1-lib</td>

                                                <td class="entry cellrowborder" headers="d717115e3383 ">For the Hortonworks version 2.6.1
                                                  distribution of Apache Hive version 1.x.</td>

                                            </tr>

                                            <tr>
                                                <td class="entry cellrowborder" headers="d717115e3380 ">streamsets-datacollector-hdp_2_6_2-hive1-lib</td>

                                                <td class="entry cellrowborder" headers="d717115e3383 ">For the Hortonworks version 2.6.2
                                                  distribution of Apache Hive version 1.x.</td>

                                            </tr>

                                        </tbody>
</table>
</div>
<p class="p">Legacy stage libraries that are more than two years old
                                    are not included with <span class="ph">Data Collector</span>. Though not recommended, you can still install the older
                                    stage libraries.</p>
<p class="p">If you have pipelines that use these
                                    legacy stage libraries, you will need to update the pipelines to
                                    use a more current stage library or install the legacy stage
                                    library. For more information see <a class="xref" href="../Upgrade/PostUpgrade.html#concept_wnl_zk4_5bb">Update Pipelines using Legacy Stage
                                Libraries</a>.</p>
</li>

                        </ul>

                    </dd>

                
            </dl>
</div>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title13" id="concept_sjn_dcb_zhb">
    <h2 class="title topictitle2" id="ariaid-title13">What's New in 3.9.x</h2>

    
    <div class="body conbody"><p class="shortdesc"></p>

        <div class="p"><span class="ph">Data Collector</span>
            version 3.9.x includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">This release includes enhancements to the following origins:<ul class="ul" id="concept_sjn_dcb_zhb__ul_msy_rcb_zhb">
                            <li dir="ltr" class="li">
                                <div class="p" dir="ltr"><a class="xref" href="../Origins/HDFSStandalone.html#task_l3t_sdm_hdb">Hadoop FS Standalone</a> and <a class="xref" href="../Origins/MapRFSStandalone.html#task_tpv_kqc_mdb">MapR FS Standalone</a> - These origins include the
                                    following tab and property name changes:<ul class="ul" id="concept_sjn_dcb_zhb__ul_k3v_tcb_zhb">
                                        <li dir="ltr" class="li">The Hadoop FS tab is now the Connection
                                            tab.</li>

                                        <li dir="ltr" class="li">The Hadoop FS URI property is now the File
                                            System URI property.</li>

                                        <li dir="ltr" class="li">The HDFS User property is now the
                                            Impersonation User property.</li>

                                        <li dir="ltr" class="li">The Hadoop FS Configuration Directory property
                                            is now the Configuration Files Directory property.</li>

                                        <li dir="ltr" class="li">The Hadoop FS Configuration property is now
                                            the Additional Configuration property.</li>

                                    </ul>
</div>

                                <p dir="ltr" class="p">The functionality associated with these properties has
                                    not changed.</p>

                            </li>

                            <li class="li"><a class="xref" href="../Origins/MultiTableJDBCConsumer.html#concept_c54_hj1_q1b">JDBC Multitable Consumer</a> - The origin now supports using
                                multithreaded partition processing when the primary key or
                                user-defined offset column is an Oracle Timestamp with time zone
                                data type and each row has the same time zone.</li>

                            <li class="li">JMS Consumer - The origin now supports reading messages from durable
                                topic subscriptions, which can have only one active subscriber at a
                                time.</li>

                            <li class="li"><a class="xref" href="../Origins/SFTP.html#concept_ic5_bzd_5v">SFTP/FTP/FTPS
                                    Client</a> - The origin, formerly called SFTP/FTP Client, now
                                supports FTPS (FTP over SSL). Post processing is now disabled after
                                selecting the Whole File data format, which does not support post
                                processing.</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Processors</dt>

                    <dd class="dd">This release includes the following new processor:<ul class="ul" id="concept_sjn_dcb_zhb__ul_msf_sdb_zhb">
                            <li class="li"><a class="xref" href="../Processors/CouchbaseLookup.html#concept_rxk_1dq_2fb">Couchbase Lookup</a> - Looks up documents in Couchbase
                                Server to enrich records with data.</li>

                        </ul>
</dd>

                    <dd class="dd ddexpand">This release includes enhancements to the following processors:<ul class="ul" id="concept_sjn_dcb_zhb__ul_cyf_vdb_zhb">
                            <li class="li"><a class="xref" href="../Processors/Groovy.html#concept_osl_brr_j3b">Groovy Evaluator</a>, <a class="xref" href="../Processors/JavaScript.html#concept_n23_qnr_j3b">JavaScript Evaluator</a>, and <a class="xref" href="../Processors/Jython.html#concept_yh5_y2l_j3b">Jython Evaluator</a> - These scripting processors now
                                support direct use of <span class="ph">Data Collector</span> records after you set the new Record Type property on the
                                Advanced tab to <span class="ph">Data Collector</span> Records. </li>

                            <li class="li">Hive Metadata - The processor can now process datetime fields in
                                their native format or can convert the fields to string before
                                processing the data. By default, the processor processes datetime
                                fields in their native format. Previously, the processor always
                                converted datetime fields to string.</li>

                            <li class="li">Log Parser - The processor now has a new Data Format tab that
                                contains properties related to format. These include new properties
                                that configure the maximum line length, the character set, and the
                                retention of the original line from the log. For the Grok Pattern
                                format, the processor now supports entry of multiple grok patterns.
                                For the Log4j format, the processor now has properties to configure
                                the action taken on parse error and the size of the stack trace that
                                can be included in a record for a log.</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Destinations</dt>

                    <dd class="dd">This release includes the following new destination:<ul class="ul" id="concept_sjn_dcb_zhb__ul_d2k_t2b_zhb">
                            <li class="li"><a class="xref" href="../Destinations/SFTP.html#concept_sgt_m2m_xhb">SFTP/FTP/FTPS Client</a> - Writes whole files to a URL using
                                SFTP, FTP, or FTPS.</li>

                        </ul>
</dd>

                    <dd class="dd ddexpand">This release includes enhancements to the following destinations:<ul class="ul" id="concept_sjn_dcb_zhb__ul_drn_v2b_zhb">
                            <li class="li">Aerospike - The destination can now use CRUD operations defined in
                                the sdc.operation.type record header attribute to upsert or delete
                                data. You can define a default operation for records without the
                                header attribute or value. You can also configure how to handle
                                records with unsupported operations.</li>

                            <li class="li"><a class="xref" href="../Destinations/DataLakeStore.html#concept_jzm_kf4_zx">Azure Data Lake Storage (Legacy)</a> - The destination,
                                formerly called Azure Data Lake Storage, has been renamed. <span class="ph">Data Collector</span> now includes the <a class="xref" href="../Destinations/ADLS-G1-D.html#concept_xzc_wfq_xhb">Azure
                                    Data Lake Storage Gen1 destination</a> that also writes data
                                to Microsoft Azure Data Lake Storage Gen1. The Azure Data Lake
                                Storage Gen1 destination is a <a class="xref" href="../Pipeline_Design/TechPreview.html#concept_prl_qfv_gfb" title="Data Collector includes certain new features and stages with the Technology Preview designation. Technology Preview functionality is available for use in development and testing, but is not meant for use in production.">Technology Preview stage</a>. </li>

                            <li class="li"><a class="xref" href="../Destinations/Couchbase.html#concept_ahq_1wq_h2b">Couchbase</a> - The destination includes the following
                                    enhancements:<ul class="ul" id="concept_sjn_dcb_zhb__ul_cqx_z2b_zhb">
                                    <li dir="ltr" class="li">
                                        <p dir="ltr" class="p">Supports CRUD operations defined in the
                                                <code class="ph codeph">sdc.operation.type</code> record header
                                            attribute to write data. You can define a default
                                            operation for records without the header attribute or
                                            value. You can also configure how to handle records with
                                            unsupported operations.</p>

                                    </li>

                                    <li dir="ltr" class="li">
                                        <p dir="ltr" class="p">Supports writing to sub-documents.</p>

                                    </li>

                                    <li dir="ltr" class="li">
                                        <p dir="ltr" class="p">Supports writing data using the Avro, Binary,
                                            Delimited, JSON, Protobuf, SDC Record, and Text data
                                            formats.</p>

                                    </li>

                                </ul>
</li>

                            <li dir="ltr" class="li"><a class="xref" href="../Destinations/HadoopFS-destination.html#task_m2m_skm_zq">Hadoop FS</a> and <a class="xref" href="../Destinations/MapRFS.html#task_spl_1fj_fv">MapR FS</a> - These destinations include the following tab
                                and property name changes:<ul class="ul" id="concept_sjn_dcb_zhb__ul_tgs_3fb_zhb">
                                    <li dir="ltr" class="li">The Hadoop FS tab is now the Connection tab.</li>

                                    <li dir="ltr" class="li">The Hadoop FS URI property is now the File System
                                        URI property.</li>

                                    <li dir="ltr" class="li">The HDFS User property is now the Impersonation
                                        User property.</li>

                                    <li dir="ltr" class="li">The Hadoop FS Configuration Directory property is
                                        now the Configuration Files Directory property.</li>

                                    <li dir="ltr" class="li">The Hadoop FS Configuration property is now the
                                        Additional Configuration property.</li>

                                </ul>
<p dir="ltr" class="p">The functionality associated with these properties
                                    has not changed.</p>
</li>

                            <li class="li"><a class="xref" href="../Destinations/HBase.html#task_pyq_qx5_vr">HBase</a> - The destination can now skip validating that a
                                table exists in HBase before writing to the table. By default, the
                                destination validates that a table exists, which requires that the
                                HBase user that writes to HBase has HBase administrator
                                    rights.<p class="p">You might want to configure the destination to skip
                                    the validation when you do not want to grant HBase administrator
                                    rights to the HBase user. If you configure the destination to
                                    skip validation and a table does not exist, then the pipeline
                                    encounters an error. Previously, the destination always
                                    validated that a table existed.</p>
</li>

                            <li class="li"><a class="xref" href="../Destinations/Solr.html#task_ld1_phr_wr">Solr</a> - The following destination properties are now
                                enabled by default:<ul class="ul" id="concept_sjn_dcb_zhb__ul_oft_vfb_zhb">
                                    <li dir="ltr" class="li">
                                        <p dir="ltr" class="p">Map Fields Automatically</p>

                                    </li>

                                    <li dir="ltr" class="li">
                                        <p dir="ltr" class="p">Ignore Optional Fields</p>

                                    </li>

                                </ul>
<p class="p">Previously, both properties were disabled by default.
                                </p>
</li>

                            <li dir="ltr" class="li"><a class="xref" href="../Destinations/Syslog.html#concept_dcb_4v3_gfb" title="A syslog message includes fields such as a timestamp, facility code, severity level, message ID, and the log message itself. You construct the syslog message content by specifying the values for message fields on the Message tab. The content of the log message depends on how you configure the Data Format tab.">Syslog</a> - The following destination properties have been
                                removed from the Message tab:<ul class="ul" id="concept_sjn_dcb_zhb__ul_yqx_yfb_zhb">
                                    <li dir="ltr" class="li">
                                        <p dir="ltr" class="p">Use Non-Text Message Format</p>

                                    </li>

                                    <li dir="ltr" class="li">
                                        <p dir="ltr" class="p">Message Text</p>

                                    </li>

                                </ul>
<p class="p">You now configure the destination to use the text data
                                    format on the Data Format tab. If you upgrade pipelines that
                                    used the Syslog destination configured to use the text data
                                    format, you must complete the post upgrade task described in
                                        <a class="xref" href="../Upgrade/PostUpgrade.html#concept_hkf_ylq_xhb">Update Syslog Pipelines</a>.</p>
</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Executors</dt>

                    <dd class="dd">This release includes the following enhancements to executors:<ul class="ul" id="concept_sjn_dcb_zhb__ul_pbr_4gb_zhb">
                            <li class="li"><a class="xref" href="../Executors/PipelineFinisher.html#concept_vnr_3sg_rhb">Pipeline Finisher</a> - The executor includes a new Reset
                                Offset option that ensures that the pipeline processes all available
                                data with each pipeline run.</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Technology Preview Functionality</dt>

                    <dd class="dd"><span class="ph">Data Collector</span> includes certain new features and stages with the Technology Preview
                        designation. <a class="xref" href="../Pipeline_Design/TechPreview.html#concept_prl_qfv_gfb" title="Data Collector includes certain new features and stages with the Technology Preview designation. Technology Preview functionality is available for use in development and testing, but is not meant for use in production.">Technology Preview functionality</a> is available for use in
                        development and testing, but is not meant for use in production.<p dir="ltr" class="p">Technology Preview stages include the following image on the stage
                            icon: <img class="image" id="concept_sjn_dcb_zhb__image_k2c_n3b_zhb" src="../Graphics/TechPreviewImage.png" />.</p>
<p dir="ltr" class="p">When Technology Preview
                            functionality becomes approved for use in production, the release notes
                            and documentation reflect the change, and the Technology Preview icon is
                            removed from the UI.</p>
<p dir="ltr" class="p">The following Technology Preview
                            stages are available in this release:</p>
<ul class="ul" id="concept_sjn_dcb_zhb__ul_yq4_rmb_zhb">
                            <li dir="ltr" class="li"><a class="xref" href="../Origins/ADLS-G1.html#concept_osx_qgz_xhb">Azure Data Lake Storage Gen1 origin</a> - Reads data from
                                Microsoft Azure Data Lake Storage Gen1. </li>

                            <li dir="ltr" class="li"><a class="xref" href="../Origins/ADLS-G2.html#concept_osx_qgz_xhb">Azure Data Lake Storage Gen2 origin</a> - Reads data from
                                Microsoft Azure Data Lake Storage Gen2. </li>

                            <li dir="ltr" class="li"><a class="xref" href="../Destinations/ADLS-G1-D.html#concept_xzc_wfq_xhb">Azure
                                    Data Lake Storage Gen1 destination</a> - Writes data to
                                Microsoft Azure Data Lake Storage Gen1.</li>

                            <li dir="ltr" class="li"><a class="xref" href="../Destinations/ADLS-G2-D.html#concept_ajp_1d2_vhb">Azure
                                    Data Lake Storage Gen2 destination</a> - Writes data to
                                Microsoft Azure Data Lake Storage Gen2.</li>

                            <li dir="ltr" class="li"><a class="xref" href="../Executors/ADLS-G1-FileMeta.html#concept_zhp_ldk_xhb">ADLS Gen1 File Metadata executor</a> - Changes file
                                metadata, creates an empty file, or removes a file or directory in
                                Microsoft Azure Data Lake Storage Gen1 upon receipt of an event. </li>

                            <li class="li"><a class="xref" href="../Executors/ADLS-G2-FileMeta.html#concept_i22_k2k_xhb">ADLS Gen2 File Metadata executor</a> - Changes file
                                metadata, creates an empty file, or removes a file or directory in
                                Microsoft Azure Data Lake Storage Gen2 upon receipt of an event.
                            </li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Pipelines</dt>

                    <dd class="dd">This release includes the following pipeline enhancements:<ul class="ul" id="concept_sjn_dcb_zhb__ul_rvq_cqb_zhb">
                            <li dir="ltr" class="li">Pipeline Start menu - The <span class="ph">Data Collector</span> toolbar now includes a pipeline Start menu with the following
                                    options:<ul class="ul" id="concept_sjn_dcb_zhb__ul_svq_cqb_zhb">
                                    <li dir="ltr" class="li">
                                        <p dir="ltr" class="p">Start Pipeline</p>

                                    </li>

                                    <li dir="ltr" class="li">
                                        <p dir="ltr" class="p">Reset Origin and Start</p>

                                    </li>

                                    <li dir="ltr" class="li">
                                        <p dir="ltr" class="p">Start with Parameters</p>

                                    </li>

                                </ul>
<p dir="ltr" class="p">Previously, the Reset Origin and Start option was
                                    not available. The Start with Parameters option was located
                                    under the More icon. </p>
</li>

                            <li class="li">Generated events - For stages that generate events, the properties
                                panel now includes a Generated Events tab, which lists and describes
                                the events that origins can generate.</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Data Governance Tools</dt>

                    <dd class="dd">This release includes the following data governance tool enhancement:</dd>

                    <dd class="dd ddexpand">
                        <ul class="ul" id="concept_sjn_dcb_zhb__ul_psr_qx5_b3b">
                            <li class="li"><a class="xref" href="../Configuration/PublishMetadata.html#concept_hn4_s1t_pcb" title="You can configure Data Collector to publish metadata about running pipelines to Apache Atlas. You then use Apache Atlas to explore the pipeline metadata, including viewing lineage diagrams of the metadata.">Apache Atlas versions</a> - <span class="ph">Data Collector</span> can now publish metadata to Apache Atlas version 1.1.0. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Expression Language</dt>

                    <dd class="dd">This release includes the following new time function:<ul class="ul" id="concept_sjn_dcb_zhb__ul_dqc_wqb_zhb">
                            <li class="li"><code class="ph codeph">time:extractNanosecondsFromString(&lt;string&gt;)</code> -
                                Converts a String date with nanoseconds precision to an epoch or
                                UNIX time in milliseconds, and then adds the nanoseconds using the
                                following format:
                                    <code class="ph codeph">&lt;milliseconds_from_epoch&gt;&lt;n&gt;&lt;nanoseconds&gt;</code><p class="p">For
                                    example, the string <code class="ph codeph">29/05/2019
                                        10:12:09.123456789</code> is converted to
                                        <code class="ph codeph">1559124729123&lt;n&gt;456789</code>. </p>
</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm"><span class="ph">Data Collector</span> Configuration</dt>

                    <dd class="dd">This release includes the following <span class="ph">Data Collector</span> configuration enhancements: <ul class="ul" id="concept_sjn_dcb_zhb__ul_jzq_hrb_zhb">
                            <li dir="ltr" class="li">Antenna Doctor - <span class="ph">Data Collector</span> now includes Antenna Doctor, which is a rule-based engine that
                                suggests potential fixes and workarounds to common issues. When
                                needed, you can edit the <span class="ph">Data Collector</span> configuration file, <span class="ph filepath">sdc.properties</span>, to
                                disable Antenna Doctor or to disable Antenna Doctor from
                                periodically retrieving knowledge base updates from the
                                internet.</li>

                            <li dir="ltr" class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_fw3_zt3_tbb">Legacy stage libraries</a> - Package Manager can now install
                                legacy stage libraries. </li>

                            <li class="li">Thycotic Secret Server support - <span class="ph">Data Collector</span> now integrates with the Thycotic Secret Server credential store
                                system.</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Stage Libraries</dt>

                    <dd class="dd">This release includes the following stage library enhancements:<ul class="ul" id="concept_sjn_dcb_zhb__ul_nnf_zxk_1hb">
                            <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">New Stage Libraries</a> - This release includes the
                                following new stage libraries:
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="concept_sjn_dcb_zhb__table_edq_2yk_1hb" class="table" frame="border" border="1" rules="all"><colgroup><col style="width:50%" /><col style="width:50%" /></colgroup><thead class="thead" style="text-align:left;">
                                            <tr>
                                                <th class="entry cellrowborder" id="d717115e4000">Stage Library Name</th>

                                                <th class="entry cellrowborder" id="d717115e4003">Description</th>

                                            </tr>

                                        </thead>
<tbody class="tbody">
                                            <tr>
                                                <td class="entry cellrowborder" headers="d717115e4000 ">streamsets-datacollector-cdh_5_16-lib</td>

                                                <td class="entry cellrowborder" headers="d717115e4003 ">For the Cloudera CDH version 5.16
                                                  distribution of Apache Hadoop.</td>

                                            </tr>

                                            <tr>
                                                <td class="entry cellrowborder" headers="d717115e4000 ">streamsets-datacollector-kinetica_7_0-lib</td>

                                                <td class="entry cellrowborder" headers="d717115e4003 ">For Kinetica 7.0. </td>

                                            </tr>

                                            <tr>
                                                <td class="entry cellrowborder" headers="d717115e4000 ">streamsets-datacollector-thycotic-credentialstore-lib</td>

                                                <td class="entry cellrowborder" headers="d717115e4003 ">For the Thycotic Secret Server credential
                                                  store system. </td>

                                            </tr>

                                        </tbody>
</table>
</div>
</li>

                            <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_fw3_zt3_tbb">Legacy Stage Libraries</a> - The following stage libraries
                                are now legacy stage libraries: 
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="concept_sjn_dcb_zhb__table_f3x_ycn_bhb" class="table" frame="border" border="1" rules="all"><colgroup><col style="width:50%" /><col style="width:50%" /></colgroup><thead class="thead" style="text-align:left;">
                                            <tr>
                                                <th class="entry cellrowborder" id="d717115e4057">Stage Library Name</th>

                                                <th class="entry cellrowborder" id="d717115e4060">Description</th>

                                            </tr>

                                        </thead>
<tbody class="tbody">
                                            <tr>
                                                <td class="entry cellrowborder" headers="d717115e4057 ">streamsets-datacollector-apache-kafka_0_11-lib</td>

                                                <td class="entry cellrowborder" headers="d717115e4060 ">For Kafka version 0.11.x.</td>

                                            </tr>

                                            <tr>
                                                <td class="entry cellrowborder" headers="d717115e4057 ">streamsets-datacollector-cdh_5_12-lib</td>

                                                <td class="entry cellrowborder" headers="d717115e4060 ">For the Cloudera CDH version 5.12
                                                  distribution of Apache Hadoop.</td>

                                            </tr>

                                            <tr>
                                                <td class="entry cellrowborder" headers="d717115e4057 ">streamsets-datacollector-cdh_5_13-lib</td>

                                                <td class="entry cellrowborder" headers="d717115e4060 ">For the Cloudera CDH version 5.13
                                                  distribution of Apache Hadoop.</td>

                                            </tr>

                                            <tr>
                                                <td class="entry cellrowborder" headers="d717115e4057 ">streamsets-datacollector-cdh_kafka_2_1-lib</td>

                                                <td class="entry cellrowborder" headers="d717115e4060 ">For the Cloudera distribution of Apache Kafka
                                                  2.1.x (0.9.0).</td>

                                            </tr>

                                            <tr>
                                                <td class="entry cellrowborder" headers="d717115e4057 ">streamsets-datacollector-cdh_kafka_3_0-lib</td>

                                                <td class="entry cellrowborder" headers="d717115e4060 ">For the Cloudera distribution of Apache Kafka
                                                  3.0.0 (0.11.0).</td>

                                            </tr>

                                            <tr>
                                                <td class="entry cellrowborder" headers="d717115e4057 ">streamsets-datacollector-cdh-spark_2_1-lib</td>

                                                <td class="entry cellrowborder" headers="d717115e4060 ">For the Cloudera CDH cluster Kafka with CDS
                                                  powered by Spark 2.1.</td>

                                            </tr>

                                            <tr>
                                                <td class="entry cellrowborder" headers="d717115e4057 ">streamsets-datacollector-mapr_5_2-lib</td>

                                                <td class="entry cellrowborder" headers="d717115e4060 ">For MapR version 5.2.</td>

                                            </tr>

                                        </tbody>
</table>
</div>
<p class="p">Legacy stage libraries that are more than two years old
                                    are not included with <span class="ph">Data Collector</span>. Though not recommended, you can still install the older
                                    stage libraries.</p>
<p class="p">If you have pipelines that use these
                                    legacy stage libraries, you will need to update the pipelines to
                                    use a more current stage library or install the legacy stage
                                    library. For more information see <a class="xref" href="../Upgrade/PostUpgrade.html#concept_wnl_zk4_5bb">Update Pipelines using Legacy Stage
                                Libraries</a>.</p>
</li>

                        </ul>

                    </dd>

                
            </dl>
</div>

    </div>

<article class="topic concept nested2" aria-labelledby="ariaid-title14" id="concept_pdz_nsb_zhb">
    <h3 class="title topictitle3" id="ariaid-title14"><span class="ph">Data Collector Edge</span> New Features and Enhancements</h3>

    
    <div class="body conbody"><p class="shortdesc"></p>

        <div class="p">This <span class="ph">Data Collector Edge</span> (<span class="ph">SDC Edge</span>) version includes new features and enhancements in the following areas: <dl class="dl">
                
                    <dt class="dt dlterm">Origins in Edge Pipelines</dt>

                    <dd class="dd">When you enable SSL/TLS for the HTTP Server origin in a <span class="ph">Data Collector Edge</span> pipeline, the origin now supports using a keystore file in the PKCS #12
                        format. </dd>

                
                
                    <dt class="dt dlterm">Processors in Edge Pipelines</dt>

                    <dd class="dd"><span class="ph">Data Collector Edge</span> pipelines now support the HTTP Client processor. </dd>

                
                
                    <dt class="dt dlterm">Destinations in Edge Pipelines</dt>

                    <dd class="dd"><span class="ph">Data Collector Edge</span> pipelines now support the Azure Event Hub Producer and the Azure IOT Hub
                        Producer destinations. </dd>

                
            </dl>
</div>

    </div>

</article>
</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title15" id="concept_lvk_lsk_1hb">
    <h2 class="title topictitle2" id="ariaid-title15">What's New in 3.8.0</h2>

    <div class="body conbody">
        <p class="p"><span class="ph">Data Collector</span>
            version 3.8.0 includes the following new features and enhancements:</p>

        <dl class="dl">
            
                <dt class="dt dlterm">Memory Monitoring</dt>

                <dd class="dd">With version 3.8.0, <span class="ph">Data Collector</span> memory monitoring has been removed. Memory monitoring was disabled by default
                    and was to be used only in development for troubleshooting specific issues. <span class="ph">StreamSets</span> recommends using JMX or operating system monitoring for memory consumption. </dd>

                <dd class="dd ddexpand">If memory monitoring is enabled for <span class="ph">Data Collector</span> after upgrading to 3.8.0, a message appears in the log indicating that memory
                    monitoring is no longer supported.</dd>

                <dd class="dd ddexpand">As part of this feature removal, the following changes have been made:<ul class="ul" id="concept_lvk_lsk_1hb__ul_l4p_mtk_1hb">
                        <li class="li">The <code class="ph codeph">monitor.memory</code>
                            <span class="ph">Data Collector</span> configuration property has been removed from the <span class="ph">Data Collector</span> configuration file.</li>

                        <li class="li">Two related pipeline configuration properties have also been removed:
                            Max Pipeline Memory and On Memory Exceeded.</li>

                        <li class="li">Two related counter statistics are no longer available: Heap Memory
                            Usage and Stage Heap Memory Usage.</li>

                    </ul>
</dd>

            
            
                <dt class="dt dlterm">Enterprise Stage Libraries</dt>

                <dd class="dd">Enterprise stage libraries are free for development purposes only. For
                    information about purchasing the stage library for use in production, <a class="xref" href="https://streamsets.com/contact-us/" target="_blank"><u class="ph u">contact StreamSets</u></a>.</dd>

                <dd class="dd ddexpand">With this release, you can use the following new Enterprise stage library:
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="concept_lvk_lsk_1hb__table_ij3_jch_kgb" class="table" frame="border" border="1" rules="all"><colgroup><col style="width:50%" /><col style="width:50%" /></colgroup><thead class="thead" style="text-align:left;">
                                <tr>
                                    <th class="entry cellrowborder" id="d717115e4307">Stage Library Name</th>

                                    <th class="entry cellrowborder" id="d717115e4310">Description</th>

                                </tr>

                            </thead>
<tbody class="tbody">
                                <tr>
                                    <td class="entry cellrowborder" headers="d717115e4307 ">streamsets-datacollector-oracle-lib</td>

                                    <td class="entry cellrowborder" headers="d717115e4310 ">For bulk loading from static Oracle tables.<p class="p">Includes
                                            the <a class="xref" href="../Origins/OracleBulk.html#concept_lnz_kzp_zgb">Oracle Bulkload</a> origin.</p>
</td>

                                </tr>

                            </tbody>
</table>
</div>
</dd>

            
            
                <dt class="dt dlterm">Origins</dt>

                <dd class="dd">This release includes enhancements to the following origins:<ul class="ul" id="concept_lvk_lsk_1hb__ul_itq_vtk_1hb">
                        <li class="li"><a class="xref" href="../Pipeline_Design/DevStages.html#concept_czx_ktn_ht">Dev
                                Raw Data Source</a> - The development origin can now generate
                            events.</li>

                        <li class="li"><a class="xref" href="../Origins/HDFSStandalone.html#concept_djz_pdm_hdb">Hadoop
                                FS Standalone </a>- The origin can now read files from multiple
                            directories specified with a glob pattern.</li>

                        <li class="li"><a class="xref" href="../Origins/OracleCDC.html#task_ehh_mjj_tw">Oracle CDC Client</a> - JDBC Fetch Size property has been
                            replaced by the following new properties:<ul class="ul" id="concept_lvk_lsk_1hb__ul_v1g_15k_1hb">
                                <li class="li">JDBC Fetch Size for Current Window</li>

                                <li class="li">JDBC Fetch Size for Past Windows</li>

                            </ul>
<p class="p">To enable expected behavior, upgraded pipelines use the previous
                                JDBC Fetch Size configuration for the new properties.</p>
</li>

                        <li class="li"><a class="xref" href="../Origins/RESTService.html#concept_mhb_sbv_s2b">REST Service</a> - The origin can now generate responses in XML
                            format in addition to JSON format.</li>

                        <li class="li"><a class="xref" href="../Origins/Salesforce.html#concept_oc2_zsj_lhb">Salesforce</a> - The origin now supports aggregate functions in
                            SOQL queries.</li>

                        <li class="li">SFTP/FTP Client - The origin includes the following enhancements:<ul class="ul" id="concept_lvk_lsk_1hb__ul_iyv_35k_1hb">
                                <li class="li">The origin now supports using an email address for the user name
                                    in the <a class="xref" href="../Origins/SFTP.html#task_lfx_fzd_5v">resource URL</a>.</li>

                                <li class="li">The origin now supports using a glob pattern or a regular
                                    expression to define the <a class="xref" href="../Origins/SFTP.html#concept_xd5_5z4_4y" title="Use a file name pattern to define the files that the SFTP/FTP/FTPS Client origin processes. You can use either a glob pattern or a regular expression to define the file name pattern.">file name pattern</a>. Previously, the origin supported
                                    only a glob pattern.</li>

                                <li class="li">When configured for <a class="xref" href="../Origins/SFTP.html#concept_vnj_njp_yv">private key authentication</a>, the origin now supports
                                    entering the full path to the private key file or entering the
                                    private key in plain text. Previously, the origin supported only
                                    entering the full path to the file.</li>

                                <li class="li"><a class="xref" href="../Origins/SFTP.html#concept_jbv_xvs_fhb">After processing a file</a>, the origin can now keep,
                                    archive, or delete the file.</li>

                            </ul>
</li>

                        <li class="li"><a class="xref" href="../Origins/SQLServerCDC.html#task_nsg_fxc_v1b">SQL Server CDC Client</a> - The default for the Maximum
                            Transaction Length property has changed from <code class="ph codeph">${1*HOUR}</code>
                            to <code class="ph codeph">-1</code> to opt out of using the property. Upgraded
                            pipelines are not affected.</li>

                        <li class="li"><a class="xref" href="../Origins/WebSocketClient.html#concept_mhb_sbv_s2b">WebSocket Client</a> - The origin can now generate responses in
                            XML format in addition to JSON format.</li>

                        <li class="li"><a class="xref" href="../Origins/WebSocketServer.html#concept_mhb_sbv_s2b">WebSocket Server</a> - The origin can now generate responses in
                            XML format in addition to JSON format.</li>

                    </ul>
</dd>

            
            
                <dt class="dt dlterm">Processors</dt>

                <dd class="dd"> This release includes the following new processor:<ul class="ul" id="concept_lvk_lsk_1hb__ul_flv_cvk_1hb">
                        <li class="li"><a class="xref" href="../Processors/FieldMapper.html#concept_q5y_tdq_xgb">Field
                                Mapper</a> - Maps an expression to a set of fields to alter field
                            paths, field names, or field values.</li>

                    </ul>
</dd>

                <dd class="dd ddexpand">This release includes enhancements to the following processors: <ul class="ul" id="concept_lvk_lsk_1hb__ul_h2r_fvk_1hb">
                        <li class="li"><a class="xref" href="../Processors/FieldFlattener.html#task_xdv_kkk_fx">Field Flattener</a> - When flattening specific fields, the
                            processor now supports selecting fields using preview data in addition
                            to entering the path to each field.</li>

                        <li class="li"><a class="xref" href="../Processors/SalesforceLookup.html#concept_oc2_zsj_lhb">Salesforce Lookup</a> - The processor now supports aggregate
                            functions in SOQL queries.</li>

                        <li class="li"><a class="xref" href="../Processors/Aggregator.html#concept_ofb_svm_5bb">Windowing
                                Aggregator</a> - The Aggregator processor has been renamed to the
                            Windowing Aggregator processor.</li>

                    </ul>
</dd>

            
            
                <dt class="dt dlterm">Destinations</dt>

                <dd class="dd">This release includes enhancements to the following destinations: <ul class="ul" id="concept_lvk_lsk_1hb__ul_tmf_vwk_1hb">
                        <li class="li"><a class="xref" href="../Destinations/PubSubPublisher.html#task_n1k_sk1_v1b">Google Pub/Sub Publisher</a> - The destination now includes
                            properties to configure batches.</li>

                        <li class="li"><a class="xref" href="../Destinations/Solr.html#task_ld1_phr_wr">Solr</a> - The destination can now directly map record fields to
                            Solr schema fields.</li>

                    </ul>
</dd>

            
            
                <dt class="dt dlterm">Pipelines</dt>

                <dd class="dd">This release includes the following pipeline enhancements: <ul class="ul" id="concept_lvk_lsk_1hb__ul_kpg_xwk_1hb">
                        <li class="li"><a class="xref" href="../Pipeline_Maintenance/PipelineMaintenance_title.html#task_dtz_4tr_5q">Pipeline export</a> with or without plain text credentials - <span class="ph">Data Collector</span> now provides the following pipeline export options:<ul class="ul" id="concept_lvk_lsk_1hb__ul_qwr_zwk_1hb">
                                <li class="li">Export - Strips all plain text credentials from the exported
                                    pipeline.</li>

                                <li class="li">Export with Plain Text Credentials - Includes all plain text
                                    credentials in the exported pipeline.</li>

                            </ul>
<p class="p">Previously, <span class="ph">Data Collector</span> always included plain text credentials in exported
                            pipelines.</p>
</li>

                        <li class="li"><a class="xref" href="../Origins/RESTService.html#concept_mhb_sbv_s2b">New microservice raw responses</a> - Origins in microservice
                            pipelines can now send raw responses, passing responses to the origin
                            system without an envelope.</li>

                        <li class="li">Pipeline labels enhancement - You can now configure pipeline labels in
                            the New Pipeline dialog box when you create a pipeline. As in earlier
                            releases, you can also configure labels on the General tab of pipeline
                            properties.</li>

                    </ul>
</dd>

            
            
                <dt class="dt dlterm">Data Formats</dt>

                <dd class="dd">This release includes the following data formats enhancement:<ul class="ul" id="concept_lvk_lsk_1hb__ul_etb_kxk_1hb">
                        <li class="li"><a class="xref" href="../Data_Formats/Delimited.html#concept_ust_d4q_qgb">Delimited</a> - <span class="ph">Data Collector</span> now supports multi-character field separators in delimited data.</li>

                    </ul>
</dd>

            
            
                <dt class="dt dlterm"><span class="ph">Data Collector</span> Configuration</dt>

                <dd class="dd">This release includes the following <span class="ph">Data Collector</span> configuration enhancements: <ul class="ul" id="concept_lvk_lsk_1hb__ul_msb_nxk_1hb">
                        <li class="li"><a class="xref" href="../Configuration/DCConfig.html#concept_aqc_dbt_zr" title="You can protect sensitive data in Data Collector configuration files by storing the data in an external location and then using the file or exec function to retrieve the data.">Protect sensitive data in configuration files</a> - You can now
                            protect sensitive data in <span class="ph">Data Collector</span> configuration files by storing the data in an external location and
                            then using the <code class="ph codeph">exec</code> function to call a script or
                            executable that retrieves the data. For example, you can develop a
                            script that decrypts an encrypted file containing a password. Or you can
                            develop a script that calls an external REST API to retrieve a password
                            from a remote vault system.<div class="p">After developing the script, use the
                                    <code class="ph codeph">exec</code> function in the <span class="ph">Data Collector</span> configuration file to call the script or executable as
                                follows:<pre class="pre codeblock"><code>${exec("&lt;script name&gt;")}</code></pre></div>
</li>

                        <li class="li"><a class="xref" href="../Configuration/CredentialStores.html#concept_vl1_d5j_xgb">AWS Secrets Manager support</a> - <span class="ph">Data Collector</span> now integrates with the AWS Secrets Manager credential store
                            system.</li>

                    </ul>
</dd>

            
            
                <dt class="dt dlterm">Stage Libraries</dt>

                <dd class="dd">This release includes the following stage library enhancements:<ul class="ul" id="concept_lvk_lsk_1hb__ul_nnf_zxk_1hb">
                        <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">New Stage Libraries</a> - This release includes the following
                            new stage libraries:
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="concept_lvk_lsk_1hb__table_edq_2yk_1hb" class="table" frame="border" border="1" rules="all"><colgroup><col style="width:50%" /><col style="width:50%" /></colgroup><thead class="thead" style="text-align:left;">
                                        <tr>
                                            <th class="entry cellrowborder" id="d717115e4660">Stage Library Name</th>

                                            <th class="entry cellrowborder" id="d717115e4663">Description</th>

                                        </tr>

                                    </thead>
<tbody class="tbody">
                                        <tr>
                                            <td class="entry cellrowborder" headers="d717115e4660 ">streamsets-datacollector-aws-secrets-manager-credentialstore-lib</td>

                                            <td class="entry cellrowborder" headers="d717115e4663 ">For the AWS Secrets Manager credential store
                                                system.</td>

                                        </tr>

                                        <tr>
                                            <td class="entry cellrowborder" headers="d717115e4660 ">streamsets-datacollector-hdp_3_1-lib</td>

                                            <td class="entry cellrowborder" headers="d717115e4663 ">For Hortonworks version 3.1.</td>

                                        </tr>

                                        <tr>
                                            <td class="entry cellrowborder" headers="d717115e4660 ">streamsets-datacollector-mapr_6_1-lib</td>

                                            <td class="entry cellrowborder" headers="d717115e4663 ">For MapR version 6.1.0.</td>

                                        </tr>

                                        <tr>
                                            <td class="entry cellrowborder" headers="d717115e4660 ">streamsets-datacollector-mapr_6_1-mep6-lib</td>

                                            <td class="entry cellrowborder" headers="d717115e4663 ">For MapR 6.1.0, MapR Ecosystem Pack (MEP) version
                                                6.</td>

                                        </tr>

                                    </tbody>
</table>
</div>
</li>

                        <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_fw3_zt3_tbb">Legacy Stage Libraries</a> - The following stage libraries are
                            now legacy stage libraries: 
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="concept_lvk_lsk_1hb__table_f3x_ycn_bhb" class="table" frame="border" border="1" rules="all"><colgroup><col style="width:50%" /><col style="width:50%" /></colgroup><thead class="thead" style="text-align:left;">
                                        <tr>
                                            <th class="entry cellrowborder" id="d717115e4726">Stage Library Name</th>

                                            <th class="entry cellrowborder" id="d717115e4729">Description</th>

                                        </tr>

                                    </thead>
<tbody class="tbody">
                                        <tr>
                                            <td class="entry cellrowborder" headers="d717115e4726 ">streamsets-datacollector-cdh_5_10-lib</td>

                                            <td class="entry cellrowborder" headers="d717115e4729 ">For the Cloudera CDH version 5.10 distribution of
                                                Apache Hadoop.</td>

                                        </tr>

                                        <tr>
                                            <td class="entry cellrowborder" headers="d717115e4726 ">streamsets-datacollector-cdh_5_11-lib</td>

                                            <td class="entry cellrowborder" headers="d717115e4729 ">For the Cloudera CDH version 5.11 distribution of
                                                Apache Hadoop.</td>

                                        </tr>

                                    </tbody>
</table>
</div>
<p class="p">Legacy stage libraries that are more than two years old are
                                not included with <span class="ph">Data Collector</span>. Though not recommended, you can still download and install the
                                older stage libraries as custom stage libraries.</p>
<p class="p">If you have
                                pipelines that use these legacy stage libraries, you will need to
                                update the pipelines to use a more current stage library or install
                                the legacy stage library manually. For more information see <a class="xref" href="../Upgrade/PostUpgrade.html#concept_wnl_zk4_5bb">Update Pipelines using Legacy Stage Libraries</a>.</p>
</li>

                    </ul>

                </dd>

            
        </dl>

    </div>

<article class="topic concept nested2" aria-labelledby="ariaid-title16" id="concept_jgf_vyk_1hb">
    <h3 class="title topictitle3" id="ariaid-title16"><span class="ph">Data Collector Edge</span> New Features and Enhancements</h3>

    <div class="body conbody">
        <p class="p">This <span class="ph">Data Collector Edge</span> (<span class="ph">SDC Edge</span>) version includes new features and enhancements in the following areas:</p>

        <dl class="dl">
            
                <dt class="dt dlterm">Origins in Edge Pipelines</dt>

                <dd class="dd">This release includes the following origin enhancements:<ul class="ul" id="concept_jgf_vyk_1hb__ul_kpz_knc_vhb">
                        <li class="li">In edge pipelines, <a class="xref" href="../Edge_Mode/EdgePipelineTypes.html#concept_n2n_r2q_s2b" title="Edge pipelines support a limited number of origins.">origins</a>
                            can now process compressed files.</li>

                        <li class="li">The Directory origin now supports file post-processing in edge
                            pipelines. You can now configure an error directory, and you can have
                            the origin archive or delete files after processing.</li>

                    </ul>
</dd>

            
            
                <dt class="dt dlterm">Destinations in Edge Pipelines</dt>

                <dd class="dd">This release includes the following destination enhancements: <ul class="ul" id="concept_jgf_vyk_1hb__ul_orl_31h_bhb">
                        <li class="li">Edge pipelines now support the <a class="xref" href="../Destinations/AmazonS3.html#concept_avx_bnq_rt">Amazon S3
                                destination</a>.</li>

                        <li class="li">In edge pipelines, you can now configure the Kafka Producer destination
                            to <a class="xref" href="../Destinations/KProducer.html#concept_znr_b3c_rw">connect securely to Kafka</a> through SSL/TLS.</li>

                    </ul>
</dd>

            
            
                <dt class="dt dlterm">Data Formats in Edge Pipelines</dt>

                <dd class="dd">This release includes the following data formats enhancements:<ul class="ul" id="concept_jgf_vyk_1hb__ul_czf_y1l_1hb">
                        <li class="li">Stages included in edge pipelines now list only the data formats that
                            are supported in edge pipelines.</li>

                        <li class="li">The following stages can now process the binary data format when they
                            are included in edge pipelines:<ul class="ul" id="concept_jgf_vyk_1hb__ul_gds_z1l_1hb">
                                <li class="li"><a class="xref" href="../Destinations/AmazonS3.html#concept_k2z_ncx_rt">Amazon S3 destination</a></li>

                                <li class="li"><a class="xref" href="../Destinations/CoAPClient.html#concept_fcy_cj4_sz">CoAP Client destination</a></li>

                                <li class="li">HTTP Client <a class="xref" href="../Origins/HTTPClient.html#concept_mnv_s5r_35">origin</a> and <a class="xref" href="../Destinations/HTTPClient.html#concept_l2r_gy5_lz">destination</a></li>

                                <li class="li"><a class="xref" href="../Origins/HTTPServer.html#concept_anf_ss4_qy">HTTP Server origin</a></li>

                                <li class="li"><a class="xref" href="../Destinations/KProducer.html#concept_lww_3b3_kra">Kafka Producer destination</a></li>

                                <li class="li"><a class="xref" href="../Destinations/KinProducer.html#concept_x33_b5c_r5">Kinesis Producer destination</a></li>

                                <li class="li"><a class="xref" href="../Origins/MQTTSubscriber.html#concept_bxn_fxq_mz">MQTT Subscriber origin</a> and <a class="xref" href="../Destinations/MQTTPublisher.html#concept_xn3_zxq_mz">MQTT Publisher destination</a></li>

                                <li class="li"><a class="xref" href="../Origins/WebSocketClient.html#concept_k3k_czr_fbb">WebSocket Client origin</a> and <a class="xref" href="../Destinations/WebSocketClient.html#concept_dsx_f5b_mz">destination</a></li>

                            </ul>
</li>

                        <li class="li">The following stages can now process the whole file data format when
                            they are included in edge pipelines:<ul class="ul" id="concept_jgf_vyk_1hb__ul_n4l_2bl_1hb">
                                <li class="li"><a class="xref" href="../Destinations/AmazonS3.html#concept_k2z_ncx_rt">Amazon S3 destination</a></li>

                                <li class="li"><a class="xref" href="../Origins/Directory.html#concept_gz5_dqw_yq">Directory origin</a></li>

                            </ul>
</li>

                    </ul>

                </dd>

            
        </dl>

    </div>

</article>
</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title17" id="concept_fnl_4lt_dgb">
    <h2 class="title topictitle2" id="ariaid-title17">What's New in 3.7.0</h2>

    <div class="body conbody">
        <div class="p"><span class="ph">Data Collector</span>
            version 3.7.0 includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Microsoft Azure Support</dt>

                    <dd class="dd">With this release, you can now use the <a class="xref" href="../Origins/HDFSStandalone.html#concept_djz_pdm_hdb">Hadoop FS
                            Standalone origin</a> to read data from Azure Data Lake Storage. You
                        can also use the <a class="xref" href="../Destinations/HadoopFS-destination.html#concept_awl_4km_zq">Hadoop FS destination</a> to write to Azure Data Lake Storage. <p dir="ltr" class="p">Use the Hadoop FS destination when you need to use Azure
                            Active Directory refresh token authentication to connect to Azure Data
                            Lake Storage, or when you want to write to Azure Data Lake Storage in a
                            cluster streaming pipeline. For all other cases, use the existing Azure
                            Data Lake Storage destination. </p>
</dd>

                
                
                    <dt class="dt dlterm">Enterprise Stage Libraries</dt>

                    <dd class="dd">Enterprise stage libraries are free for development purposes only. For
                        information about purchasing the stage library for use in production, <a class="xref" href="https://streamsets.com/contact-us/" target="_blank"><u class="ph u">contact StreamSets</u></a>.</dd>

                    <dd class="dd ddexpand">This release includes the following new Enterprise stage libraries:
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="concept_fnl_4lt_dgb__table_ij3_jch_kgb" class="table" frame="border" border="1" rules="all"><colgroup><col style="width:50%" /><col style="width:50%" /></colgroup><thead class="thead" style="text-align:left;">
                                    <tr>
                                        <th class="entry cellrowborder" id="d717115e5007">Stage Library Name</th>

                                        <th class="entry cellrowborder" id="d717115e5010">Description</th>

                                    </tr>

                                </thead>
<tbody class="tbody">
                                    <tr>
                                        <td class="entry cellrowborder" headers="d717115e5007 ">streamsets-datacollector-memsql-lib</td>

                                        <td class="entry cellrowborder" headers="d717115e5010 ">For MemSQL.<p dir="ltr" class="p">Includes the MemSQL Fast
                                                Loader destination.</p>
</td>

                                    </tr>

                                    <tr>
                                        <td class="entry cellrowborder" headers="d717115e5007 ">streamsets-datacollector-snowflake-lib</td>

                                        <td class="entry cellrowborder" headers="d717115e5010 ">For Snowflake.<p dir="ltr" class="p">Includes the Snowflake
                                                destination.</p>
</td>

                                    </tr>

                                    <tr>
                                        <td class="entry cellrowborder" headers="d717115e5007 ">streamsets-datacollector-teradata-lib</td>

                                        <td class="entry cellrowborder" headers="d717115e5010 ">For Teradata.<p dir="ltr" class="p">Includes the Teradata
                                                Consumer origin.</p>
</td>

                                    </tr>

                                </tbody>
</table>
</div>
</dd>

                
                
                    <dt class="dt dlterm">Installation</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_fnl_4lt_dgb__ul_ith_yzq_4gb">
                            <li class="li"><a class="xref" href="../Installation/CloudInstall.html#task_op3_tvp_vdb" title="You can install the full Data Collector on Microsoft Azure.">Install Data Collector on Microsoft Azure</a> - The process
                                to install <span class="ph">Data Collector</span> on Microsoft Azure has been enhanced. <span class="ph">Data Collector</span> now automatically starts as a service after the resource is
                                deployed. You no longer need to use SSH to log in to the virtual
                                machine to run the <span class="ph">Data Collector</span> installation script and then start <span class="ph">Data Collector</span>.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">This release includes the following new origin:<ul class="ul" id="concept_fnl_4lt_dgb__ul_oyb_rch_kgb">
                            <li class="li"><a class="xref" href="../Origins/Teradata.html#concept_zp3_wnw_4y">Teradata
                                    Consumer origin</a> - Reads data from multiple Teradata
                                Database tables. To use this origin, you must install the Teradata
                                stage library. This is an enterprise stage library.</li>

                        </ul>
</dd>

                    <dd class="dd ddexpand">This release includes enhancements to the following origins:<ul class="ul" id="concept_fnl_4lt_dgb__ul_icj_vmt_dgb">
                            <li class="li"><a class="xref" href="../Origins/AmazonS3.html#concept_pcl_nwn_qbb" title="The Amazon S3 origin uses multiple concurrent threads to process data based on the Number of Threads property.">Amazon S3</a> - The origin can now create multiple threads
                                to enable parallel processing in a multithreaded pipeline.</li>

                            <li class="li"><a class="xref" href="../Pipeline_Design/DevStages.html#concept_czx_ktn_ht">Dev Data Generator</a> - The development origin now
                                generates fake data for email, race, sex, and social security
                                numbers. </li>

                            <li class="li"><a class="xref" href="../Origins/Elasticsearch.html#concept_cxd_l1z_xfb">Elasticsearch</a> - The origin now supports authentication
                                with AWS credentials when using Amazon Elasticsearch Service.</li>

                            <li class="li"><a class="xref" href="../Origins/HDFSStandalone.html#concept_djz_pdm_hdb">Hadoop FS Standalone</a> - The origin now supports reading
                                data from Microsoft Azure Data Lake Storage.</li>

                            <li class="li">Kafka Consumer - The origin includes the following enhancements:<ul class="ul" id="concept_fnl_4lt_dgb__ul_jgm_l4t_dgb">
                                    <li class="li">A new Auto Offset Reset property determines the first
                                        message read in a topic when a consumer group and topic has
                                        no previous <a class="xref" href="../Origins/KConsumer.html#concept_zlc_ppn_js" title="The first time that a Kafka Consumer origin identified by a consumer group receives messages from a topic, an offset entry is created for that consumer group and topic. The offset entry is created in ZooKeeper or Kafka, depending on your Kafka version and broker configuration.">offset stored</a>. The origin can read from the
                                        earliest message, the latest message, or a particular
                                        timestamp. The default setting causes the origin to read all
                                        existing messages in a topic.<p class="p">In previous versions, the
                                            origin read only new messages by default. For
                                            information on upgrading pipelines that use the Kafka
                                            Consumer origin, see <a class="xref" href="../Upgrade/PostUpgrade.html#concept_lxn_s5h_gnb">Update Kafka Consumer or Kafka Multitopic Consumer Pipelines</a>. </p>
</li>

                                    <li class="li">A new Include Timestamps property enables you to include
                                        Kafka timestamps in the <a class="xref" href="../Origins/KConsumer.html#concept_tlj_3g1_2z">record header</a>. </li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Origins/KafkaMultiConsumer.html#concept_zlc_ppn_js" title="The first time that a Kafka Multitopic Consumer origin identified by a consumer group receives messages from a topic, an offset entry is created for that consumer group and topic. The offset entry is created in Kafka.">Kafka Multitopic Consumer</a> - The origin includes a new
                                Auto Offset Reset property that determines the first message read in
                                a topic when a consumer group and topic has no previous offset
                                stored. The origin can read from the earliest message, the latest
                                message, or a particular timestamp. The default setting causes the
                                origin to read all existing messages in a topic.<p dir="ltr" class="p">In
                                    previous versions, the origin read only new messages by default.
                                    For information on upgrading pipelines that use the Kafka
                                    Multitopic Consumer origin, see <a class="xref" href="../Upgrade/PostUpgrade.html#concept_lxn_s5h_gnb">Update Kafka Consumer or Kafka Multitopic Consumer Pipelines</a>.</p>
</li>

                            <li class="li"><a class="xref" href="../Origins/PostgreSQL.html#task_v21_nm4_n2b">PostgreSQL CDC Client</a> - The origin now has a new default
                                value for the Replication Slot property: <code class="ph codeph">sdc</code>. This
                                property must contain only lowercase letters and numbers. </li>

                            <li class="li"><a class="xref" href="../Origins/RESTService.html#task_upp_lgp_q2b">REST Service</a> - This microservice origin now supports SSL
                                mutual authentication.</li>

                            <li class="li"><a class="xref" href="../Origins/Salesforce.html#concept_cwb_mkg_5cc">Salesforce</a> - The origin now includes a new subscription
                                type: Change Data Capture. </li>

                            <li class="li"><a class="xref" href="../Origins/SQLServerCDC.html#task_nsg_fxc_v1b">SQL Server CDC Client</a> - The origin now includes the Use
                                Direct Table Query property to enable direct table queries and the
                                Maximum Transaction Length property to specify the amount of time to
                                check for changes to a record before committing the data.</li>

                            <li class="li"><a class="xref" href="../Origins/TCPServer.html#task_w2y_yb1_4z">TCP Server</a> - The origin now includes a Read Timeout
                                property that sets the amount of time that the origin waits to
                                receive data before Data Collector closes the connection. The
                                default is 5 minutes. In previous releases, the connection remained
                                opened indefinitely. </li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Processors</dt>

                    <dd class="dd">This release includes enhancements to the following processors:<ul class="ul" id="concept_fnl_4lt_dgb__ul_rt4_xpt_dgb">
                            <li class="li"><a class="xref" href="../Processors/DatabricksML.html#concept_nlz_k3v_gfb">Databricks ML Evaluator</a> - With this release, this
                                processor is no longer considered a Technology Preview feature and
                                is approved for use in production. Also, you can now specify a model
                                path relative to the Data Collector resources directory.</li>

                            <li class="li"><a class="xref" href="../Processors/FieldHasher.html#task_xjd_dlk_wq">Field Hasher</a> - The processor now supports hashing with
                                the SHA512 cryptographic hash function.</li>

                            <li class="li"><a class="xref" href="../Processors/FieldRemover.html#concept_jdd_blr_wq">Field Remover</a> - In addition to removing fields, and
                                removing fields with null values, the processor now supports
                                removing fields under the following conditions: <ul class="ul" id="concept_fnl_4lt_dgb__ul_scd_zpt_dgb">
                                    <li class="li">When the values are empty strings.</li>

                                    <li class="li">When the values are null or empty strings.</li>

                                    <li class="li">When the values are a specified value.</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="WhatsNew_Title.html#concept_fnl_4lt_dgb">Field Renamer</a> - The
                                processor now supports the StreamSets expression language in target
                                field paths. With this feature, you can use string functions to
                                change field names to be all uppercase or lowercase. </li>

                            <li class="li"><a class="xref" href="../Processors/JDBCLookup.html#task_kbr_2cy_hw">JDBC Lookup</a> - The processor now supports returning
                                multiple matching values as a list in a single record.</li>

                            <li class="li"><a class="xref" href="../Processors/KuduLookup.html#concept_jrc_xl3_jbb">Kudu Lookup</a> - The processor now supports the Decimal
                                data type available with Apache Kudu 1.7 and later. </li>

                            <li class="li"><a class="xref" href="../Processors/MLeap.html#concept_wnr_wlv_gfb">MLeap
                                    Evaluator</a> - With this release, this processor is no
                                longer considered a Technology Preview feature and is approved for
                                use in production. Also, you can now specify a model path relative
                                to the Data Collector resources directory.</li>

                            <li class="li"><a class="xref" href="../Processors/MongoDBLookup.html#task_yt1_w4w_2fb">MongoDB Lookup</a> - The processor includes the following
                                updates to property and tab names: <ul class="ul" id="concept_fnl_4lt_dgb__ul_pty_nqt_dgb">
                                    <li dir="ltr" class="li">
                                        <p dir="ltr" class="p">Several properties have moved from the MongoDB
                                            tab to a new Lookup tab.</p>

                                    </li>

                                    <li dir="ltr" class="li">
                                        <p dir="ltr" class="p">The SDC Field to Document Field Mapping
                                            property is now known as Document to SDC Field
                                            Mappings.</p>

                                    </li>

                                    <li dir="ltr" class="li">
                                        <p dir="ltr" class="p">The Field Name in Document property is now
                                            known as Document Field. </p>

                                    </li>

                                    <li dir="ltr" class="li">
                                        <p dir="ltr" class="p">The New Field to Save Lookup Result property is
                                            now known as Result Field.</p>

                                    </li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Processors/PMML.html#concept_r3s_3fv_gfb">PMML
                                    Evaluator</a> - The processor is approved for use in
                                production. This release removes the Technology Preview designation.
                                Also, you can now specify a model path relative to the Data
                                Collector resources directory.</li>

                            <li class="li"><a class="xref" href="../Processors/SalesforceLookup.html#concept_ow1_lj3_xbb">Salesforce Lookup</a> - The processor now supports using
                                time functions in SOQL queries. </li>

                            <li class="li"><a class="xref" href="../Processors/TensorFlow.html#concept_otg_csh_z2b">TensorFlow Evaluator</a> - With this release, this processor
                                is no longer considered a Technology Preview feature and is approved
                                for use in production. Also, you can now specify a model path
                                relative to the <span class="ph">Data Collector</span> resources directory.</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Destinations</dt>

                    <dd class="dd">This release includes the following new destinations:<ul class="ul" id="concept_fnl_4lt_dgb__ul_mxv_5ch_kgb">
                            <li class="li"><a class="xref" href="../Destinations/MemSQLLoader.html#concept_kvs_3hh_ht">MemSQL Fast Loader destination</a> - Inserts data into a
                                MemSQL or MySQL database table with a LOAD statement. To use this
                                destination, you must install the MemSQL stage library. This is an
                                Enterprise stage library.</li>

                            <li class="li"><a class="xref" href="../Destinations/Snowflake.html#concept_vxl_zzc_1gb">Snowflake destination</a> - Writes new or CDC data to tables
                                in a Snowflake database schema. To use this destination, you must
                                install the Snowflake stage library. This is an Enterprise stage
                                library.</li>

                        </ul>
</dd>

                    <dd class="dd ddexpand">This release includes enhancements to the following destinations: <ul class="ul" id="concept_fnl_4lt_dgb__ul_cth_qst_dgb">
                            <li class="li"><a class="xref" href="../Destinations/DataLakeStore.html#concept_jzm_kf4_zx">Azure Data Lake Storage</a> - Due to Microsoft rebranding,
                                the Azure Data Lake Store destination is now known as the Azure Data
                                Lake Storage destination. </li>

                            <li class="li">Elasticsearch - The destination now includes:<ul class="ul" id="concept_fnl_4lt_dgb__ul_n3v_qst_dgb">
                                    <li class="li">
                                        <p class="p">Support for <a class="xref" href="../Destinations/Elasticsearch.html#concept_cxd_l1z_xfb">authentication</a> with AWS credentials when
                                            using Amazon Elasticsearch Service.</p>

                                    </li>

                                    <li class="li">
                                        <p class="p">A new <a class="xref" href="../Destinations/Elasticsearch.html#task_uns_gtv_4r">Additional Properties property</a> to specify an
                                            extra field in an action statement.</p>

                                    </li>

                                </ul>
</li>

                            <li class="li">
                                <p class="p">
                                    <a class="xref" href="../Destinations/HadoopFS-destination.html#concept_awl_4km_zq">Hadoop FS</a> - The destination now supports writing
                                    data to Microsoft Azure Data Lake Storage. </p>

                            </li>

                            <li class="li">
                                <p class="p"><a class="xref" href="../Destinations/Kudu.html#concept_gjj_dg3_jbb">Kudu</a> - The destination now supports the Decimal data
                                    type if using the Apache Kudu 1.7.0 stage library. </p>

                            </li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Data Governance Tools</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_fnl_4lt_dgb__ul_erv_g5t_dgb">
                            <li class="li">Pipeline metadata - <span class="ph">Data Collector</span> now publishes additional pipeline metadata to <a class="xref" href="../Configuration/PublishMetadata.html#concept_pvq_cs3_v1b" title="You can view published pipeline metadata in near real time in Cloudera Navigator. Cloudera Navigator lists each pipeline by pipeline title and displays supported origins as inputs and supported destinations as outputs. Cloudera Navigator also includes a URL to the pipeline within Data Collector along with other metadata, such as the pipeline description, labels, version, and the user who started the pipeline.">Cloudera Navigator</a> and <a class="xref" href="../Configuration/PublishMetadata.html#concept_vvt_vpt_pcb" title="Data Collector publishes pipeline metadata to Apache Atlas when a pipeline stops. Apache Atlas lists each pipeline by pipeline title and displays supported origins as inputs and supported destinations as outputs. Apache Atlas also includes other metadata, such as the pipeline description, labels, version, and the user who started the pipeline.">Apache Atlas</a>, including the pipeline description,
                                labels, parameters, version, and the user who started the
                                pipeline.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Pipeline Parameters</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_fnl_4lt_dgb__ul_yfs_35t_dgb">
                            <li class="li"><a class="xref" href="../Pipeline_Configuration/RuntimeValues.html#concept_d4c_hs3_2gb" title="To call a runtime parameter in a stage or pipeline property that displays as a checkbox or drop-down menu, you first must convert the property to a text box.">Parameters for checkboxes and drop-down menus</a> - You can
                                now call pipeline parameters for properties that display as
                                checkboxes and drop-down menus. The parameters must evaluate to a
                                valid option for the property.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Cluster Pipelines</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_fnl_4lt_dgb__ul_af1_k5t_dgb">
                            <li class="li"><a class="xref" href="../Cluster_Mode/ClusterPipelines.html#concept_cwy_xgl_cgb" title="Data Collector requires that the Java temporary directory on the gateway node in the cluster is writable.">Gateway
                                    node requires writable temporary directory</a> - When you run
                                a cluster pipeline, <span class="ph">Data Collector</span> now requires that the Java temporary directory on the gateway
                                node is writable. The Java temporary directory is specified by the
                                Java system property <code class="ph codeph">java.io.tmpdir</code>. On UNIX, the
                                default value of this property is typically <code class="ph codeph">/tmp</code>
                                and is writable.<p class="p">For information about upgrading cluster pipelines
                                    that previously ran on gateway nodes without a writable
                                    temporary directory, see <a class="xref" href="../Upgrade/PostUpgrade.html#concept_clv_tzk_cgb" title="Starting with version 3.7.0, Data Collector now requires that the Java temporary directory on the gateway node in the cluster is writable.">Update Cluster Pipelines</a>.</p>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Expression Language</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_fnl_4lt_dgb__ul_mnw_z5t_dgb">
                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_ahp_f4v_1r">String functions</a> - This release includes the following
                                new function:<ul class="ul" id="concept_fnl_4lt_dgb__ul_r25_bvt_dgb">
                                    <li class="li"><code class="ph codeph">str:lastIndexOf(&lt;string&gt;,&lt;subset&gt;)</code> -
                                        Returns the index within a string of the last occurrence of
                                        the specified subset of characters.</li>

                                </ul>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm"><span class="ph">Data Collector</span> Configuration</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_fnl_4lt_dgb__ul_fp4_jvt_dgb">
                            <li class="li"><a class="xref" href="../Configuration/DCEnvironmentConfig.html#concept_tm4_pbg_ht" title="Data Collector includes a Java Security Manager that is enabled by default. For enhanced security, you can enable the Data Collector Security Manager which prevents stages from accessing files in protected Data Collector directories."><span class="ph">Data Collector</span> Security Manager</a> - For enhanced security, the Data
                                Collector configuration file now provides a property to enable the
                                    <span class="ph">Data Collector</span> Security Manager, instead of the Java Security Manager. The <span class="ph">Data Collector</span> Security Manager does not allow stages to access files in the
                                following directories:<div class="p">
                                    <ul class="ul" id="concept_fnl_4lt_dgb__ul_nyp_jvt_dgb">
                                        <li dir="ltr" class="li">
                                            <p dir="ltr" class="p">Configuration directory defined in the
                                                SDC_CONF environment variable.</p>

                                        </li>

                                        <li dir="ltr" class="li">
                                            <p dir="ltr" class="p">Data directory defined in the SDC_DATA
                                                environment variable.</p>

                                        </li>

                                    </ul>

                                </div>
<p class="p">In addition, the <span class="ph">Data Collector</span> Security Manager does not allow stages to write to files in
                                    the resources directory defined in the SDC_RESOURCES environment
                                    variable. Stages can only read files in the resources directory.
                                    </p>
<p class="p">By default, <span class="ph">Data Collector</span> uses the Java Security Manager which allows stages to access
                                    files in all <span class="ph">Data Collector</span> directories.</p>
</li>

                            <li class="li"><a class="xref" href="../Configuration/DCConfig.html#task_lxk_kjw_1r" title="You can customize Data Collector by editing the Data Collector configuration file, sdc.properties.">HTTP/2 support</a> - <span class="ph">Data Collector</span> now provides a property in the Data Collector configuration file
                                to enable support of the HTTP/2 protocol for the UI and API. Because
                                HTTP/2 requires TLS, to enable HTTP/2, configure both the
                                    <code class="ph codeph">http2.enable</code> and the
                                    <code class="ph codeph">https.port</code> properties.</li>

                            <li class="li">Package Manager - The Package Manager includes the following
                                    enhancements:<ul class="ul" id="concept_fnl_4lt_dgb__ul_blv_svt_dgb">
                                    <li dir="ltr" class="li">
                                        <p dir="ltr" class="p">Data Collector now provides a <a class="xref" href="../Configuration/DCConfig.html#task_lxk_kjw_1r" title="You can customize Data Collector by editing the Data Collector configuration file, sdc.properties."><code class="ph codeph">package.manager.repository.links</code>
                                                property</a> to enable specifying alternate
                                            locations for the Package Manager repository.</p>

                                    </li>

                                    <li dir="ltr" class="li">
                                        <p dir="ltr" class="p">The Package Manager now displays the <a class="xref" href="https://streamsets.com/documentation/datacollector/3.7.0/help/datacollector/UserGuide/Installation/AddtionalStageLibs.html#concept_h5k_jbl_nx" target="_blank"><u class="ph u">list of
                                                stages</u></a> associated with each stage
                                            library.</p>

                                    </li>

                                </ul>
</li>

                            <li class="li">Data Collector logging - Data Collector now logs the stage instance
                                that generates each log line.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Stage Libraries</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_fnl_4lt_dgb__ul_kwp_hwt_dgb">
                            <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">New Stage Libraries</a> - This release includes the
                                following new stage libraries:
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="concept_fnl_4lt_dgb__table_j34_d23_2gb" class="table" frame="border" border="1" rules="all"><colgroup><col style="width:50%" /><col style="width:50%" /></colgroup><thead class="thead" style="text-align:left;">
                                            <tr>
                                                <th class="entry cellrowborder" id="d717115e5706">Stage Library Name</th>

                                                <th class="entry cellrowborder" id="d717115e5709">Description</th>

                                            </tr>

                                        </thead>
<tbody class="tbody">
                                            <tr>
                                                <td class="entry cellrowborder" headers="d717115e5706 ">streamsets-datacollector-cdh_kafka_3_1-lib</td>

                                                <td class="entry cellrowborder" headers="d717115e5709 ">Cloudera distribution of Apache Kafka 3.1.0
                                                  (1.0.1).</td>

                                            </tr>

                                            <tr>
                                                <td class="entry cellrowborder" headers="d717115e5706 ">streamsets-datacollector-kinetica_6_2-lib</td>

                                                <td class="entry cellrowborder" headers="d717115e5709 ">For Kinetica 6.2.<p class="p">Includes the KineticaDB
                                                  destination.</p>
</td>

                                            </tr>

                                        </tbody>
</table>
</div>
</li>

                            <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">Updated Stage Libraries</a> - This release includes updates
                                to the following stage libraries:
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="concept_fnl_4lt_dgb__table_s3z_t23_2gb" class="table" frame="border" border="1" rules="all"><colgroup><col style="width:50%" /><col style="width:50%" /></colgroup><thead class="thead" style="text-align:left;">
                                            <tr>
                                                <th class="entry cellrowborder" id="d717115e5756">Stage Library Name</th>

                                                <th class="entry cellrowborder" id="d717115e5759">Description</th>

                                            </tr>

                                        </thead>
<tbody class="tbody">
                                            <tr>
                                                <td class="entry cellrowborder" headers="d717115e5756 ">streamsets-datacollector-apache-pulsar_2-lib</td>

                                                <td class="entry cellrowborder" headers="d717115e5759 ">Apache Pulsar version 2.x.</td>

                                            </tr>

                                            <tr>
                                                <td class="entry cellrowborder" headers="d717115e5756 ">streamsets-datacollector-cdh_6_0-lib</td>

                                                <td class="entry cellrowborder" headers="d717115e5759 ">Cloudera CDH version 6.0.x distribution of
                                                  Apache Hadoop. Now includes the following
                                                  stages:<ul class="ul" id="concept_fnl_4lt_dgb__ul_wbk_v23_2gb">
                                                  <li class="li">HBase Lookup processor</li>

                                                  <li class="li">Spark Evaluator processor</li>

                                                  <li class="li">HBase destination</li>

                                                  </ul>
<div class="p">The stage library no longer includes the
                                                  following stage:<ul class="ul" id="concept_fnl_4lt_dgb__ul_cdp_f2p_3gb">
                                                  <li class="li">Solr destination</li>

                                                  </ul>
</div>
</td>

                                            </tr>

                                        </tbody>
</table>
</div>
</li>

                        </ul>

                    </dd>

                
            </dl>
</div>

    </div>

<article class="topic concept nested2" aria-labelledby="ariaid-title18" id="concept_fq5_zwt_dgb">
    <h3 class="title topictitle3" id="ariaid-title18"><span class="ph">Data Collector Edge</span> New Features and Enhancements</h3>

    <div class="body conbody">
        <div class="p">This <span class="ph">Data Collector Edge</span> (<span class="ph">SDC Edge</span>) version includes new features and enhancements in the following areas:<dl class="dl">
                
                    <dt class="dt dlterm">Technology Preview Functionality</dt>

                    <dd class="dd">The following Technology Preview stage is available for edge pipelines in
                        this release:<ul class="ul" id="concept_fq5_zwt_dgb__ul_szv_kyt_dgb">
                            <li class="li"><a class="xref" href="../Origins/gRPCClient.html#concept_yp1_4zs_yfb">gRPC
                                    Client origin</a> - A new origin that processes data from a
                                gRPC server by calling gRPC server methods. The origin can call
                                unary RPC and server streaming RPC methods. Use this origin only in
                                pipelines configured for edge execution mode.</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Origins in Edge Pipelines</dt>

                    <dd class="dd">This release includes enhancements to the following origins that are
                        supported in edge pipelines: <ul class="ul" id="concept_fq5_zwt_dgb__ul_zms_cb5_dgb">
                            <li class="li"><a class="xref" href="../Edge_Mode/EdgePipelineTypes.html#concept_n2n_r2q_s2b" title="Edge pipelines support a limited number of origins.">File
                                    Tail</a> - The origin can now read multiple sets of files
                                when it is included in edge pipelines. </li>

                            <li class="li"><a class="xref" href="../Origins/WindowsLog.html#concept_ewn_yvp_2gb">Windows Event Log</a> - The origin can now use the Event
                                Logging API or the Windows Event Log API to read data from a
                                Microsoft Windows event log. Microsoft recommends using the newer
                                Windows Event Log API. <p class="p">Previously, the origin used the Event
                                    Logging API only. Upgraded pipelines continue to use the Event
                                    Logging API. </p>
</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Processors in Edge Pipelines</dt>

                    <dd class="dd">Edge pipelines now support the <a class="xref" href="../Edge_Mode/EdgePipelineTypes.html#concept_ih5_s2q_s2b" title="Edge pipelines support a limited number of processors. Processors function the same way in edge pipelines as they do in other pipelines. However, some processors have limitations in edge pipelines as noted below.">Dev Random Error
                            processor</a>.</dd>

                
                
                    <dt class="dt dlterm">Destinations in Edge Pipelines</dt>

                    <dd class="dd">Edge pipelines now support the following <a class="xref" href="../Edge_Mode/EdgePipelineTypes.html#concept_fs1_52q_s2b" title="Edge pipelines support a limited number of destinations.">destinations</a>:<ul class="ul" id="concept_fq5_zwt_dgb__ul_kqc_hb5_dgb">
                            <li class="li">To Error</li>

                            <li class="li">To Event</li>

                            <li class="li">Kinesis Firehose</li>

                            <li class="li">Kinesis Producer</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">SDC Edge as a System Service</dt>

                    <dd class="dd">If you register SDC Edge to <a class="xref" href="../Edge_Mode/SDCEInstall.html#concept_sbg_h4v_gfb" title="After installation, you can register SDC Edge to run as a system service named on the edge device.">run as a
                            system service</a>, you can now run a command as an administrator to
                        display the status of the service.</dd>

                
                
                    <dt class="dt dlterm">SDC Edge Configuration</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_fq5_zwt_dgb__ul_mb2_pb5_dgb">
                            <li class="li"><a class="xref" href="../Edge_Mode/SDCeAdminister.html#concept_hbv_3wl_pbb" title="You can customize SDC Edge by editing the SDC Edge configuration file, &lt;SDCEdge_home&gt;/etc/edge.conf.">Log
                                    file enhancement</a> - You can now modify the default log
                                file directory in the SDC Edge configuration file,
                                    <code class="ph codeph">&lt;SDCEdge_home&gt;/etc/edge.conf</code>. You can no
                                longer modify the default log file directory when you manually start
                                SDC Edge.</li>

                            <li class="li"><a class="xref" href="../Edge_Mode/SDCeAdminister.html#concept_hbv_3wl_pbb" title="You can customize SDC Edge by editing the SDC Edge configuration file, &lt;SDCEdge_home&gt;/etc/edge.conf.">SDC
                                    Edge host name</a> - You can now configure the host name
                                where SDC Edge runs by defining the <code class="ph codeph">base-http-url</code>
                                property in the SDC Edge configuration file. </li>

                        </ul>

                    </dd>

                
            </dl>
</div>

    </div>

</article>
</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title19" id="concept_fmr_1sz_tfb">
    <h2 class="title topictitle2" id="ariaid-title19">What's New in 3.6.0</h2>

    <div class="body conbody">
        <p class="p"><span class="ph">Data Collector</span>
            version 3.6.0 includes the following new features and enhancements:</p>

        <dl class="dl">
            
                <dt class="dt dlterm"><span class="ph">Data Collector Edge</span> (<span class="ph">SDC Edge</span>)</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_fmr_1sz_tfb__ul_j3z_gsz_tfb">
                        <li class="li"><a class="xref" href="../Edge_Mode/SDCeAdminister.html#concept_nz5_lqr_tfb" title="To use SDC Edge with StreamSets Control Hub, you must register SDC Edge with Control Hub.">Register
                                    <span class="ph">SDC Edge</span> with <span class="ph">Control Hub</span></a> - You can now use the command line to register <span class="ph">SDC Edge</span> with <span class="ph">Control Hub</span>.</li>

                        <li class="li"><a class="xref" href="../Edge_Mode/EdgePipelineTypes.html#concept_i32_2vf_pbb" title="Stages included in edge pipelines can process a limited number of data formats.">Delimited data format</a> - Stages in edge pipelines can now
                            process the delimited data format.</li>

                        <li class="li"><a class="xref" href="../Edge_Mode/EdgePipelineTypes.html#concept_pbm_q4r_4bb" title="Edge pipelines run on SDC Edge which is a lightweight agent without a UI. As a result, some features available for standalone pipelines are not available for edge pipelines at this time. We will provide support for some of these features in edge pipelines in a future release.">Functions</a> - The <code class="ph codeph">sdc:hostname()</code> function can
                            now return the host name of a <span class="ph">Data Collector</span> or <span class="ph">Data Collector Edge</span> machine and can be used within edge pipelines.</li>

                    </ul>

                </dd>

            
        </dl>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title20" id="concept_rs3_c31_2fb">
    <h2 class="title topictitle2" id="ariaid-title20">What's New in 3.5.0</h2>

    <div class="body conbody">
        <p class="p"><span class="ph">Data Collector</span>
            version 3.5.0 includes the following new features and enhancements:</p>

        <dl class="dl">
            
                <dt class="dt dlterm">Origins</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_rs3_c31_2fb__ul_bb4_h31_2fb">
                        <li class="li"><a class="xref" href="../Origins/PulsarConsumer.html#concept_o2b_1pc_r2b">New
                                Pulsar Consumer origin</a> - A new origin that reads messages
                            from one or more topics in an Apache Pulsar cluster.</li>

                        <li class="li">JDBC Multitable Consumer and JDBC Query Consumer origin enhancements -
                            These origins now include an option to convert timestamp data to the
                            String data type instead of to the Datetime data type to ensure that the
                            precision is maintained.</li>

                        <li class="li">Salesforce origin enhancement - When using the Bulk API, the origin can
                            now execute an SOQL query that includes one or more subqueries.</li>

                        <li class="li"><a class="xref" href="../Origins/WebSocketClient.html#concept_mhb_sbv_s2b">WebSocket Client</a> and <a class="xref" href="../Origins/WebSocketServer.html#concept_mhb_sbv_s2b">WebSocket Server</a> origin enhancements - When included in a
                            microservice pipeline, the origins can now send responses back to the
                            originating endpoint when used with microservice destinations in the
                            same pipeline.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Processors</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_rs3_c31_2fb__ul_z1w_s31_2fb">
                        <li class="li"><a class="xref" href="../Processors/EncryptDecrypt.html#concept_zs3_vfk_hfb">New
                                Encrypt and Decrypt Fields processor</a> - A new processor that
                            encrypts or decrypts individual field values.</li>

                        <li class="li"><a class="xref" href="../Processors/MongoDBLookup.html#concept_rrp_t4w_2fb">New
                                MongoDB Lookup processor</a> - A new processor that performs
                            lookups in MongoDB and passes all values from the returned document to a
                            new list-map field. Use the MongoDB Lookup to enrich records with
                            additional data.</li>

                        <li class="li"><a class="xref" href="../Processors/HTTPRouter.html#concept_ghx_ypr_fw">New HTTP
                                Router processor</a> - A new processor that passes records to
                            streams based on the HTTP method and URL path in record header
                            attributes. Use the HTTP Router processor in pipelines with an origin
                            that creates HTTP method and path record header attributes - including
                            the HTTP Server origin and the REST Service origin.</li>

                        <li class="li"><a class="xref" href="../Processors/FieldTypeConverter.html#concept_is3_zkp_wq">Field Type Converter
                                processor enhancement</a> - The processor can now convert the
                            Boolean data type to the Integer, Long, or Short data type.</li>

                        <li class="li"><a class="xref" href="../Processors/SalesforceLookup.html#concept_k23_3rk_yx">Salesforce Lookup
                                processor enhancements</a> - The processor includes the following
                                enhancements:<ul class="ul" id="concept_rs3_c31_2fb__ul_iwq_fj1_2fb">
                                <li class="li">The processor can now return multiple values. You can configure
                                    the lookup to return the first value or to return all matches as
                                    separate records.</li>

                                <li class="li">You can now configure how the processor handles a lookup that
                                    returns no value in fields with no default value defined.
                                    Upgraded pipelines continue to send records with no return value
                                    and no default value to error. </li>

                            </ul>
</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Destinations</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_rs3_c31_2fb__ul_krl_kj1_2fb">
                        <li class="li"><a class="xref" href="../Destinations/PulsarProducer.html#concept_fq3_kpc_r2b">New
                                Pulsar Producer destination</a> - A new destination that writes
                            data to topics in an Apache Pulsar cluster.</li>

                        <li class="li"><a class="xref" href="../Destinations/Syslog.html#concept_idr_ct5_w2b">New Syslog
                                destination</a> - A new destination that writes data to a Syslog
                            server.</li>

                        <li class="li"><a class="xref" href="../Destinations/HTTPClient.html#concept_fvd_cg2_rfb">HTTP Client</a>, <a class="xref" href="../Destinations/KProducer.html#concept_jlq_btd_rfb">Kafka Producer</a>, and <a class="xref" href="../Destinations/KinProducer.html#concept_o2w_gf2_rfb">Kinesis Producer</a> destination enhancements - When included in
                            a microservice pipeline, the destinations can now send responses to the
                            microservice origin in the pipeline.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Executors</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_rs3_c31_2fb__ul_xbm_sj1_2fb">
                        <li class="li"><a class="xref" href="../Executors/Databricks.html#concept_fdc_qrx_jz">New
                                Databricks Job Launcher executor</a> - A new executor that starts
                            a Databricks job each time it receives an event.<p class="p">With the addition of
                                this new executor, <span class="ph">Data Collector</span> has removed the ability to use the Spark executor with
                                Databricks. If you upgrade pipelines that include the Spark executor
                                with Databricks, you must update the pipeline to use the Databricks
                                Job Launcher executor after you upgrade.</p>
</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Hive Stages</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_rs3_c31_2fb__ul_wc1_zj1_2fb">
                        <li class="li">JDBC Credentials - The following Hive stages now allow you to enter
                            credentials separately from the JDBC URL for Hive: <ul class="ul" id="concept_rs3_c31_2fb__ul_spn_zj1_2fb">
                                <li class="li">Hive Metadata processor</li>

                                <li class="li">Hive Metastore destination</li>

                                <li class="li">Hive Query executor</li>

                            </ul>
</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Salesforce Stages</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_rs3_c31_2fb__ul_ifm_2k1_2fb">
                        <li class="li">API version - Data Collector now ships with version 43.0 of the
                            Salesforce Web Services Connector libraries used by the following
                            Salesforce stages:<ul class="ul" id="concept_rs3_c31_2fb__ul_qjy_2k1_2fb">
                                <li class="li"><a class="xref" href="../Origins/Salesforce.html#task_ssp_krd_dy">Salesforce origin</a></li>

                                <li class="li"><a class="xref" href="../Processors/SalesforceLookup.html#task_rjz_lck_fy">Salesforce Lookup processor</a></li>

                                <li class="li"><a class="xref" href="../Destinations/WaveAnalytics.html#task_kff_4vn_jz">Einstein Analytics destination</a></li>

                                <li class="li"><a class="xref" href="../Destinations/Salesforce.html#task_es3_25d_dy">Salesforce destination</a></li>

                            </ul>
</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Technology Preview Functionality</dt>

                <dd class="dd">
                    <p class="p"><span class="ph">Data Collector</span> now includes certain new features and stages with the Technology Preview
                        designation. <a class="xref" href="../Pipeline_Design/TechPreview.html#concept_prl_qfv_gfb" title="Data Collector includes certain new features and stages with the Technology Preview designation. Technology Preview functionality is available for use in development and testing, but is not meant for use in production.">Technology Preview functionality</a> is available for use in
                        development and testing, but is not meant for use in production.</p>

                    <p class="p">Technology Preview stages display a Technology Preview icon on the upper left
                        corner of the preview stage, as follows:</p>

                    <p class="p"><img class="image" id="concept_rs3_c31_2fb__image_exx_2mw_gfb" src="../Graphics/TechPreview-inStage.png" height="92" width="130" /></p>

                    <p class="p">When Technology Preview functionality becomes approved for use in production,
                        the release notes and documentation reflect the change, and the Technology
                        Preview icon is removed from the UI.</p>

                    <div class="p">The following Technology Preview stages are available in this release:<ul class="ul" id="concept_rs3_c31_2fb__ul_jpj_2bw_gfb">
                            <li class="li"><a class="xref" href="../Processors/DatabricksML.html#concept_nlz_k3v_gfb">Databricks ML Evaluator processor</a> - A new processor that
                                uses a machine learning model exported with Databricks ML Model
                                Export to generate evaluations, scoring, or classifications of
                                data.</li>

                            <li class="li"><a class="xref" href="../Processors/MLeap.html#concept_wnr_wlv_gfb">MLeap
                                    Evaluator processor</a> - A new processor that uses a machine
                                learning model stored in an MLeap bundle to generate evaluations,
                                scoring, or classifications of data. </li>

                            <li class="li"><a class="xref" href="../Processors/PMML.html#concept_r3s_3fv_gfb">PMML
                                    Evaluator processor</a> - A new processor that uses a machine
                                learning model stored in the Predictive Model Markup Language (PMML)
                                format to generate predictions or classifications of data.</li>

                            <li class="li"><a class="xref" href="../Processors/TensorFlow.html#concept_otg_csh_z2b">TensorFlow Evaluator processor</a> - A new processor that
                                uses TensorFlow machine learning models to generate predictions or
                                classifications of data. </li>

                        </ul>
</div>

                </dd>

            
            
                <dt class="dt dlterm">Data Formats</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_rs3_c31_2fb__ul_vxz_pk1_2fb">
                        <li class="li">Delimited data format enhancement - When reading delimited data that
                            contains headers with empty values, <span class="ph">Data Collector</span> now replaces the empty values with the string empty- plus the
                            column number starting from zero. For example, if the 3rd column header
                            is empty, then the field name in <span class="ph">Data Collector</span> becomes empty-2. Previously, <span class="ph">Data Collector</span> retained the empty field name.</li>

                        <li class="li">Excel data format enhancement - When reading Excel data, <span class="ph">Data Collector</span> now processes the underlying raw values for numeric columns in a
                            spreadsheet, rather than the displayed values. For example, if a cell
                            contains 3.14159 but the display format is set to 2 decimals such that
                            the spreadsheet displays 3.14, <span class="ph">Data Collector</span> still processes the full value of 3.14159. Previously, <span class="ph">Data Collector</span> encountered errors when processing an Excel spreadsheet that
                            contained displayed values.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm"><span class="ph">Data Collector Edge</span> (<span class="ph">SDC Edge</span>)</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_rs3_c31_2fb__ul_pfy_hl1_2fb">
                        <li class="li"><a class="xref" href="../Edge_Mode/SDCEInstall.html#task_adb_x5k_pbb" title="You can download the SDC Edge executable from the StreamSets website as a tarball, ZIP file, or a Windows MSI installer for Windows operating systems.">Download an installer for Windows</a> - You can now download a
                            Microsoft installer to install <span class="ph">SDC Edge</span> on a Windows operating system.</li>

                        <li class="li"><a class="xref" href="../Edge_Mode/SDCEInstall.html#concept_qhp_1th_pbb" title="Download and install SDC Edge on each edge device where you want to run edge pipelines.">Run <span class="ph">SDC Edge</span> as a service</a> - You can now register <span class="ph">SDC Edge</span> to run as a system service on Darwin, Linux, or Windows operating
                            systems.</li>

                        <li class="li"><a class="xref" href="../Origins/SystemMetrics.html#concept_trh_kgh_3fb" title="The System Metrics origin can read metrics from processes running on the edge device. When configured to read process metrics, the origin reads statistics for all running processes by default.">System Metrics origin enhancement</a> - The origin can now read
                            metrics from specific processes running on the edge device. </li>

                        <li class="li"><a class="xref" href="../Origins/WindowsLog.html#concept_agf_5jv_sbb">Windows
                                Event Log origin enhancement</a> - The origin can now read from a
                            custom Windows log.</li>

                        <li class="li"><a class="xref" href="../Edge_Mode/EdgePipelineTypes.html#concept_n2n_r2q_s2b" title="Edge pipelines support a limited number of origins.">Dev Data
                                Generator origin supported</a> - Edge pipelines now support the
                            Dev Data Generator origin.</li>

                        <li class="li"><a class="xref" href="../Edge_Mode/EdgePipelineTypes.html#concept_ih5_s2q_s2b" title="Edge pipelines support a limited number of processors. Processors function the same way in edge pipelines as they do in other pipelines. However, some processors have limitations in edge pipelines as noted below.">TensorFlow
                                Evaluator processor supported</a> - Edge pipelines support the
                            new TensorFlow Evaluator processor.</li>

                        <li class="li"><a class="xref" href="../Edge_Mode/EdgePipelineTypes.html#concept_pbm_q4r_4bb" title="Edge pipelines run on SDC Edge which is a lightweight agent without a UI. As a result, some features available for standalone pipelines are not available for edge pipelines at this time. We will provide support for some of these features in edge pipelines in a future release.">Functions</a> - Edge pipelines now support all job functions and
                            the pipeline:startTime() function.</li>

                        <li class="li"><a class="xref" href="../Edge_Mode/SDCeAdminister.html#concept_hbv_3wl_pbb" title="You can customize SDC Edge by editing the SDC Edge configuration file, &lt;SDCEdge_home&gt;/etc/edge.conf.">Disable
                                the ability to manage production edge pipelines</a> - By default,
                            you can use the <span class="ph">Data Collector</span> UI or REST API to manage edge pipelines deployed to an <span class="ph">SDC Edge</span> - including previewing, validating, starting, stopping, resetting the
                            origin, and monitoring the pipelines. You can now disable the ability to
                            manage edge pipelines in a production environment using the <span class="ph">Data Collector</span> UI or REST API. When disabled, you manage edge pipelines using <span class="ph">Control Hub</span> or by starting pipelines when you start <span class="ph">SDC Edge</span>.</li>

                        <li class="li"><a class="xref" href="../Edge_Mode/SDCeAdminister.html#concept_zfb_lwl_pbb">Skip
                                verifying trusted certificates</a> - In a test or development
                            environment, you can start <span class="ph">SDC Edge</span> so that it skips verifying trusted certificates. For example, you
                            might want to skip verifying trusted certificates when <span class="ph">SDC Edge</span> is registered with a <span class="ph">Control Hub</span> on-premises installation enabled for HTTPS and you want to
                            temporarily avoid configuring the truststore file for <span class="ph">SDC Edge</span>.<span class="ph">StreamSets</span> highly recommends that you configure <span class="ph">SDC Edge</span> to verify trusted certificates in a production environment.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Working with Control Hub</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_rs3_c31_2fb__ul_sft_3m1_2fb">
                        <li class="li">Automate <a class="xref" href="../DPM/RegisterSDCwithDPM.html#concept_js4_2z1_cx" title="For a Data Collector tarball installation, you can register the Data Collector with Control Hub using the Data Collector command line interface. The Data Collector must be running before you can use the command line interface.">registering</a> and <a class="xref" href="../DPM/UnregisterSDCwithDPM.html#concept_r4v_fwf_nx" title="For a Data Collector tarball installation, you can unregister the Data Collector from Control Hub using the Data Collector command line interface.">unregistering</a>
                            <span class="ph">Data Collector</span>s - You can now use an automation tool such as Ansible, Chef, or
                            Puppet to automate the registering and unregistering of <span class="ph">Data Collector</span>s using the following
                            commands:<pre class="pre codeblock"><code>streamsets sch register
streamsets sch unregister</code></pre></li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Microservice Pipelines</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_rs3_c31_2fb__ul_ip4_5m1_2fb">
                        <li class="li">Origins for microservice pipelines - The following origins can now send
                            responses back to the originating REST API client when used with
                            destinations that send records to the origin in the same microservice
                                pipeline:<ul class="ul" id="concept_rs3_c31_2fb__ul_qym_vm1_2fb">
                                <li class="li">WebSocket Client origin</li>

                                <li class="li">WebSocket Server origin</li>

                            </ul>
</li>

                        <li class="li">Destinations for microservice pipelines - The following destinations can
                            now send records to the origin in the microservice pipeline with the
                            specified response:<ul class="ul" id="concept_rs3_c31_2fb__ul_o3b_jn1_2fb">
                                <li class="li">HTTP Client destination</li>

                                <li class="li">Kafka Producer destination</li>

                                <li class="li">Kines Producer destination</li>

                            </ul>
</li>

                        <li class="li">Sample microservice pipeline - When you create a microservice pipeline,
                            the sample microservice pipeline now includes the new HTTP Router
                            processor instead of the Stream Selector processor to route data to
                            different streams based on the request method.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Data Governance Tools</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_rs3_c31_2fb__ul_bqp_241_2fb">
                        <li class="li"><a class="xref" href="../Configuration/PublishMetadata.html#concept_h4w_qld_v1b" title="At this time, only some pipeline stages support publishing metadata to data governance tools.">Supported stages</a> - <span class="ph">Data Collector</span> can now publish metadata to data governance tools for the following
                                stages:<ul class="ul" id="concept_rs3_c31_2fb__ul_izx_h41_2fb">
                                <li class="li">Amazon S3 origin</li>

                                <li class="li">Kafka Multitopic Consumer origin</li>

                                <li class="li">SFTP/FTP Client origin</li>

                                <li class="li">Kafka Producer destination</li>

                            </ul>
</li>

                        <li class="li"><a class="xref" href="../Configuration/PublishMetadata.html#concept_iz5_ptt_p1b">Cloudera Navigator versions</a> - <span class="ph">Data Collector</span> can now publish metadata to Cloudera Navigator running on Cloudera
                            Manager versions 5.10 to 5.15.<p class="p">Previously, publishing metadata to
                                Cloudera Navigator was supported only on Cloudera Manager version
                                5.10 or 5.11.</p>
</li>

                        <li class="li"><a class="xref" href="../Configuration/PublishMetadata.html#task_e12_xyt_p1b" title="Add lineage publisher properties to the Data Collector configuration file, $SDC_CONF/sdc.properties, and then configure the properties as needed. When administering Data Collector with Cloudera Manager, configure the Data Collector configuration properties through the StreamSets service in Cloudera Manager. Manual changes to the configuration file can be overwritten by Cloudera Manager.">Secure connections to Cloudera Navigator</a> - If Cloudera
                            Navigator is configured for TLS/SSL, <span class="ph">Data Collector</span> requires a local truststore file to verify the identity of the
                            Cloudera Navigator Metadata Server. You now configure the truststore
                            file location and password in the
                                <span class="ph filepath">$SDC_CONF/sdc.properties</span> file when you
                            configure the connection to Cloudera Navigator. </li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Credential Stores</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_rs3_c31_2fb__ul_wg2_541_2fb">
                        <li class="li"><a class="xref" href="../Configuration/CredentialStores.html#concept_t3k_ct3_cfb">New Microsoft Azure Key Vault credential store</a> - You can now
                            define credentials in Microsoft Azure Key Vault and then use the <span class="ph">Data Collector</span> credential functions in stage properties to retrieve those
                            values.</li>

                        <li class="li"><a class="xref" href="../Configuration/CredentialStores.html#concept_wxl_44k_cfb" title="The stagelib-cli jks-credentialstore command provides subcommands to add, list, and delete secrets in the Java keystore credential store.">Commands for a Java keystore credential store</a> - You now use
                            the <code class="ph codeph">stagelib-cli jks-credentialstore</code> command to add,
                            list, and delete credentials in a Java keystore credential store.
                            Previously you used the <code class="ph codeph">jks-cs</code> command, which has now
                            been deprecated.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Expression Language</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_rs3_c31_2fb__ul_mck_bp1_2fb">
                        <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_ahp_f4v_1r">String
                                functions</a> - This release includes the following new
                                function:<ul class="ul" id="concept_rs3_c31_2fb__ul_bjd_dp1_2fb">
                                <li class="li">str:split() - Splits a string into a list of string values.</li>

                            </ul>
</li>

                        <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_dvg_nqn_wx">Pipeline
                                functions</a> - This release includes the following new
                                function:<ul class="ul" id="concept_rs3_c31_2fb__ul_qtt_2p1_2fb">
                                <li class="li">pipeline:startTime() - Returns the start time of the pipeline as
                                    a Datetime value.</li>

                            </ul>
</li>

                        <li class="li"><a class="xref" href="../Expression_Language/Functions.html#unique_11657424">Job
                                functions</a> - This release includes the following new
                                functions:<ul class="ul" id="concept_rs3_c31_2fb__ul_jcz_gp1_2fb">
                                <li class="li">job:id() - Returns the ID of the job if the pipeline was run
                                    from a <span class="ph">Control Hub</span> job. Otherwise, returns "UNDEFINED".</li>

                                <li class="li">job:name() - Returns the name of the job if the pipeline was run
                                    from a <span class="ph">Control Hub</span> job. Otherwise, returns "UNDEFINED".</li>

                                <li class="li">job:startTime() - Returns the start time of the job if the
                                    pipeline was run from a <span class="ph">Control Hub</span> job. Otherwise, returns the start time of the pipeline.</li>

                                <li class="li">job:user() - Returns the user who started the job if the
                                    pipeline was run from a <span class="ph">Control Hub</span> job. Otherwise, returns "UNDEFINED".</li>

                            </ul>
</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Stage Libraries</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_rs3_c31_2fb__ul_h3g_wp1_2fb">
                        <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">New stage libraries</a> - This release includes the following
                            new stage libraries:
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="concept_rs3_c31_2fb__table_vm3_zp1_2fb" class="table" frame="border" border="1" rules="all"><colgroup><col style="width:50%" /><col style="width:50%" /></colgroup><thead class="thead" style="text-align:left;">
                                        <tr>
                                            <th class="entry cellrowborder" id="d717115e6901">Stage Library Name</th>

                                            <th class="entry cellrowborder" id="d717115e6904">Description</th>

                                        </tr>

                                    </thead>
<tbody class="tbody">
                                        <tr>
                                            <td class="entry cellrowborder" headers="d717115e6901 ">streamsets-datacollector-apache-kafka_1_1-lib</td>

                                            <td class="entry cellrowborder" headers="d717115e6904 ">Apache Kafka version 1.1.x</td>

                                        </tr>

                                        <tr>
                                            <td class="entry cellrowborder" headers="d717115e6901 ">streamsets-datacollector-apache-kafka_2_0-lib</td>

                                            <td class="entry cellrowborder" headers="d717115e6904 ">Apache Kafka version 2.0.x</td>

                                        </tr>

                                        <tr>
                                            <td class="entry cellrowborder" headers="d717115e6901 ">streamsets-datacollector-apache-pulsar_2-lib</td>

                                            <td class="entry cellrowborder" headers="d717115e6904 ">Apache Pulsar version 2.1.0-incubating</td>

                                        </tr>

                                        <tr>
                                            <td class="entry cellrowborder" headers="d717115e6901 ">streamsets-datacollector-azure-keyvault-credentialstore-lib</td>

                                            <td class="entry cellrowborder" headers="d717115e6904 ">Microsoft Azure Key Vault credential store
                                                system</td>

                                        </tr>

                                        <tr>
                                            <td class="entry cellrowborder" headers="d717115e6901 ">streamsets-datacollector-cdh_6_0-lib</td>

                                            <td class="entry cellrowborder" headers="d717115e6904 ">Cloudera CDH version 6.0 distribution of Apache
                                                  Hadoop<div class="note note"><span class="notetitle">Note:</span> Does not include the following
                                                  stages:<ul class="ul" id="concept_rs3_c31_2fb__ul_slh_npx_2fb">
                                                  <li class="li">HBase Lookup processor</li>

                                                  <li class="li">Spark Evaluator processor</li>

                                                  <li class="li">HBase destination</li>

                                                  </ul>
</div>
</td>

                                        </tr>

                                        <tr>
                                            <td class="entry cellrowborder" headers="d717115e6901 ">streamsets-datacollector-crypto-lib</td>

                                            <td class="entry cellrowborder" headers="d717115e6904 ">For cryptography stages, including the Encrypt
                                                and Decrypt Fields processor</td>

                                        </tr>

                                        <tr>
                                            <td class="entry cellrowborder" headers="d717115e6901 ">streamsets-datacollector-mapr_6_0-mep5-lib</td>

                                            <td class="entry cellrowborder" headers="d717115e6904 ">MapR Ecosystem Pack (MEP) version 5 for MapR
                                                6.0.1</td>

                                        </tr>

                                        <tr>
                                            <td class="entry cellrowborder" headers="d717115e6901 ">streamsets-datacollector-tensorflow-lib</td>

                                            <td class="entry cellrowborder" headers="d717115e6904 ">TensorFlow</td>

                                        </tr>

                                    </tbody>
</table>
</div>
</li>

                        <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_fw3_zt3_tbb">Legacy stage libraries</a> - The following stage libraries are
                            now legacy stage libraries:
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="concept_rs3_c31_2fb__table_am4_nq1_2fb" class="table" frame="border" border="1" rules="all"><colgroup><col style="width:50%" /><col style="width:50%" /></colgroup><thead class="thead" style="text-align:left;">
                                        <tr>
                                            <th class="entry cellrowborder" id="d717115e7017">Stage Library Name</th>

                                            <th class="entry cellrowborder" id="d717115e7020">Description</th>

                                        </tr>

                                    </thead>
<tbody class="tbody">
                                        <tr>
                                            <td class="entry cellrowborder" headers="d717115e7017 ">streamsets-datacollector-apache-kafka_0_9-lib</td>

                                            <td class="entry cellrowborder" headers="d717115e7020 ">Apache Kafka version 0.9.x</td>

                                        </tr>

                                        <tr>
                                            <td class="entry cellrowborder" headers="d717115e7017 ">streamsets-datacollector-apache-kafka_0_10-lib</td>

                                            <td class="entry cellrowborder" headers="d717115e7020 ">Apache Kafka version 0.10.x</td>

                                        </tr>

                                        <tr>
                                            <td class="entry cellrowborder" headers="d717115e7017 ">streamsets-datacollector-cdh_5_8-lib</td>

                                            <td class="entry cellrowborder" headers="d717115e7020 ">Cloudera CDH version 5.8 distribution of Apache
                                                Hadoop</td>

                                        </tr>

                                        <tr>
                                            <td class="entry cellrowborder" headers="d717115e7017 ">streamsets-datacollector-cdh_5_9-lib</td>

                                            <td class="entry cellrowborder" headers="d717115e7020 ">Cloudera CDH version 5.9 distribution of Apache
                                                Hadoop</td>

                                        </tr>

                                        <tr>
                                            <td class="entry cellrowborder" headers="d717115e7017 ">streamsets-datacollector-cdh_kafka_2_0-lib</td>

                                            <td class="entry cellrowborder" headers="d717115e7020 ">Cloudera distribution of Apache Kafka 2.0.x
                                                (0.9.0)</td>

                                        </tr>

                                        <tr>
                                            <td class="entry cellrowborder" headers="d717115e7017 ">streamsets-datacollector-hdp_2_4-lib</td>

                                            <td class="entry cellrowborder" headers="d717115e7020 ">Hortonworks version 2.4 distribution of Apache
                                                Hadoop</td>

                                        </tr>

                                        <tr>
                                            <td class="entry cellrowborder" headers="d717115e7017 ">streamsets-datacollector-hdp_2_4-hive1-lib</td>

                                            <td class="entry cellrowborder" headers="d717115e7020 ">Hortonworks version 2.4.x distribution of Apache
                                                Hive version 1.x</td>

                                        </tr>

                                        <tr>
                                            <td class="entry cellrowborder" headers="d717115e7017 ">streamsets-datacollector-hdp_2_5-lib</td>

                                            <td class="entry cellrowborder" headers="d717115e7020 ">Hortonworks version 2.5.x distribution of Apache
                                                Hadoop</td>

                                        </tr>

                                        <tr>
                                            <td class="entry cellrowborder" headers="d717115e7017 ">streamsets-datacollector-hdp_2_5-flume-lib</td>

                                            <td class="entry cellrowborder" headers="d717115e7020 ">Hortonworks version 2.5.x distribution of Apache
                                                Flume</td>

                                        </tr>

                                        <tr>
                                            <td class="entry cellrowborder" headers="d717115e7017 ">streamsets-datacollector-mapr_5_1-lib</td>

                                            <td class="entry cellrowborder" headers="d717115e7020 ">MapR version 5.1</td>

                                        </tr>

                                    </tbody>
</table>
</div>
<p class="p">Legacy stage libraries that are more than two years old are
                                not included with <span class="ph">Data Collector</span>. Though not recommended, you can still download and install the
                                older stage libraries as custom stage libraries.</p>
<p class="p">If you have
                                pipelines that use these legacy stage libraries, you will need to
                                update the pipelines to use a more current stage library or install
                                the legacy stage library manually. For more information see <a class="xref" href="../Upgrade/PostUpgrade.html#concept_wnl_zk4_5bb">Update Pipelines using Legacy Stage Libraries</a>.</p>
</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Miscellaneous</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_rs3_c31_2fb__ul_bgj_3r1_2fb">
                        <li class="li"><a class="xref" href="../Pipeline_Maintenance/PipelineMaintenance_title.html#task_uyf_gz1_qfb">Import pipelines from an external HTTP URL</a> - You can now use
                                <span class="ph">Data Collector</span> to import pipelines from an external HTTP URL. For example, you can
                            import pipelines from the <span class="ph">StreamSets</span> GitHub repository.</li>

                        <li class="li">Collection of usage statistics - When you log in to <span class="ph">Data Collector</span> as the admin/admin user for the first time, you can now choose to
                            improve <span class="ph">Data Collector</span> by sending anonymized usage data. Previously, the
                            ui.enable.usage.data.collection property in the <span class="ph">Data Collector</span> configuration file determined whether usage data was collected. This
                            property has been removed.</li>

                    </ul>

                </dd>

            
        </dl>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title21" id="concept_ynr_5lt_m2b">
    <h2 class="title topictitle2" id="ariaid-title21">What's New in 3.4.0</h2>

    <div class="body conbody">
        <p class="p"><span class="ph">Data Collector</span>
            version 3.4.0 includes the following new features and enhancements:</p>

        <dl class="dl">
            
                <dt class="dt dlterm">Origins</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ynr_5lt_m2b__ul_ohb_jmt_m2b">
                        <li class="li"><a class="xref" href="../Origins/PostgreSQL.html#concept_cfs_4m4_n2b">New
                                PostgreSQL CDC Client origin</a> - Use the PostgreSQL CDC Client
                            origin to process change data capture information for a PostgreSQL
                            database.</li>

                        <li class="li"><a class="xref" href="../Pipeline_Design/TestOrigin.html#concept_sgt_s5v_g2b" title="A test origin can provide test data for data preview to aid in pipeline development. In Control Hub, you can also use test origins when developing pipeline fragments. Test origins are not used when running a pipeline.">New
                                Test origin</a>- You can now configure a virtual test origin to
                            provide test data for data preview to aid in pipeline development. In
                                <span class="ph">Control Hub</span>, you can also use test origins when developing pipeline fragments. </li>

                        <li class="li">Amazon S3, Directory, SFTP/FTP Client, Google Cloud Storage enhancements
                            - The listed origins can now process <a class="xref" href="../Data_Formats/Excel.html#concept_w1z_zc1_22b">Microsoft
                                Excel files</a>.</li>

                        <li class="li"><a class="xref" href="../Pipeline_Design/DevStages.html#concept_czx_ktn_ht">Dev
                                Data Generator origin enhancement</a> - The development origin
                            can now generate additional types of data for testing purposes - such as
                            sample address data, names, or prices.</li>

                        <li class="li">Hadoop FS origin enhancements - The origin includes the following
                                enhancements:<ul class="ul" id="concept_ynr_5lt_m2b__ul_ssl_ymt_m2b">
                                <li class="li">Process Amazon S3 data in <a class="xref" href="../Cluster_Mode/AmazonS3Requirements.html#concept_opj_jmf_f2b" title="Cluster EMR batch and cluster batch mode pipelines can process data from Amazon S3.">cluster EMR batch mode</a> - Use the origin in a cluster
                                    EMR batch pipeline that runs on an Amazon EMR cluster to process
                                    data from Amazon S3. </li>

                                <li class="li">Process Amazon S3 data in <a class="xref" href="../Cluster_Mode/AmazonS3Requirements.html#concept_opj_jmf_f2b" title="Cluster EMR batch and cluster batch mode pipelines can process data from Amazon S3.">cluster batch mode</a> - Use the origin in a cluster
                                    batch pipeline that runs on a Cloudera distribution of Hadoop
                                    (CDH) or Hortonworks Data Platform (HDP) cluster to process data
                                    from Amazon S3.</li>

                            </ul>
</li>

                        <li class="li">HTTP Client origin enhancements - The origin includes the following
                            changes and enhancements:<ul class="ul" id="concept_ynr_5lt_m2b__ul_off_lnt_m2b">
                                <li class="li">The origin now uses <a class="xref" href="../Origins/HTTPClient.html#task_akl_rkz_5r">buffered
                                        request transfer encoding</a> by default. Upgraded
                                    pipelines retain their previous configuration.</li>

                                <li class="li"><a class="xref" href="../Origins/HTTPClient.html#concept_etl_bsh_l2b" title="The HTTP Client origin generates records based on the responses it receives.">HEAD
                                        request responses create an empty record</a>. Information
                                    returned from the HEAD appear in record header attributes.</li>

                            </ul>
</li>

                        <li class="li"><a class="xref" href="../Origins/HTTPServer.html#concept_rvj_5qy_4cb">HTTP Server
                                origin enhancement</a> - The origin now includes the name of the
                            client or proxy that made the request in the remoteHost record header
                            attribute.</li>

                        <li class="li"><a class="xref" href="../Origins/MongoDB.html#concept_kx3_zrs_ns">MongoDB origin enhancement</a> - You can now use a date field as
                            the offset field.</li>

                        <li class="li">Oracle CDC Client origin enhancements - The origin includes the
                            following changes and enhancements:<ul class="ul" id="concept_ynr_5lt_m2b__ul_esg_f4t_m2b">
                                <li class="li"><a class="xref" href="../Origins/OracleCDC.html#concept_syx_pjy_12b" title="When you configure the origin to use local buffering and to parse the SQL query, you can configure the Oracle CDC Client origin to use multiple threads to parse transactions. You can use multithreaded parsing with both the default Oracle CDC Client parser and the alternate PEG parser.">Multithreaded parsing</a> - When using local caching and
                                    parsing the SQL query, the origin can now use multiple threads
                                    to parse transactions. </li>

                                <li class="li"><a class="xref" href="../Origins/OracleCDC.html#concept_jy5_dyd_12b" title="The Oracle CDC Client origin provides an alternate PEG parser that you can try when concerned about pipeline performance.">PEG Parser</a> - To improve performance for very wide
                                    tables, you can try our experimental PEG parser.</li>

                                <li class="li">With this release, the Query Timeout property has been removed.
                                    You can no longer configure a query to timeout before the end of
                                    a LogMiner session. The existing LogMiner Session Window
                                    property defines how long the session lasts. </li>

                            </ul>
</li>

                        <li class="li">Salesforce origin enhancement - When using the SOAP API, the origin can
                            now execute an SOQL query that includes one or more subqueries. Support
                            for subqueries using the Bulk API will be added in a future release.
                        </li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Processors</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ynr_5lt_m2b__ul_fxf_s4t_m2b">
                        <li class="li"><a class="xref" href="../Processors/WholeFileTransformer.html#concept_nwg_rx4_l2b">New Whole File Transformer processor</a> - Use the Whole File
                            Transformer processor to convert fully written Avro files to Parquet in
                            a whole file pipeline. </li>

                        <li class="li"><a class="xref" href="../Processors/FieldHasher.html#concept_pmb_sws_f2b" title="You can configure the Field Hasher processor to add a field separator character to the end of all fields to be hashed. You might want to add a field separator character when you hash multiple fields to a single field or when you hash an entire record.">Field Hasher processor enhancement</a> - The processor can now
                            add a user-defined field separator to fields before hashing. </li>

                        <li class="li">HTTP Client processor enhancements - The processor includes the
                            following changes and enhancements:<ul class="ul" id="concept_ynr_5lt_m2b__ul_rhv_y4t_m2b">
                                <li class="li">The processor now uses <a class="xref" href="../Processors/HTTPClient.html#task_z54_1qr_fw" title="Configure an HTTP Client processor to perform requests against a resource URL.">buffered
                                        request transfer encoding</a> by default. Upgraded
                                    pipelines retain their previous configuration.</li>

                                <li class="li"><a class="xref" href="../Processors/HTTPClient.html#concept_yqv_23v_bhb">HEAD request
                                        responses create an empty record</a>. Information
                                    returned from the HEAD appear in record header attributes.</li>

                                <li class="li"><a class="xref" href="../Processors/HTTPClient.html#concept_kjb_gfz_m2b" title="You can write the resolved resource URL to the Data Collector log.">The
                                        resolved request URL</a> is now written to the <span class="ph">Data Collector</span> log when <span class="ph">Data Collector</span> logging is set to debug or higher.</li>

                            </ul>
</li>

                        <li class="li"><a class="xref" href="../Processors/JDBCLookup.html#concept_vsl_dvt_d2b" title="When using local caching, you can increase the number of threads that the JDBC Lookup processor uses to prepopulate the lookup cache. After the cache is populated, the additional threads are released. This can substantially increase the performance of the processor.">JDBC Lookup processor enhancement</a> - When using local
                            caching, the processor can now use additional cores to prepopulate the
                            cache to enhance pipeline performance. </li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Destinations</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ynr_5lt_m2b__ul_ekt_gpt_m2b">
                        <li class="li"><a class="xref" href="../Destinations/Couchbase.html#concept_ahq_1wq_h2b">New
                                Couchbase destination</a> - A new destination that writes data to
                            a Couchbase database. </li>

                        <li class="li"><a class="xref" href="../Destinations/Splunk.html#concept_zzr_pqn_xdb">New Splunk
                                destination</a> - A new destination that writes data to Splunk
                            using the Splunk HTTP Event Collector (HEC).</li>

                        <li class="li"><a class="xref" href="../Destinations/Cassandra.html#task_t1d_z3l_sr">Cassandra destination enhancement</a> - You can now use SSL/TLS
                            to connect to Cassandra.</li>

                        <li class="li"><a class="xref" href="../Destinations/HTTPClient.html#task_bdf_fk5_lz">HTTP Client
                                destination enhancement</a> - The destination now uses buffered
                            request transfer encoding by default. Upgraded pipelines retain their
                            previous configuration. </li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Executors</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ynr_5lt_m2b__ul_qgm_ppt_m2b">
                        <li class="li">Amazon S3 executor enhancements - The executor includes the following
                                enhancements:<ul class="ul" id="concept_ynr_5lt_m2b__ul_fbw_ppt_m2b">
                                <li class="li">The executor can now <a class="xref" href="../Executors/AmazonS3.html#concept_v5j_pjr_f2b" title="You can use the Amazon S3 executor to copy an object to another location within the same bucket when the executor receives an event record. You can optionally delete the original object after the copy. The object must be under 5 GB in size.">copy objects</a> to a new location and optionally delete
                                    the original object.</li>

                                <li class="li">The executor can now <a class="xref" href="../Executors/AmazonS3.html#concept_jv4_12x_gjb">generate event records</a> each time the executor
                                    creates a new object, adds tags to an existing object, or
                                    completes copying an object to a new location.</li>

                            </ul>
</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm"><span class="ph">Data Collector Edge</span> (<span class="ph">SDC Edge</span>)</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ynr_5lt_m2b__ul_osl_5pt_m2b">
                        <li class="li"><a class="xref" href="../Origins/SystemMetrics.html#concept_gzy_gmv_32b">New
                                System Metrics origin</a> - A new origin that reads system
                            metrics - such as CPU and memory usage - from the edge device where <span class="ph">SDC Edge</span> is installed.</li>

                        <li class="li"><a class="xref" href="../Edge_Mode/EdgePipelineTypes.html#concept_n2n_r2q_s2b" title="Edge pipelines support a limited number of origins.">HTTP Client
                                origin supported</a> - Edge sending pipelines now support the
                            HTTP Client origin. However, the origin does not currently support batch
                            processing mode, pagination, or OAuth2 authorization in edge
                            pipelines.</li>

                        <li class="li"><a class="xref" href="../Edge_Mode/EdgePipelineTypes.html#concept_n2n_r2q_s2b" title="Edge pipelines support a limited number of origins.">WebSocket
                                Client origin supported</a> - Edge sending pipelines now
                            support the WebSocket Client origin.</li>

                        <li class="li"><a class="xref" href="../Edge_Mode/EdgePipelineTypes.html#concept_pbm_q4r_4bb" title="Edge pipelines run on SDC Edge which is a lightweight agent without a UI. As a result, some features available for standalone pipelines are not available for edge pipelines at this time. We will provide support for some of these features in edge pipelines in a future release.">Pipeline functions</a> - Edge pipelines now support the
                            following pipeline functions:<ul class="ul" id="concept_ynr_5lt_m2b__ul_ezh_3qt_m2b">
                                <li class="li">pipeline:id()</li>

                                <li class="li">pipeline:title()</li>

                                <li class="li">pipeline:user()</li>

                            </ul>
</li>

                        <li class="li"><a class="xref" href="../Edge_Mode/EdgePipelines_Manage.html#concept_tqk_dbb_4db" title="After designing edge pipelines in Data Collector and then deploying the edge pipelines to SDC Edge, you can manage the pipelines on SDC Edge. Managing edge pipelines includes previewing, validating, starting, stopping, and monitoring the pipelines as well as resetting the origin for the pipelines.">Preview and validate edge pipelines</a> - You can now use the
                                <span class="ph">Data Collector</span> UI or the command line and REST API to preview and validate edge
                            pipelines.</li>

                        <li class="li"><a class="xref" href="../Edge_Mode/EdgePipelines_Deploy.html#task_obt_d25_32b">Publish
                                multiple edge pipelines to <span class="ph">SDC Edge</span></a> - You can now use the <span class="ph">Data Collector</span> Home page to directly publish multiple edge pipelines at one time to
                            an <span class="ph">SDC Edge</span> that is running. Previously, you could only publish a single edge
                            pipeline at a time.</li>

                        <li class="li"><a class="xref" href="../Edge_Mode/DownloadPipelines.html#task_vsg_wf5_32b" title="When SDC Edge is running and is accessible by the Data Collector machine, you can download edge pipelines from SDC Edge into Data Collector. When you download pipelines from SDC Edge, you download all edge pipelines deployed to that SDC Edge.">Download edge pipelines from <span class="ph">SDC Edge</span></a> - You can now use the <span class="ph">Data Collector</span> UI to download all edge pipelines deployed to an <span class="ph">SDC Edge</span> in addition to all sample edge pipelines included with <span class="ph">SDC Edge</span>.</li>

                        <li class="li"><a class="xref" href="../Getting_Started/GettingStarted_Title.html#concept_wc5_ysp_1w" title="Data Collector displays a list of all available pipelines and related information on the Home page. You can select a category of pipelines, such as Running Pipelines, to view a subset of all available pipelines.">Filter the Home page by edge pipelines</a> - You can now select
                            Edge Pipelines as a category on the <span class="ph">Data Collector</span> Home page to view all available edge pipelines. </li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Microservice pipelines</dt>

                <dd class="dd">You can now create microservices using <a class="xref" href="../Microservice/Microservice_Title.html#concept_qfh_xdm_p2b" title="A microservice pipeline is a pipeline that creates a fine-grained service to perform a specific task.">microservice
                        pipelines</a>. Use the following new stages in microservice pipelines: <ul class="ul">
                        <li class="li"><a class="xref" href="../Origins/RESTService.html#concept_hfg_2sn_p2b">New REST
                                Service origin</a> - <span class="ph"><span class="ph" id="concept_ynr_5lt_m2b__d245e2477">Listens on an HTTP
                              endpoint, parses the contents of all authorized requests, and sends
                              responses back to the originating REST API.</span> Creates multiple
                        threads to enable parallel processing in a multithreaded pipeline.</span>
                        </li>

                        <li class="li"><a class="xref" href="../Destinations/SendResponse.html#concept_eyd_zx4_q2b">Send
                                Response to Origin</a> destination - <span class="ph">Sends records with the specified response to the
                        microservice origin in the pipeline.</span></li>

                    </ul>
</dd>

            
            
                <dt class="dt dlterm">Pipelines</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ynr_5lt_m2b__ul_nyy_rrt_m2b">
                        <li class="li"><a class="xref" href="../Pipeline_Configuration/Notifications.html#concept_mtn_k4j_rz">Notifications</a> - You can now configure a pipeline to send an
                            email or webhook when the pipeline changes to the Running_Error
                            state.</li>

                        <li class="li"><a class="xref" href="../Pipeline_Design/RecordHeaderAttributes.html#concept_itf_55z_dz">Error records</a> - Error records now include an errorJobID
                            internal header attribute when the pipeline that generated the error
                            record was started by a <span class="ph">Control Hub</span> job.</li>

                        <li class="li"><a class="xref" href="../Configuration/ExternalLibs.html#concept_pdv_qlw_ft">Install external libraries from the properties panel</a> - You
                            can now select a stage in the pipeline canvas and then install external
                            libraries for that stage from the properties panel. Previously, you had
                            to navigate to the Package Manager page to install external
                            libraries.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Cluster Pipelines</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ynr_5lt_m2b__ul_hp3_bst_m2b">
                        <li class="li"><a class="xref" href="../Cluster_Mode/AmazonS3Requirements.html#concept_opj_jmf_f2b" title="Cluster EMR batch and cluster batch mode pipelines can process data from Amazon S3.">New cluster EMR batch mode</a> - <span class="ph">Data Collector</span> can now use the cluster EMR batch mode to run on an Amazon EMR
                            cluster to process data from Amazon S3. <span class="ph">Data Collector</span> runs as an application on top of MapReduce in the EMR cluster.<p class="p"><span class="ph">Data Collector</span> can run on an existing EMR cluster or on a new EMR cluster that
                                is provisioned when the cluster pipeline starts. When you provision
                                a new EMR cluster, you can configure whether the cluster remains
                                active or terminates when the pipeline stops.</p>
<p class="p">Use the Hadoop
                                FS origin to process data from Amazon S3 in cluster EMR batch mode.
                            </p>
</li>

                        <li class="li"><a class="xref" href="../Cluster_Mode/ClusterPipelines.html#concept_iyx_23c_j2b" title="Because cluster pipelines run as either MapReduce or Spark applications, each Data Collectorworker in the cluster manages its own log.">Logs</a> -
                            You can now configure the <span class="ph">Data Collector</span> on the master gateway node to use the log4j rolling file appender to
                            write log messages to an sdc.log file. This configuration is propagated
                            to the worker nodes such that each <span class="ph">Data Collector</span> worker writes log messages to an sdc.log file within the YARN
                            application directory.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Data Formats</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ynr_5lt_m2b__ul_e3w_hst_m2b">
                        <li class="li"><a class="xref" href="../Data_Formats/Excel.html#concept_w1z_zc1_22b">New Excel
                                data format</a> - You can now use the following file-based
                            origins to process Microsoft Excel files:<ul class="ul" id="concept_ynr_5lt_m2b__ul_yyf_jst_m2b">
                                <li class="li">Amazon S3 origin</li>

                                <li class="li">Directory origin</li>

                                <li class="li">Google Cloud Storage origin</li>

                                <li class="li">SFTP/FTP Client origin</li>

                            </ul>
</li>

                        <li class="li">Avro and Protobuf data formats - To preserve the ordering of fields, the
                            Avro and Protobuf data formats now use the list-map root field type
                            instead of the map root field type. </li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Stage Libraries</dt>

                <dd class="dd">
                    <div class="p">This version of <span class="ph">Data Collector</span> includes the following new <a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">stage libraries</a>:<ul class="ul" id="concept_ynr_5lt_m2b__ul_j2y_4st_m2b">
                            <li class="li">streamsets-datacollector-cdh_5_15-lib - The Cloudera CDH 5.15
                                distribution of Hadoop.</li>

                            <li class="li">streamsets-datacollector-emr_hadoop_2_8_3-lib - Includes the Hadoop
                                FS origin for cluster EMR batch mode pipelines that run on an Amazon
                                EMR cluster to process data from Amazon S3.</li>

                        </ul>
</div>

                </dd>

            
            
                <dt class="dt dlterm">Miscellaneous</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ynr_5lt_m2b__ul_osx_rst_m2b">
                        <li class="li">Cloudera Manager CSD enhancement - The Cloudera Manager CSD now enables
                            specifying a StreamSets Customer ID, used when generating support
                            bundles. The customer ID is generated by the StreamSets Support team for
                            users with a paid subscription.</li>

                        <li class="li">Postgres rename - Postgres CSV and Postgres Text delimited format types
                            are now known as PostgreSQL CSV and PostgreSQL Text, respectively The
                            Postgres Metadata processor is now known as the PostgreSQL Metadata
                            processor. And the Drift Synchronization Solution for Postgres is now
                            known as the Drift Synchronization Solution for PostgreSQL.</li>

                        <li class="li">Documentation enhancement - The online help has a new look and feel. All
                            of the previous documentation remains exactly where you expect it, but
                            it is now easier to view and navigate on smaller devices like your
                            tablet or mobile phone. </li>

                    </ul>

                </dd>

            
        </dl>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title22" id="concept_gbv_rcr_h2b">
    <h2 class="title topictitle2" id="ariaid-title22">What's New in 3.3.1</h2>

    <div class="body conbody">
        <div class="p"><span class="ph">Data Collector</span>
            version 3.3.1 includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Origin</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_gbv_rcr_h2b__ul_err_zcr_h2b">
                            <li class="li"><a class="xref" href="../Origins/MultiTableJDBCConsumer.html#concept_rx3_3hx_4y">JDBC Multitable Consumer origin enhancement</a> - You can
                                now optionally define a schema exclusion pattern to exclude some
                                schemas from being read. The schema exclusion pattern uses a
                                Java-based regular expression, or regex.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Processor</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_gbv_rcr_h2b__ul_jky_gdr_h2b">
                            <li class="li"><a class="xref" href="../Processors/KuduLookup.html#task_b5b_dyl_p1b">Kudu Lookup processor enhancements</a>:<ul class="ul" id="concept_gbv_rcr_h2b__ul_j5j_mdr_h2b">
                                    <li class="li">You can now configure the Maximum Number of Worker Threads
                                        property to limit the number of threads that the processor
                                        uses. </li>

                                    <li class="li">You can now configure an Admin Operation Timeout property to
                                        determine how many milliseconds to allow for admin-type
                                        operations, such as opening a table or getting a table
                                        schema.</li>

                                </ul>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Destination</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_gbv_rcr_h2b__ul_chx_4dr_h2b">
                            <li class="li"><a class="xref" href="../Destinations/Kudu.html#task_c4x_tmh_4v">Kudu destination enhancements:</a><ul class="ul" id="concept_gbv_rcr_h2b__ul_rnl_pdr_h2b">
                                    <li class="li">You can now configure the Maximum Number of Worker Threads
                                        property to limit the number of threads that the destination
                                        uses. </li>

                                    <li class="li">You can now configure an Admin Operation Timeout property to
                                        determine how many milliseconds to allow for admin-type
                                        operations, such as opening a table or getting a table
                                        schema.</li>

                                </ul>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Environment Variables</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_gbv_rcr_h2b__ul_mhd_pkr_h2b">
                            <li class="li">Data Collector now includes a SPARK_KAFKA_VERSION environment
                                variable that is set to 0.10 by default in the Data Collector
                                environment configuration file - <code class="ph codeph">sdc.env.sh</code> or
                                    <code class="ph codeph">sdcd.env.sh</code>. Do not change this environment
                                variable value. This variable is used only when you run <a class="xref" href="../Cluster_Mode/KafkaRequirements.html#task_gmd_msw_yr">cluster streaming mode pipelines</a> on a Cloudera CDH
                                cluster.</li>

                        </ul>

                    </dd>

                
            </dl>
</div>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title23" id="concept_k42_pbc_xdb">
    <h2 class="title topictitle2" id="ariaid-title23">What's New in 3.3.0</h2>

    <div class="body conbody">
        <p class="p"><span class="ph">Data Collector</span>
            version 3.3.0 includes the following new features and enhancements:</p>

        <dl class="dl">
            
                <dt class="dt dlterm">Cluster Pipelines</dt>

                <dd class="dd">
                    <div class="p">
                        <ul class="ul" id="concept_k42_pbc_xdb__ul_nvg_jdr_h2b">
                            <li class="li">When using Spark 2.1 or later and Kafka 0.10.0.0 or later in a
                                cluster pipeline that reads from a Kafka cluster on YARN, you can
                                now enable the pipeline to use Kafka security features such as
                                SSL/TLS and Kerberos authentication.</li>

                        </ul>

                    </div>

                </dd>

            
            
                <dt class="dt dlterm">Origins</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_k42_pbc_xdb__ul_zr1_tcc_xdb">
                        <li class="li"><a class="xref" href="../Origins/WebSocketClient.html#task_u4n_rzk_fbb">WebSocket Client Origin enhancement</a> - You can now configure
                            the origin to send an initial message or command after connecting to the
                            WebSocket server.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Processors</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_k42_pbc_xdb__ul_od1_bdc_xdb">
                        <li class="li">New SQL Parser processor - A processor that parses SQL queries. For
                            example, if you set the Parse SQL Query property to false in the Oracle
                            CDC origin, the origin writes the SQL query to an sql field that can
                            be parsed by the SQL Parser.</li>

                        <li class="li"><a class="xref" href="../Processors/FieldZip.html#task_nqj_51k_yx">Field Zip processor enhancement</a> - The Continue option for
                            the Field Does Not Exist property is now named Include without
                            Processing.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Pipelines</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_k42_pbc_xdb__ul_bpq_ndc_xdb">
                        <li class="li"><a class="xref" href="../Pipeline_Configuration/Notifications.html#concept_mtn_k4j_rz">Notifications</a> - You can now configure a pipeline to send an
                            email or webhook when the pipeline changes to the Stop_Error state.</li>

                        <li class="li">Preview - The default value of the <a class="xref" href="../Data_Preview/DataPreview_Title.html#task_cxd_p25_qq">Preview Timeout property</a> has been increased to 30,000
                            milliseconds. Previously the default was 10,000 milliseconds.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Edge Pipelines</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_k42_pbc_xdb__ul_vqx_hdc_xdb">
                        <li class="li"><a class="xref" href="../Pipeline_Design/DevStages.html#concept_czx_ktn_ht">Sensor
                                Reader origin enhancement</a> - This development stage can now
                            generate records with thermal data such as that generated by BCM2835
                            onboard thermal sensors.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Stage Libraries</dt>

                <dd class="dd">
                    <div class="p">
                        <ul class="ul" id="concept_k42_pbc_xdb__ul_xqt_jdr_h2b">
                            <li class="li">
                                <p class="p">This version of Data Collector includes several new, changed, and
                                    removed stage libraries because of the introduction of cluster
                                    streaming mode with support for Kafka security features using
                                    Spark 2.1 or later and Kafka 0.10.0.0 or later. </p>

                                <p class="p">For more information about the changed stage libraries, see <a class="xref" href="../Upgrade/PreUpgrade.html#concept_zgm_vj2_mdb" title="Data Collector version 3.3.0 introduces cluster streaming mode with support for Kafka security features such as SSL/TLS and Kerberos authentication using Spark 2.1 or later and Kafka 0.10.0.0 or later.">Upgrade to Spark 2.1 or Later</a>.</p>

                            </li>

                        </ul>

                    </div>

                </dd>

            
        </dl>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title24" id="concept_yv1_cm2_pdb">
    <h2 class="title topictitle2" id="ariaid-title24">What's New in 3.2.0.0</h2>

    <div class="body conbody">
        <p class="p"><span class="ph">Data Collector</span>
            version 3.2.0.0 includes the following new features and enhancements:</p>

        <dl class="dl">
            
                <dt class="dt dlterm">Origins</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_yv1_cm2_pdb__ul_irn_3m2_pdb">
                        <li class="li"><a class="xref" href="../Origins/HDFSStandalone.html#concept_djz_pdm_hdb">New
                                Hadoop FS Standalone origin</a> - Similar to the Directory
                            origin, the Hadoop FS Standalone origin can use multiple threads to read
                            fully-written files. Use this origin in standalone execution mode
                            pipelines to read files in HDFS.</li>

                        <li class="li"><a class="xref" href="../Origins/MapRFSStandalone.html#concept_b43_3qc_mdb">New
                                MapR FS Standalone origin</a> - Similar to the Directory origin,
                            the MapR FS Standalone origin can use multiple threads to read
                            fully-written files. Use this origin in standalone execution mode
                            pipelines to read files in MapR FS. </li>

                        <li class="li"><a class="xref" href="../Pipeline_Design/DevStages.html#concept_czx_ktn_ht">New
                                Dev Snapshot Replaying origin</a> - The Dev Snapshot Replaying
                            origin is a development stage that reads records from a downloaded
                            snapshot file.</li>

                        <li class="li"><a class="xref" href="../Origins/HTTPClient.html#concept_mnv_s5r_35">HTTP Client origin enhancement</a> - You can now configure the
                            origin to process JSON files that include multiple JSON objects or a
                            single JSON array</li>

                        <li class="li"><a class="xref" href="../Origins/MultiTableJDBCConsumer.html#concept_h4j_4zs_kz">JDBC Multitable Consumer origin enhancements</a> - The origin
                            can now generate table-finished and schema-finished events when it
                            completes processing all rows in a table or schema. You can also
                            configure the number of seconds that the origin delays generating the
                            no-more-data event. You might want to configure a delay if you want the
                            table-finished or schema-finished events to appear in the event stream
                            before the no-more-data event. </li>

                        <li class="li">Oracle CDC Client origin enhancements - The origin includes the
                            following enhancements:<ul class="ul" id="concept_yv1_cm2_pdb__ul_ksy_ym2_pdb">
                                <li class="li">You can set a new Parse SQL Query property to false to skip
                                    parsing the SQL queries. Instead, the origin writes the SQL
                                    query to a sql field that can be parsed later. Default is
                                    true, which retains the previous behavior of parsing the SQL
                                    queries.</li>

                                <li class="li">The Send Redo Query property has been renamed. The new name is
                                    Send Redo Query in Headers.</li>

                            </ul>
</li>

                    </ul>

                    <ul class="ul" id="concept_yv1_cm2_pdb__ul_s5l_bn2_pdb">
                        <li class="li"><a class="xref" href="../Origins/TCPServer.html#concept_bqt_tl4_sz">TCP
                                Server origin enhancement</a> - You can now use the origin to
                            read the supported <span class="ph">Data Collector</span> data formats when passed in Flume events as Avro messages.</li>

                    </ul>

                </dd>

            
        </dl>

        <dl class="dl">
            
                <dt class="dt dlterm">Processors</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_yv1_cm2_pdb__ul_tvz_2n2_pdb">
                        <li class="li"><a class="xref" href="../Processors/HTTPClient.html#concept_t4k_2hh_jw">HTTP Client processor enhancement</a> - You can now use the
                            PATCH method with the processor. </li>

                        <li class="li"><a class="xref" href="../Processors/JDBCLookup.html#task_kbr_2cy_hw">JDBC Lookup processor enhancement</a> - The Retry on Cache Miss
                            property has been renamed to Retry on Missing Value.</li>

                        <li class="li">Kudu Lookup processor enhancement - You can now configure the processor
                            behavior when a lookup returns no value. </li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Destinations</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_yv1_cm2_pdb__ul_tkd_2p2_pdb">
                        <li class="li"><a class="xref" href="../Destinations/HadoopFS-destination.html#concept_awl_4km_zq">Hadoop FS</a>, <a class="xref" href="../Destinations/LocalFS.html#concept_zvc_bv5_1r">Local
                                FS</a>, and <a class="xref" href="../Destinations/MapRFS.html#concept_spv_xlc_fv">MapR FS</a>
                            destination enhancements - These destinations now support writing
                            records using the SDC Record format. </li>

                        <li class="li"><a class="xref" href="../Destinations/HTTPClient.html#concept_ry4_ct5_lz">HTTP Client destination enhancement</a> - You can now use the
                            PATCH method with the destination. </li>

                        <li class="li"><a class="xref" href="../Destinations/KineticaDB.html#task_r1q_vxg_qbb">KineticaDB destination enhancement</a> - You can now define a
                            list of custom worker node URLs so that the destination uses host names
                            instead of IP addresses to connect to the worker nodes.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Executors</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_yv1_cm2_pdb__ul_lr5_gp2_pdb">
                        <li class="li"><a class="xref" href="../Executors/MapReduce.html#concept_jqk_g4y_mx">MapReduce executor enhancement</a> - You can now use the new
                            Avro to ORC job to convert Avro files to ORC files.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm"><span class="ph">Data Collector Edge</span> (<span class="ph">SDC Edge</span>)</dt>

                <dd class="dd">
                    <div class="p"><span class="ph">SDC Edge</span> includes the following enhancements:<ul class="ul" id="concept_yv1_cm2_pdb__ul_qpd_4p2_pdb">
                            <li class="li"><a class="xref" href="../Edge_Mode/EdgePipelineTypes.html#concept_ih5_s2q_s2b" title="Edge pipelines support a limited number of processors. Processors function the same way in edge pipelines as they do in other pipelines. However, some processors have limitations in edge pipelines as noted below.">JavaScript Evaluator processor supported</a> - Both edge
                                sending pipelines and edge receiving pipelines now support the
                                JavaScript Evaluator processor.</li>

                            <li class="li"><a class="xref" href="../Edge_Mode/EdgePipelines_Deploy.html#task_isp_m2f_3db">Publish edge pipelines to SDC Edge</a> - You can now use the
                                    <span class="ph">Data Collector</span> UI to directly publish edge pipelines to an <span class="ph">SDC Edge</span> that is running. Previously, you had to first export edge
                                pipelines from <span class="ph">Data Collector</span>, and then move them to the <span class="ph">SDC Edge</span> installed on the edge device.</li>

                            <li class="li"><a class="xref" href="../Edge_Mode/EdgePipelines_Manage.html#concept_tqk_dbb_4db" title="After designing edge pipelines in Data Collector and then deploying the edge pipelines to SDC Edge, you can manage the pipelines on SDC Edge. Managing edge pipelines includes previewing, validating, starting, stopping, and monitoring the pipelines as well as resetting the origin for the pipelines.">Manage edge pipelines from the Data Collector UI</a> - You
                                can now use the <span class="ph">Data Collector</span> UI to start, monitor, stop, and reset the origin for edge
                                pipelines running on a remote <span class="ph">SDC Edge</span>. Previously, you had to use the command line and REST API to
                                manage edge pipelines on <span class="ph">SDC Edge</span>.</li>

                        </ul>
</div>

                </dd>

            
            
                <dt class="dt dlterm">Miscellaneous</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_yv1_cm2_pdb__ul_dvy_2r2_pdb">
                        <li class="li"><a class="xref" href="../Pipeline_Design/ErrorHandling.html#concept_pm4_txm_vq">Pipeline error handling enhancement</a> - You can now configure
                            pipelines to write error records to Azure Event Hub.</li>

                        <li class="li">Pipeline runner idle time enhancement - You can configure the number of
                            seconds that a pipeline runner waits before sending an empty batch.</li>

                        <li class="li">Runtime statistics enhancement - Runtime statistics now include the
                            number of empty or idle batches that are generated by the pipeline. </li>

                        <li class="li">Snapshot enhancement - Snapshots now include record header attributes
                            for error records. Previously, snapshots included only the record fields
                            in an error record.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Stages</dt>

                <dd class="dd">
                    <div class="p">This version of <span class="ph">Data Collector</span> includes the following new stage library:<ul class="ul" id="concept_yv1_cm2_pdb__ul_u5c_pv2_pdb">
                            <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">Apache Kudu version 1.7</a></li>

                        </ul>
</div>

                </dd>

            
        </dl>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title25" id="concept_ozm_kxk_fdb">
    <h2 class="title topictitle2" id="ariaid-title25">What's New in 3.1.2.0</h2>

    <div class="body conbody">
        <div class="p"><span class="ph">Data Collector</span>
            version 3.1.2.0 includes the following new features and enhancements:<ul class="ul" id="concept_ozm_kxk_fdb__ul_bvm_d3l_fdb">
                <li class="li"><a class="xref" href="../Origins/Directory.html#concept_b4d_fym_xv">Directory
                        origin enhancement</a> - When processing files using the Last Modified
                    Timestamp read order, the Directory origin now assesses the change timestamp in
                    addition to the last modified timestamp to establish file processing order.</li>

                <li class="li">Impersonation enhancement for Control Hub - You can now configure Data Collector
                    to use a partial Control Hub user ID for <a class="xref" href="../Configuration/DCConfig.html#concept_rcn_gqk_fdb" title="When Data Collector is registered with Control Hub, you can configure Data Collector to use an abbreviated version of the Control Hub user ID to impersonate a Hadoop user.">Hadoop impersonation mode</a> and <a class="xref" href="../Executors/Shell.html#concept_n2w_txv_vz">shell
                        impersonation mode</a>. Use this feature when Data Collector is
                    registered with Control Hub, and when the Hadoop or target operating system has
                    user name requirements that do not allow using the full Control Hub user
                    ID.</li>

                <li class="li">NetFlow 9 processing enhancement - When processing NetFlow 9 data, Data
                    Collector now includes FIELD_SENDER and FIELD_RECIPIENT fields to include sender
                    and receiver information.</li>

            </ul>
</div>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title26" id="concept_g3c_fn2_ycb">
    <h2 class="title topictitle2" id="ariaid-title26">What's New in 3.1.1.0</h2>

    <div class="body conbody">
        <div class="p"><span class="ph">Data Collector</span>
            version 3.1.1.0 includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_g3c_fn2_ycb__ul_v4n_r5q_1db">
                            <li class="li"><a class="xref" href="../Origins/Directory.html#task_o5v_bp1_n3b" title="Configure a Directory origin to read data from files in a directory.">Directory origin enhancements</a> - The origin includes the
                                following enhancements:<ul class="ul" id="concept_g3c_fn2_ycb__ul_bff_s5q_1db">
                                    <li class="li">The Max Files in Directory property has been renamed to Max
                                        Files Soft Limit. As the name indicates, the property is now
                                        a soft limit rather than a hard limit. As such, if the
                                        directory contains more files than the configured Max Files
                                        Soft Limit, the origin can temporarily exceed the soft limit
                                        and the pipeline can continue running.<p class="p">Previously, this
                                            property was a hard limit. When the directory contained
                                            more files, the pipeline failed.</p>
</li>

                                    <li class="li">The origin includes a new Spooling Period property that
                                        determines the number of seconds to continue adding files to
                                        the processing queue after the maximum files soft limit has
                                        been exceeded.</li>

                                </ul>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Destinations</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_g3c_fn2_ycb__ul_c1b_f5q_1db">
                            <li class="li">
                                <a class="xref" href="../Destinations/WaveAnalytics.html#task_mdt_dv3_rx">Einstein Analytics destination enhancement</a> - The Append
                                Timestamp to Alias property is now disabled by default for new
                                pipelines. When disabled, the destination can append, delete,
                                overwrite, or upsert data to an existing dataset. When enabled, the
                                destination creates a new dataset for each upload of data.<p class="p">The
                                    property was added in version 3.1.0.0 and was enabled by
                                    default. Pipelines upgraded from versions earlier than 3.1.0.0
                                    have the property enabled by default.</p>
</li>

                            <li class="li"><a class="xref" href="../Destinations/Solr.html#task_ld1_phr_wr">Solr destination enhancements</a> - The destination includes
                                the following enhancements:<ul class="ul" id="concept_g3c_fn2_ycb__ul_j1k_dt2_ycb">
                                    <li class="li">The destination now includes an Ignore Optional Fields
                                        property that allows ignoring null values in optional fields
                                        when writing records.</li>

                                    <li class="li">The destination allows you to configure Wait Flush, Wait
                                        Searcher, and Soft Commit properties to tune write
                                        performance.</li>

                                </ul>
</li>

                        </ul>

                    </dd>

                
            </dl>
</div>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title27" id="concept_agl_4tw_scb">
    <h2 class="title topictitle2" id="ariaid-title27">What's New in 3.1.0.0</h2>

    <div class="body conbody">
        <div class="p"><span class="ph">Data Collector</span>
            version 3.1.0.0 includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Data Synchronization Solution for Postgres</dt>

                    <dd class="dd">This release includes a beta version of the <a class="xref" href="../Solutions/JDBC_DriftSyncSolution.html#concept_ljq_knr_4cb" title="The Drift Synchronization Solution for PostgreSQL detects drift in incoming data and automatically creates or alters corresponding PostgreSQL tables as needed before the data is written.">Data Synchronization Solution for Postgres</a>. The solution uses
                        the new <a class="xref" href="../Processors/PostgreSQLMetadata.html#concept_lcp_ssh_qcb">Postgres Metadata processor</a> to detect drift in incoming data and
                        automatically create or alter corresponding PostgreSQL tables as needed
                        before the data is written. The solution also leverages the JDBC Producer
                        destination to perform the writes. </dd>

                    <dd class="dd ddexpand">As a beta feature, use the Data Synchronization Solution for Postgres for
                        development or testing only. Do not use the solution in production
                        environments.</dd>

                    <dd class="dd ddexpand">Support for additional databases is planned for future releases. To state a
                        preference, leave a comment <a class="xref" href="https://issues.streamsets.com/browse/SDC-8051" target="_blank"><u class="ph u">on this issue</u></a>.</dd>

                
                
                    <dt class="dt dlterm">Data Collector Edge (SDC Edge)</dt>

                    <dd class="dd">SDC Edge includes the following enhancements:<ul class="ul" id="concept_agl_4tw_scb__ul_x34_y5w_scb">
                            <li class="li"><a class="xref" href="../Edge_Mode/EdgePipelineTypes.html#concept_c14_m4r_4bb" title="Edge pipelines run in edge execution mode. You design edge pipelines in Data Collector.">Edge pipelines</a> now support the following stages:<ul class="ul" id="concept_agl_4tw_scb__ul_p11_dvw_scb">
                                    <li class="li">Dev Raw Data Source origin</li>

                                    <li class="li">Kafka Producer destination</li>

                                </ul>
</li>

                            <li class="li">Edge pipelines now support the following functions:<ul class="ul" id="concept_agl_4tw_scb__ul_b1t_2vw_scb">
                                    <li class="li">emptyList()</li>

                                    <li class="li">emptyMap()</li>

                                    <li class="li">isEmptyMap()</li>

                                    <li class="li">isEmptyList()</li>

                                    <li class="li">length()</li>

                                    <li class="li">record:attribute()</li>

                                    <li class="li">record:attributeOrDefault()</li>

                                    <li class="li">size()</li>

                                </ul>
</li>

                            <li class="li">When you start SDC Edge, you can now <a class="xref" href="../Edge_Mode/SDCeAdminister.html#concept_ecy_hjx_scb" title="By default, SDC Edge writes log messages at the info severity level to the &lt;SDCEdge_home&gt;/log/edge.log file. To view the logs, simply open the edge.log file in a text editor.">change the
                                    default log directory</a>.</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_agl_4tw_scb__ul_jxt_jvw_scb">
                            <li class="li"><a class="xref" href="../Origins/HTTPClient.html#concept_edk_j5t_zw">HTTP Client origin enhancement</a> - You can now configure
                                the origin to use the Link in Response Field pagination type. After
                                processing the current page, this pagination type uses a field in
                                the response body to access the next page. </li>

                            <li class="li"><a class="xref" href="../Origins/HTTPServer.html#concept_s2p_5hb_4y">HTTP
                                    Server origin enhancement</a> - You can now use the origin to
                                process the contents of authorized HTTP PUT requests.</li>

                            <li class="li"><a class="xref" href="../Origins/KinConsumer.html#concept_qy4_rky_vcb" title="The Kinesis Consumer origin stores offsets - the location where the origin stops reading - in DynamoDB lease tables. You can optionally define tags to apply to a lease table created by the origin. The origin cannot add tags to existing lease tables.">Kinesis Consumer origin enhancement</a> - You can now define
                                tags to apply to the DynamoDB lease table that the origin creates to
                                store offsets.</li>

                            <li class="li"><a class="xref" href="../Origins/MQTTSubscriber.html#concept_fwm_2kn_scb">MQTT Subscriber origin enhancement</a> - The origin now
                                includes a TOPIC_HEADER_NAME record header attribute that includes
                                the topic information for each record.</li>

                            <li class="li"><a class="xref" href="../Origins/MongoDB.html#concept_vx3_1gh_scb">MongoDB origin enhancement</a> - The origin now generates a
                                no-more-data event when it has processed all available documents and
                                the configured batch wait time has elapsed.</li>

                            <li class="li"><a class="xref" href="../Origins/OracleCDC.html#concept_gj4_sjq_qcb">Oracle CDC Client origin enhancement</a> - You can now
                                specify the tables to process by using SQL-like syntax in table
                                inclusion patterns and exclusion patterns.</li>

                            <li class="li">Salesforce origin enhancements - The origin includes the following
                                    enhancements:<ul class="ul" id="concept_agl_4tw_scb__ul_mfg_wvw_scb">
                                    <li class="li">The origin can now subscribe to <a class="xref" href="../Origins/Salesforce.html#concept_cwb_mkg_5cb">Salesforce platform events</a>. </li>

                                    <li class="li">You can now configure the origin to use <a class="xref" href="../Origins/Salesforce.html#concept_aq1_kd1_5cb">Salesforce PK Chunking</a>.</li>

                                    <li class="li">When necessary, you can <a class="xref" href="../Origins/Salesforce.html#task_h1n_bs3_rx">disable query validation</a>.</li>

                                    <li class="li">You can now use <a class="xref" href="../Origins/Salesforce.html#task_h1n_bs3_rx">Mutual Authentication to connect to
                                        Salesforce</a>.</li>

                                </ul>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Processors</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_agl_4tw_scb__ul_vyk_1ww_scb">
                            <li class="li"><a class="xref" href="../Processors/FieldReplacer.html#concept_rw4_2d3_4cb">New Field Replacer processor</a> - A new processor that
                                replaces values in fields with nulls or with new values. <p class="p">The
                                    Field Replacer processor replaces the Value Replacer processor
                                    which has been deprecated. The Field Replacer processor lets you
                                    define more complex conditions to replace values. For example,
                                    the Field Replacer can replace values which fall within a
                                    specified range. The Value Replacer cannot replace values that
                                    fall within a specified range.</p>
<p class="p">StreamSets recommends that
                                    you <a class="xref" href="../Upgrade/PostUpgrade.html#concept_hxf_3yd_qcb" title="Starting with version 3.1.0.0, Data Collector introduces a new Field Replacer processor and has deprecated the Value Replacer processor.">update Value Replacer pipelines</a> as soon as possible.
                                </p>
</li>

                            <li class="li"><a class="xref" href="../Processors/PostgreSQLMetadata.html#concept_lcp_ssh_qcb">New
                                    Postgres Metadata processor</a> - A new processor that
                                determines when changes in data structure occur and creates and
                                alters PostgreSQL tables accordingly. Use as part of the <a class="xref" href="../Solutions/JDBC_DriftSyncSolution.html#concept_ljq_knr_4cb" title="The Drift Synchronization Solution for PostgreSQL detects drift in incoming data and automatically creates or alters corresponding PostgreSQL tables as needed before the data is written.">Drift Synchronization Solution for Postgres</a> in
                                development or testing environments only.</li>

                            <li class="li">Aggregator processor enhancements - The processor includes the
                                following enhancements:<ul class="ul" id="concept_agl_4tw_scb__ul_e2h_nww_scb">
                                    <li class="li"><a class="xref" href="../Processors/Aggregator.html#concept_bc4_c42_wbb">Event records</a> now include the results of the
                                        aggregation. </li>

                                    <li class="li">You can now specify the <a class="xref" href="../Processors/Aggregator.html#concept_dld_z2q_vcb">root field for event records</a>. You can use a
                                        String or Map root field. Upgraded pipelines retain the
                                        previous behavior, writing aggregation data to a String root
                                        field. </li>

                                </ul>
</li>

                            <li class="li">JDBC Lookup processor enhancement - The processor includes the
                                following enhancements:<ul class="ul" id="concept_agl_4tw_scb__ul_k14_pww_scb">
                                    <li class="li">You can now configure a <a class="xref" href="../Processors/JDBCLookup.html#task_kbr_2cy_hw">Missing Values Behavior property</a> that defines
                                        processor behavior when a lookup returns no value. Upgraded
                                        pipelines continue to send records with no return value to
                                        error.</li>

                                    <li class="li">You can now enable the <a class="xref" href="../Processors/JDBCLookup.html#concept_jt5_kx2_px">Retry on Cache Miss</a> property so that the
                                        processor retries lookups for known missing values. By
                                        default, the processor always returns the default value for
                                        known missing values to avoid unnecessary lookups.</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Processors/KuduLookup.html#concept_cs4_vj1_s1b">Kudu Lookup processor enhancement</a> - The processor no
                                longer requires that you add a primary key column to the Key Columns
                                Mapping. However, adding only non-primary keys can slow the
                                performance of the lookup.</li>

                            <li class="li"><a class="xref" href="../Processors/SalesforceLookup.html#task_fhn_yrk_yx">Salesforce Lookup processor enhancement</a> - You can now
                                use Mutual Authentication to connect to Salesforce.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Destinations</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_agl_4tw_scb__ul_ynj_2xw_scb">
                            <li class="li"><a class="xref" href="../Destinations/Aerospike.html#concept_gyq_rpr_4cb">New
                                    Aerospike destination</a> - A new destination that writes
                                data to Aerospike.</li>

                            <li class="li"><a class="xref" href="../Destinations/NamedPipe.html#concept_pl5_tdg_gcb">New
                                    Named Pipe destination</a>- A new destination that writes
                                data to a UNIX named pipe. </li>

                            <li class="li">Einstein Analytics destination enhancements - The destination
                                includes the following enhancements:<ul class="ul" id="concept_agl_4tw_scb__ul_r3y_xxw_scb">
                                    <li class="li">You can specify the name of the <a class="xref" href="../Destinations/WaveAnalytics.html#task_mdt_dv3_rx">edgemart container</a> that contains the
                                        dataset.</li>

                                    <li class="li">You can define the <a class="xref" href="../Destinations/WaveAnalytics.html#concept_ryp_g4r_vcb">operation to perform</a>: Append, Delete, Overwrite,
                                        or Upsert.</li>

                                    <li class="li">You can now use <a class="xref" href="../Destinations/WaveAnalytics.html#task_mdt_dv3_rx">Mutual Authentication to connect to
                                        Salesforce</a>.</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Destinations/Elasticsearch.html#concept_w2r_ktb_ry">Elasticsearch destination enhancement</a> - You can now
                                configure the destination to merge data which performs an update
                                with <code class="ph codeph">doc_as_upsert</code>. </li>

                            <li class="li">Salesforce destination enhancement - The destination includes the
                                following enhancements:<ul class="ul" id="concept_agl_4tw_scb__ul_rpj_2yw_scb">
                                    <li class="li">The destination can now <a class="xref" href="../Destinations/Salesforce.html#concept_rlb_rt3_rx">publish Salesforce platform events</a>.</li>

                                    <li class="li">You can now use <a class="xref" href="../Destinations/Salesforce.html#task_ncv_153_rx">Mutual Authentication to connect to
                                        Salesforce</a>.</li>

                                </ul>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Data Formats</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_agl_4tw_scb__ul_sl2_hyw_scb">
                            <li class="li"><a class="xref" href="../Data_Formats/LogFormats.html#concept_tr1_spd_sr" title="When you use an origin to read log data, you define the format of the log files to be read.">Log
                                    data format enhancement</a> - Data Collector can now process
                                data using the following log format types:<ul class="ul" id="concept_agl_4tw_scb__ul_dmj_3yw_scb">
                                    <li class="li">Common Event Format (CEF)</li>

                                    <li class="li">Log Event Extended Format (LEEF)</li>

                                </ul>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Expression Language</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_agl_4tw_scb__ul_vjw_nyw_scb">
                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_ndj_43v_1r" title="Error record functions provide information about error records. Use error functions to process error records.">Error record functions</a> - This release includes the
                                following new function:<ul class="ul" id="concept_agl_4tw_scb__ul_yvn_vzw_scb">
                                    <li class="li">record:errorStackTrace() - Returns the error stack trace for
                                        the record.</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_qkr_trf_sw" title="Use time functions to return the current time or to transform datetime data.">Time functions</a> - This release includes the following new
                                    functions:<ul class="ul" id="concept_agl_4tw_scb__ul_own_yzw_scb">
                                    <li class="li">time:dateTimeZoneOffset() - Returns the time zone offset in
                                        milliseconds for the specified date and time zone.</li>

                                    <li class="li">time:timeZoneOffset() - Returns the time zone offset in
                                        milliseconds for the specified time zone.</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_ddw_ld1_1s">Miscellaneous functions</a> - This release includes the
                                following changed and new functions:<ul class="ul" id="concept_agl_4tw_scb__ul_uqv_b1x_scb">
                                    <li class="li">runtime:loadResource() - This function has been changed to
                                        trim any leading or trailing whitespace characters from the
                                        file before returning the value in the file. Previously, the
                                        function did not trim white space characters - you had to
                                        avoid including unnecessary characters in the file.</li>

                                    <li class="li">runtime:loadResourceRaw() - New function that returns the
                                        value in the specified file, including any leading or
                                        trailing whitespace characters in the file.</li>

                                </ul>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Additional Stage Libraries</dt>

                    <dd class="dd">This release includes the following <a class="xref" href="../Installation/AddtionalStageLibs.html#concept_fb2_qmn_bz">additional stage libraries</a>:<ul class="ul" id="concept_agl_4tw_scb__ul_ksd_21x_scb">
                            <li class="li"> Apache Kudu 1.6 </li>

                            <li class="li"> Cloudera 5.13 distribution of Apache Kafka 2.1 </li>

                            <li class="li"> Cloudera 5.14 distribution of Apache Kafka 2.1 </li>

                            <li class="li"> Cloudera CDH 5.14 distribution of Hadoop </li>

                            <li class="li"> Kinetica 6.1 </li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Miscellaneous</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_agl_4tw_scb__ul_idl_31x_scb">
                            <li class="li"><a class="xref" href="../Configuration/DCConfig.html#task_lxk_kjw_1r" title="You can customize Data Collector by editing the Data Collector configuration file, sdc.properties.">Data Collector classpath validation</a> - Data Collector now
                                performs a classpath health check upon starting up. The results of
                                the health check are written to the Data Collector log. When
                                necessary, you can configure Data Collector to skip the health check
                                or to stop upon errors.</li>

                            <li class="li"><a class="xref" href="../Configuration/DCConfig.html#task_lxk_kjw_1r" title="You can customize Data Collector by editing the Data Collector configuration file, sdc.properties.">Support bundle Data Collector property</a> - You can
                                configure a property in the Data Collector configuration file to
                                have Data Collector automatically upload support bundles when
                                problems occur. The property is disabled by default.</li>

                            <li class="li"><a class="xref" href="../DPM/DPMConfiguration.html#concept_hrn_zz3_fx" title="You can customize how a registered Data Collector works with StreamSets Control Hub by editing the Control Hub configuration file, $SDC_CONF/dpm.properties, located in the Data Collector installation.">Redirect registered Data Collector user logins property</a>
                                - You can enable a property in the Control Hub configuration file,
                                    <code class="ph codeph">dpm.properties</code>, to redirect Data Collector user
                                logins to Control Hub using the HTML meta refresh method.</li>

                            <li class="li"><a class="xref" href="../Pipeline_Configuration/RuntimeValues.html#concept_fjx_g31_1s" title="Runtime properties are properties that you define in a file local to the Data Collector and call from within a pipeline.">Runtime properties enhancement</a> - You can now use
                                environment variables in runtime properties.</li>

                        </ul>

                    </dd>

                
            </dl>
</div>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title28" id="concept_z4b_qrb_4cb">
    <h2 class="title topictitle2" id="ariaid-title28">What's New in 3.0.3.0</h2>

    <div class="body conbody">
        <div class="p"><span class="ph">Data Collector</span>
            version 3.0.3.0 includes the following enhancements:<ul class="ul" id="concept_z4b_qrb_4cb__ul_q44_5rb_4cb">
                <li class="li"><a class="xref" href="../Origins/MySQLBinaryLog.html#task_qbt_kyh_xx">MySQL
                        Binary Log origin enhancement</a> - You can now use a Keep Alive thread
                    to connect to the MySQL server by configuring the Enable KeepAlive Thread and
                    the KeepAlive Interval advanced properties. <p class="p">By default, the origin uses Keep
                        Alive threads with an interval of one minute. Upgraded pipelines also use
                        the new defaults.</p>
</li>

                <li class="li">HTTP Client processor enhancement - The processor can now process compressed
                    data. </li>

                <li class="li">Scripting processors enhancement - The Groovy Evaluator, JavaScript Evaluator,
                    and Jython Evaluator processors can use a new boolean sdcFunction.isPreview()
                    method to determine if the pipeline is in preview mode.</li>

            </ul>
</div>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title29" id="concept_lhn_cvv_lcb">
    <h2 class="title topictitle2" id="ariaid-title29">What's New in 3.0.2.0</h2>

    <div class="body conbody">
        <div class="p"><span class="ph">Data Collector</span>
            version 3.0.2.0 includes the following enhancement:<ul class="ul" id="concept_lhn_cvv_lcb__ul_y3g_kvv_lcb">
                <li class="li">SFTP/FTP Client origin enhancement - The origin can now generate events when
                    starting and completing processing for a file and when all available files have
                    been processed. </li>

            </ul>
</div>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title30" id="concept_lgz_yp4_gcb">
    <h2 class="title topictitle2" id="ariaid-title30">What's New in 3.0.1.0</h2>

    <div class="body conbody">
        <div class="p"><span class="ph">Data Collector</span>
            version 3.0.1.0 includes the following enhancements:<ul class="ul" id="concept_lgz_yp4_gcb__ul_fjz_1q4_gcb">
                <li class="li">Azure IoT/Event Hub Consumer origin enhancement - The Azure Event Hub Consumer
                    origin has been renamed to the Azure IoT/Event Hub Consumer origin.</li>

                <li class="li"><a class="xref" href="../Origins/HTTPServer.html#concept_rvj_5qy_4cb">HTTP Server origin enhancement</a> - The HTTP Server origin now includes
                    path and queryString record header attributes, as well as any other HTTP header
                    attributes included in the request. </li>

                <li class="li">MongoDB origins enhancement - Both the <a class="xref" href="../Origins/MongoDB.html#task_mdf_2rs_ns">MongoDB
                        origin</a> and the <a class="xref" href="../Origins/MongoDBOplog.html#task_qj5_drw_4y">MongoDB
                        Oplog</a> origin now support delegated authentication and the BSON data
                    type for binary data. </li>

                <li class="li"><a class="xref" href="../Origins/SQLServerCDC.html#concept_pc4_xts_r1b">SQL Server CDC origin enhancement</a> - The SQL Server CDC origin now
                    includes information from the SQL Server CDC __$command_id column in a record
                    header attribute named jdbc. __$command_id.</li>

                <li class="li">MongoDB destination enhancement - The MongoDB destination now supports <a class="xref" href="../Destinations/MongoDB.html#task_mrc_k5n_4v">delegated
                        authentication</a>.</li>

            </ul>
</div>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title31" id="concept_cjx_y4k_wbb">
    <h2 class="title topictitle2" id="ariaid-title31">What's New in 3.0.0.0</h2>

    <div class="body conbody">
        <div class="p"><span class="ph">Data Collector</span>
            version 3.0.0.0 includes the following new features:<dl class="dl">
                
                    <dt class="dt dlterm">Installation</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_cjx_y4k_wbb__ul_dxx_z5k_wbb">
                            <li class="li"><a class="xref" href="../Installation/InstallationAndConfig.html#concept_vzg_n2p_kq" title="Install Data Collector on a machine that meets the following minimum requirements. To run pipelines in cluster execution mode, each node in the cluster must meet the minimum requirements.">Java requirement</a> - Data Collector now supports both
                                Oracle Java 8 and OpenJDK 8.</li>

                            <li class="li"><a class="xref" href="../Installation/FullInstall_ServiceStart.html#task_th5_1yj_dx" title="Users with a StreamSets enterprise account can install the Data Collector RPM package and start it as a service on CentOS, Oracle Linux, or Red Hat Enterprise Linux.">RPM packages</a> - StreamSets now provides the following <span class="ph">Data Collector</span> RPM packages:<ul class="ul" id="concept_cjx_y4k_wbb__ul_k13_cvk_wbb">
                                    <li class="li">EL6 - Use to install <span class="ph">Data Collector</span> on CentOS 6, Oracle Linux 6, or Red Hat Enterprise Linux
                                        6</li>

                                    <li class="li">EL7 - Use to install <span class="ph">Data Collector</span> on CentOS 7, Oracle Linux 7, or Red Hat Enterprise Linux
                                        7.</li>

                                </ul>
<p class="p">Previously, <span class="ph">StreamSets</span> provided a single RPM package used to install <span class="ph">Data Collector</span> on any of these operating systems.</p>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Edge Pipelines</dt>

                    <dd class="dd">You can now design and run <a class="xref" href="../Edge_Mode/EdgePipelines_Overview.html#concept_d4h_kkq_4bb" title="StreamSets Data Collector EdgeTM (SDC Edge) is a lightweight execution agent without a UI that runs pipelines on edge devices with limited resources. Use SDC Edge to read data from an edge device or to receive data from another pipeline and then act on that data to control an edge device.">edge
                            pipelines</a> to read data from or send data to an edge device. Edge
                        pipelines are bidirectional. They can send edge data to other <span class="ph">Data Collector</span> pipelines for further processing. Or, they can receive data from other
                        pipelines and then act on that data to control the edge device. </dd>

                    <dd class="dd ddexpand">Edge pipelines run in edge execution mode on <span class="ph">StreamSets</span>
                        <span class="ph">Data Collector Edge</span> (<span class="ph">SDC Edge</span>). <span class="ph">SDC Edge</span> is a lightweight agent without a UI that runs pipelines on edge devices.
                        Install <span class="ph">SDC Edge</span> on each edge device where you want to run edge pipelines. </dd>

                    <dd class="dd ddexpand">You design edge pipelines in <span class="ph">Data Collector</span>, export the edge pipelines, and then use commands to run the edge
                        pipelines on an <span class="ph">SDC Edge</span> installed on an edge device. </dd>

                
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_cjx_y4k_wbb__ul_xdj_qvk_wbb">
                            <li class="li"><a class="xref" href="../Origins/AmazonSQS.html#concept_xsh_knm_5bb">New
                                    Amazon SQS Consumer origin</a> - An origin that reads
                                messages from Amazon Simple Queue Service (SQS). Can create multiple
                                threads to enable parallel processing in a multithreaded
                                pipeline.</li>

                            <li class="li"><a class="xref" href="../Origins/GCS.html#concept_iyd_wql_nbb">New Google
                                    Cloud Storage origin</a> - An origin that reads fully written
                                objects in Google Cloud Storage. </li>

                            <li class="li"><a class="xref" href="../Origins/MapRdbCDC.html#concept_qwj_5vm_pbb">New MapR
                                    DB CDC origin</a> - An origin that reads changed MapR DB data
                                that has been written to MapR Streams. Can create multiple threads
                                to enable parallel processing in a multithreaded pipeline.</li>

                            <li class="li"><a class="xref" href="../Origins/MapRStreamsMultiConsumer.html#concept_hvd_hww_lbb">New MapR Multitopic Streams Consumer origin</a> - An origin
                                that reads messages from multiple MapR Streams topics. It can create
                                multiple threads to enable parallel processing in a multithreaded
                                pipeline.</li>

                            <li class="li"><a class="xref" href="../Origins/UDPMulti.html#concept_wng_g5f_5bb">New UDP
                                    Multithreaded Source origin</a> - The origin listens for UDP
                                messages on one or more ports and queues incoming packets on an
                                intermediate queue for processing. It can create multiple threads to
                                enable parallel processing in a multithreaded pipeline.</li>

                            <li class="li"><a class="xref" href="../Origins/WebSocketClient.html#concept_unk_nzk_fbb">New
                                    WebSocket Client origin</a> - An origin that reads data from
                                a WebSocket server endpoint.</li>

                            <li class="li"><a class="xref" href="../Origins/WindowsLog.html#concept_agf_5jv_sbb">New
                                    Windows Event Log origin</a> - An origin that reads data from
                                Microsoft Windows event logs. You can use this origin only in
                                pipelines configured for edge execution mode.</li>

                            <li class="li"><a class="xref" href="../Pipeline_Design/DevStages.html#concept_czx_ktn_ht">New Sensor Reader development origin</a> - A development
                                origin that generates sample atmospheric data for <a class="xref" href="../Edge_Mode/EdgePipelines_Overview.html#concept_d4h_kkq_4bb" title="StreamSets Data Collector EdgeTM (SDC Edge) is a lightweight execution agent without a UI that runs pipelines on edge devices with limited resources. Use SDC Edge to read data from an edge device or to receive data from another pipeline and then act on that data to control an edge device.">edge pipelines</a>.</li>

                            <li class="li">Amazon S3 origin enhancements:<ul class="ul" id="concept_cjx_y4k_wbb__ul_gxg_dbl_wbb">
                                    <li class="li">The origin now produces <a class="xref" href="../Origins/AmazonS3.html#concept_vtn_ty4_jbb">no-more-data events</a> and includes a new socket
                                        timeout property.</li>

                                    <li class="li">You can now specify the number of times the origin <a class="xref" href="../Origins/AmazonS3.html#task_gfj_ssv_yq">retries a query</a>. The default is three.</li>

                                </ul>
</li>

                            <li class="li">
                                <a class="xref" href="../Origins/Directory.html#concept_pcl_nwn_qbb">Directory origin enhancement</a> - The origin can now use
                                multiple threads to perform parallel processing of files.</li>

                            <li class="li"><a class="xref" href="../Origins/HTTPClient.html#concept_wk4_bjz_5r">HTTP
                                    Client origin enhancement</a> - The origin can now log
                                request and response data to the <span class="ph">Data Collector</span> log. </li>

                            <li class="li">JDBC Multitable Consumer origin enhancements:<ul class="ul" id="concept_cjx_y4k_wbb__ul_ij2_lbl_wbb">
                                    <li class="li">The origin can now use <a class="xref" href="../Origins/MultiTableJDBCConsumer.html#concept_xwr_bhm_nbb">non-incremental processing</a> for tables with no
                                        primary key or offset column. </li>

                                    <li class="li">You can now specify an <a class="xref" href="../Origins/MultiTableJDBCConsumer.html#task_kst_m4w_4y">Init Query</a> to be executed after establishing a
                                        connection to the database, before performing other tasks.
                                        This can be used, for example, to modify session attributes. </li>

                                    <li class="li">A new <a class="xref" href="../Origins/MultiTableJDBCConsumer.html#task_kst_m4w_4y">Queries Per Second property</a> determines how many
                                        queries can be run every second. <p class="p">This property replaces
                                            the Query Interval property. For information about
                                            possible upgrade impact, see <a class="xref" href="../Upgrade/PostUpgrade.html#concept_hky_ljl_wbb">JDBC Multitable Consumer Query Interval
                                                Change</a>.</p>
</li>

                                </ul>
</li>

                            <li class="li">JDBC Query Consumer origin enhancements:<ul class="ul" id="concept_cjx_y4k_wbb__ul_hzy_tbl_wbb">
                                    <li class="li">You can now specify an <a class="xref" href="../Origins/JDBCConsumer.html#task_ryz_tkr_bs">Init Query</a> to be executed after establishing a
                                        connection to the database, before performing other tasks.
                                        This can be used, for example, to modify session
                                        attributes.</li>

                                    <li class="li">The Microsoft SQL Server CDC functionality in the JDBC Query
                                        Consumer origin is now deprecated and will be removed from
                                        the origin in a future release. For upgrade information, see
                                            <a class="xref" href="../Upgrade/PostUpgrade.html#concept_ys3_bjl_wbb">Update JDBC Query Consumer Pipelines used for SQL
                                            Server CDC Data</a>.</li>

                                </ul>
</li>

                            <li class="li">Kafka Multitopic Consumer origin enhancement - The origin is now
                                available in the following stage libraries, in addition to the
                                Apache Kafka 0.10 stage library:<ul class="ul" id="concept_cjx_y4k_wbb__ul_emj_ccl_wbb">
                                    <li dir="ltr" class="li">
                                        <p dir="ltr" class="p">Apache Kafka 0.9</p>

                                    </li>

                                    <li dir="ltr" class="li">
                                        <p dir="ltr" class="p">CDH Kafka 2.0 (0.9.0) and 2.1 (0.9.0)</p>

                                    </li>

                                    <li dir="ltr" class="li">
                                        <p class="p">HDP 2.5 and 2.6</p>

                                    </li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Origins/KinConsumer.html#task_p4b_vv4_yr">Kinesis Consumer origin enhancement</a> - You can now
                                specify the number of times the origin retries a query. The default
                                is three.</li>

                            <li class="li">Oracle CDC Client origin enhancements:<ul class="ul" id="concept_cjx_y4k_wbb__ul_wbj_fcl_wbb">
                                    <li class="li">When using <a class="xref" href="../Origins/OracleCDC.html#concept_zrc_pyj_dx">SCNs for the initial change</a>, the origin now
                                        treats the specified SCN as a starting point rather than
                                        looking for an exact match.</li>

                                    <li class="li">The origin now passes <a class="xref" href="../Origins/OracleCDC.html#concept_gwp_d4n_n1b">raw data</a> to the pipeline as a byte array.</li>

                                    <li class="li">The origin can now include unparsed strings from the parsed
                                        SQL query for <a class="xref" href="../Origins/OracleCDC.html#concept_gwp_d4n_n1b">unsupported data types</a> in records.</li>

                                    <li class="li">The origin now uses <a class="xref" href="../Origins/OracleCDC.html#concept_yqk_3hn_n1b">local buffering</a> instead of Oracle LogMiner
                                        buffering by default. Upgraded pipelines require no changes. </li>

                                    <li class="li">The origin now supports reading the <a class="xref" href="../Origins/OracleCDC.html#concept_w1n_p5l_kcb">Timestamp with Timezone data type</a>. When reading
                                        Timestamp with Timezone data, the origin includes the offset
                                        with the datetime data in the <span class="ph">Data Collector</span> Zoned Datetime data type. It does not include the time
                                        zone ID.</li>

                                </ul>
</li>

                            <li class="li">SQL Server CDC Client origin enhancements - You can now perform the
                                following tasks with the SQL Server CDC Client origin:<ul class="ul" id="concept_cjx_y4k_wbb__ul_et1_pcl_wbb">
                                    <li class="li"><a class="xref" href="../Origins/SQLServerCDC.html#concept_nxm_1lp_qbb">Process CDC tables</a> that appear after the
                                        pipeline starts.</li>

                                    <li class="li"><a class="xref" href="../Origins/SQLServerCDC.html#concept_avq_s2q_qbb">Check for schema changes and generate events</a>
                                        when they are found.</li>

                                    <li class="li">
                                        <p dir="ltr" class="p">In addition, a new Capture Instance Name
                                            property replaces the Schema and Table Name Pattern
                                            properties from earlier releases.</p>

                                        <p dir="ltr" class="p">You can simply use the schema name and table
                                            name pattern for the capture instance name. Or, you can
                                            specify the schema name and a capture instance name
                                            pattern, which allows you to specify specific CDC tables
                                            to process when you have multiple CDC tables for a
                                            single data table.</p>

                                        <p dir="ltr" class="p">Upgraded pipelines require no changes.</p>

                                    </li>

                                </ul>
</li>

                            <li class="li">UDP Source origin enhancement - The Enable Multithreading property
                                that enabled using multiple epoll receiver threads is now named Use
                                Native Transports (epoll).</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Processors</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_cjx_y4k_wbb__ul_qfc_3fl_wbb">
                            <li class="li"><a class="xref" href="../Processors/Aggregator.html#concept_ofb_svm_5bb">New
                                    Aggregator processor</a> - A processor that aggregates data
                                within a window of time. Displays the results in Monitor mode and
                                can write the results to events. </li>

                            <li class="li"><a class="xref" href="../Processors/Delay.html#concept_ez5_pvf_wbb">New Delay
                                    processor</a> - A processor that can delay processing a batch
                                of records for a specified amount of time.</li>

                            <li class="li"><a class="xref" href="../Processors/FieldTypeConverter.html#task_g23_2tq_wq">Field Type Converter processor enhancement</a> - You can now
                                convert strings to the Zoned Datetime data type, and vice versa. You
                                can also specify the format to use. </li>

                            <li class="li"><a class="xref" href="../Processors/HiveMetadata.html#task_hpg_pft_zv">Hive Metadata processor enhancement</a> - You can now
                                configure additional JDBC configuration properties to pass to the
                                JDBC driver.</li>

                            <li class="li">HTTP Client processor enhancements: <ul class="ul" id="concept_cjx_y4k_wbb__ul_usn_wjr_hdb">
                                    <li class="li">The processor can now <a class="xref" href="../Processors/HTTPClient.html#concept_dhg_gmk_hdb" title="The HTTP Client processor can log request and response data to the Data Collector log.">log request and response data</a> to the <span class="ph">Data Collector</span> log. </li>

                                    <li class="li">The <a class="xref" href="../Processors/HTTPClient.html#task_z54_1qr_fw" title="Configure an HTTP Client processor to perform requests against a resource URL.">Rate Limit</a> now defines the minimum amount of
                                        time between requests in milliseconds. Previously, it
                                        defined the time between requests in seconds. Upgraded
                                        pipelines require no changes.</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Processors/JDBCLookup.html#task_kbr_2cy_hw">JDBC Lookup</a> and <a class="xref" href="../Processors/JDBCTee.html#task_qpj_ncy_hw">JDBC Tee</a> processor enhancements - You can now specify an
                                Init Query to be executed after establishing a connection to the
                                database, before performing other tasks. This can be used, for
                                example, to modify session attributes. </li>

                            <li class="li"><a class="xref" href="../Processors/KuduLookup.html#concept_eqf_dh3_x1b">Kudu Lookup processor enhancement</a> - The Cache Kudu Table
                                property is now named Enable Table Caching. The Maximum Entries to
                                Cache Table Objects property is now named Maximum Table Entries to
                                Cache. </li>

                            <li class="li">Salesforce Lookup processor enhancement - You can use a new <a class="xref" href="../Processors/SalesforceLookup.html#concept_ow1_lj3_xbb">Retrieve lookup mode</a> to look up data for a set of
                                records instead of record-by-record. The mode provided in previous
                                releases is now named SOQL Query. Upgraded pipelines require no
                                changes. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Destinations</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_cjx_y4k_wbb__ul_rv3_nfl_wbb">
                            <li class="li"><a class="xref" href="../Destinations/GCS.html#concept_p4n_jrl_nbb">New Google
                                    Cloud Storage destination</a> - A new destination that writes
                                data to objects in Google Cloud Storage. The destination can
                                generate events for use as dataflow triggers.</li>

                            <li class="li"><a class="xref" href="../Destinations/KineticaDB.html#concept_hxh_5xg_qbb">New
                                    KineticaDB destination</a> - A new destination that writes
                                data to a Kinetica table. </li>

                            <li class="li"><a class="xref" href="../Destinations/AmazonS3.html#task_pxb_j3r_rt">Amazon S3 destination enhancement</a> - You can now specify
                                the number of times the destination retries a query. The default is
                                three.</li>

                            <li class="li"><a class="xref" href="../Destinations/HiveMetastore.html#task_a4n_1ft_zv">Hive Metastore destination enhancement</a> - You can now
                                configure additional JDBC configuration properties to pass to the
                                JDBC driver.</li>

                            <li class="li">HTTP Client destination enhancements: <ul class="ul" id="concept_cjx_y4k_wbb__ul_tbw_3kr_hdb">
                                    <li class="li">The destination can now <a class="xref" href="../Destinations/HTTPClient.html#concept_e1s_mgr_hdb" title="The HTTP Client destination can log request and response data to the Data Collector log.">log
                                            request and response data</a> to the <span class="ph">Data Collector</span> log. </li>

                                    <li class="li">You can now use the HTTP Client destination to write <a class="xref" href="../Destinations/HTTPClient.html#concept_l2r_gy5_lz">Avro, Delimited, and Protobuf data</a> in addition
                                        to the previous data formats. </li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Destinations/JMSProducer.html#task_udk_yw5_n1b">JDBC Producer destination enhancement</a> - You can now
                                specify an Init Query to be executed after establishing a connection
                                to the database, before performing other tasks. This can be used,
                                for example, to modify session attributes. </li>

                            <li class="li"><a class="xref" href="../Destinations/Kudu.html#task_c4x_tmh_4v">Kudu destination enhancement</a> - If the destination
                                receives a change data capture log from the following source
                                systems, you now must specify the source system in the Change Log
                                Format property so that the destination can determine the format of
                                the log: Microsoft SQL Server, Oracle CDC Client, MySQL Binary Log,
                                or MongoDB Oplog. </li>

                            <li class="li">MapR DB JSON destination enhancement - The destination now supports
                                writing to MapR DB based on the <a class="xref" href="../Destinations/MapRDBJSON.html#concept_hy5_3nb_xbb">CRUD operation in record header attributes and the Insert API
                                    and Set API properties</a>.</li>

                            <li class="li">MongoDB destination enhancements - With this release, the Upsert
                                operation is no longer supported by the destination. Instead, the
                                destination includes the following enhancements:<ul class="ul" id="concept_cjx_y4k_wbb__ul_xfc_xfl_wbb">
                                    <li class="li">Support for the <a class="xref" href="../Destinations/MongoDB.html#concept_bkc_m24_4v">Replace and Update operations</a>. </li>

                                    <li class="li">Support for an <a class="xref" href="../Destinations/MongoDB.html#concept_syh_s1l_tbb">Upsert flag</a> that, when enabled, is used with
                                        both the Replace and Update operations. </li>

                                </ul>
<p class="p">For information about upgrading existing upsert pipelines,
                                    see <a class="xref" href="../Upgrade/PostUpgrade.html#concept_ncs_5jl_wbb">Update MongoDB Destination Upsert Pipelines</a>.
                                </p>
</li>

                            <li class="li">Redis destination enhancement - The destination now supports
                                processing data using <a class="xref" href="../Destinations/Redis.html#concept_dz2_4xh_xbb">CRUD operations stored in record header attributes</a>. </li>

                            <li class="li"><a class="xref" href="../Destinations/Salesforce.html#concept_xql_wbj_mcb" title="When you configure the Salesforce destination, you can override the default mapping of case-sensitive field names by mapping specific fields in the record to existing fields in the Salesforce object.">Salesforce destination enhancement</a> - When using the
                                Salesforce Bulk API to update, insert, or upsert data, you can now
                                use a colon (:) or period (.) as a field separator when defining the
                                Salesforce field to map the Data Collector field to. For example,
                                    <code class="ph codeph">Parent__r:External_Id__c</code> or
                                    <code class="ph codeph">Parent__r.External_Id__c</code> are both valid
                                Salesforce fields. </li>

                            <li class="li">Wave Analytics destination rename - With this release, the Wave
                                Analytics destination is now named the <a class="xref" href="../Destinations/WaveAnalytics.html#concept_hlx_r53_rx">Einstein Analytics destination</a>, following the recent
                                Salesforce rebranding. All of the properties and functionality of
                                the destination remain the same. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Executor</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_cjx_y4k_wbb__ul_if5_jgl_wbb">
                            <li class="li"><a class="xref" href="../Executors/HiveQuery.html#task_mgm_4lk_fx">Hive Query executor enhancement</a> - You can now configure
                                additional JDBC configuration properties to pass to the JDBC driver. </li>

                            <li class="li"><a class="xref" href="../Executors/JDBCQuery.html#task_ym2_3cv_sx">JDBC Query executor enhancement</a> - You can now specify an
                                Init Query to be executed after establishing a connection to the
                                database, before performing other tasks. This can be used, for
                                example, to modify session attributes. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Cloudera Navigator </dt>

                    <dd class="dd">Cloudera Navigator integration is now released as part of the StreamSets
                        Commercial Subscription. The beta version included in earlier releases is no
                        longer available with <span class="ph">Data Collector</span>. For information about the <span class="ph">StreamSets</span> Commercial Subscription, <a class="xref" href="https://streamsets.com/contact-us/" target="_blank">contact <span class="ph">StreamSets</span></a>.<p dir="ltr" class="p">For information about upgrading a version of <span class="ph">Data Collector</span> with Cloudera Navigator integration enabled, see <a class="xref" href="../Upgrade/PostUpgrade.html#concept_wnp_scs_wbb">Disable Cloudera Navigator Integration</a>.</p>
</dd>

                
                
                    <dt class="dt dlterm">Credential Stores</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_cjx_y4k_wbb__ul_o1b_mgl_wbb">
                            <li class="li"><a class="xref" href="../Configuration/CredentialStores.html#concept_v21_nvd_fbb">CyberArk</a> - Data Collector now provides a credential
                                store implementation for CyberArk Application Identity Manager. You
                                can define the credentials required by external systems - user names
                                or passwords - in CyberArk. Then you use credential expression
                                language functions in JDBC stage properties to retrieve those
                                values, instead of directly entering credential values in stage
                                properties. </li>

                            <li class="li"><a class="xref" href="../Configuration/CredentialStores.html#concept_bt1_bpj_r1b">Supported stages</a> - You can now use the credential
                                functions in all stages that require you to enter sensitive
                                information. Previously, you could only use the credential functions
                                in JDBC stages. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm"><span class="ph">Data Collector</span> Configuration</dt>

                    <dd class="dd">By default when <span class="ph">Data Collector</span> restarts, it automatically restarts all pipelines that were running
                        before Data Collector shut down. You can now disable the automatic restart
                        of pipelines by <a class="xref" href="../Configuration/DCConfig.html#task_lxk_kjw_1r__table_k1y_drr_bx">configuring the <code class="ph codeph">runner.boot.pipeline.restart</code>
                            property</a> in the <code class="ph codeph">$SDC_CONF/sdc.properties</code> file.
                    </dd>

                
                
                    <dt class="dt dlterm">Dataflow Performance Manager / <span class="ph">StreamSets Control Hub</span></dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_cjx_y4k_wbb__ul_p2b_4hl_wbb">
                            <li class="li"><span class="ph">StreamSets Control Hub</span> - With this release, we have created a new product called <a class="xref" href="https://streamsets.com/products/sch" target="_blank"><span class="ph">StreamSets Control Hub</span><sup class="ph sup">TM</sup></a> that includes a number of new
                                cloud-based dataflow design, deployment, and scale-up features.
                                Since this release is now our core service for controlling
                                dataflows, we have renamed the StreamSets cloud experience from
                                "Dataflow Performance Manager (DPM)" to "<span class="ph">StreamSets Control Hub</span>".<p class="p">DPM now refers to the performance management functions
                                    that reside in the cloud such as live metrics and data SLAs.
                                    Customers who have purchased the StreamSets Enterprise Edition
                                    will gain access to all <span class="ph">Control Hub</span> functionality and continue to have access to all DPM
                                    functionality as before.</p>
<p class="p">To understand the end-to-end
                                    StreamSets Data Operations Platform and how the products fit
                                    together, visit <a class="xref" href="https://streamsets.com/products/" target="_blank">https://streamsets.com/products/</a>. </p>
</li>

                            <li class="li"><a class="xref" href="../DPM/AggregatedStatistics.html#concept_qh5_v5t_mbb" title="When you write statistics to MapR Streams, Data Collector effectively adds a MapR Streams Producer destination to the pipeline that you are configuring. Control Hub automatically generates and runs a system pipeline for the job. The system pipeline reads the statistics from MapR Streams, and then aggregates and sends the statistics to Control Hub.">Aggregated statistics</a> - When working with <span class="ph">Control Hub</span>, you can now configure a pipeline to write aggregated statistics
                                to MapR Streams. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Data Formats</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_cjx_y4k_wbb__ul_sp4_rhl_wbb">
                            <li class="li"><a class="xref" href="../Data_Formats/NetFlow_Overview.html#concept_thl_nnr_hbb">New NetFlow 9 support</a> - <span class="ph">Data Collector</span> now supports processing NetFlow 9 template-based messages. Stages
                                that previously processed NetFlow 5 data can now process NetFlow 9
                                data as well. </li>

                            <li class="li">Datagram data format enhancement - The Datagram Data Format property
                                is now named the Datagram Packet Format. </li>

                            <li class="li">Delimited data format enhancement - <span class="ph">Data Collector</span> can now process data using the Postgres CSV and Postgres Text
                                delimited format types. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Expression Language</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_cjx_y4k_wbb__ul_ppq_whl_wbb">
                            <li class="li"><a class="xref" href="../Pipeline_Configuration/Expressions.html#concept_ir4_rxt_3cb">New field path expressions</a> - You can use field path
                                expressions in certain stages to specify the fields to use in an
                                expression. </li>

                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_gfs_w55_3cb" title="You can use field functions in field path expressions that determine the set of fields that a processor uses. Each function is evaluated against a set of matching fields individually.">New field functions</a> - You can use the following new
                                field functions in field path expressions:<ul class="ul" id="concept_cjx_y4k_wbb__ul_avp_bws_kcb">
                                    <li class="li">f:attribute() - Returns the value of the specified
                                        attribute.</li>

                                    <li class="li">f:path() - Returns the path of a field. </li>

                                    <li class="li">f:type() - Returns the data type of a field.</li>

                                    <li class="li">f:value() - Returns the value of a field.</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_ahp_f4v_1r">New string functions</a> - The release includes the
                                following new functions:<ul class="ul" id="concept_cjx_y4k_wbb__ul_uss_whl_wbb">
                                    <li class="li">str:isNullOrEmpty() - Returns true or false based on whether
                                        a string is null or is the empty string.</li>

                                    <li class="li">str:splitKV() - Splits key-value pairs in a string into a
                                        map of string values.</li>

                                </ul>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Stage Libraries</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_cjx_y4k_wbb__ul_qhq_c3l_wbb">
                            <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">New stage libraries</a> - This release includes the
                                following new stage libraries:<ul class="ul" id="concept_cjx_y4k_wbb__ul_rcr_23l_wbb">
                                    <li class="li">Apache Kafka 1.0 </li>

                                    <li class="li">Apache Kafka 0.11 </li>

                                    <li class="li">Apache Kudu 1.5 </li>

                                    <li class="li">Cloudera CDH 5.13 </li>

                                    <li class="li">Cloudera Kafka 3.0.0 (0.11.0) </li>

                                    <li class="li">Hortonworks 2.6.1, including Hive 1.2 </li>

                                    <li class="li">Hortonworks 2.6.2, including Hive 1.2 and 2.0 </li>

                                    <li class="li">MapR version 6.0 (MEP 4)</li>

                                    <li class="li">MapR Spark 2.1 (MEP 3) </li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_fw3_zt3_tbb">Legacy stage libraries</a> - Stage libraries that are more
                                than two years old are no longer included with <span class="ph">Data Collector</span>. Though not recommended, you can still download and install the
                                older stage libraries as custom stage libraries. <p class="p">If you have
                                    pipelines that use these legacy stage libraries, you will need
                                    to update the pipelines to use a more current stage library or
                                    install the legacy stage library manually, For more information
                                    see <a class="xref" href="../Upgrade/PostUpgrade.html#concept_wnl_zk4_5bb">Update Pipelines using Legacy Stage
                                Libraries</a>.</p>
</li>

                            <li class="li"><a class="xref" href="../Installation/CoreInstall_Overview.html#concept_vvw_p3m_s5" title="Users with a StreamSets enterprise account can use the Data Collector core installation.">Statistics stage library enhancement</a> - The statistics
                                stage library is now included in the core <span class="ph">Data Collector</span> installation. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Miscellaneous</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_cjx_y4k_wbb__ul_fw1_s3l_wbb">
                            <li class="li"><a class="xref" href="../Pipeline_Design/DatainMotion.html#concept_hjl_vyd_kcb">New data type</a> - <span class="ph">Data Collector</span> now supports the Zoned Datetime data type.</li>

                            <li class="li"><a class="xref" href="../Administration/Administration_title.html#task_wvx_4hk_br" title="You can view metrics about Data Collector, such as the CPU usage or the number of pipeline runners in the thread pool.">New <span class="ph">Data Collector</span> metrics</a> - JVM metrics have been renamed <span class="ph">Data Collector</span> Metrics and now include general <span class="ph">Data Collector</span> metrics in addition to JVM metrics. The JVM Metrics menu item has
                                also been renamed SDC Metrics.</li>

                            <li class="li"><a class="xref" href="../Pipeline_Design/ErrorHandling.html#concept_kgc_l4y_5r">Pipeline error records </a>- You can now write error records
                                to Google Pub/Sub, Google Cloud Storage, or an MQTT broker.</li>

                            <li class="li">Snapshot enhancements:<ul class="ul" id="concept_cjx_y4k_wbb__ul_cv5_53l_wbb">
                                    <li class="li">Standalone pipelines can now automatically take a <a class="xref" href="../Pipeline_Monitoring/PipelineMonitoring_title.html#concept_vkf_xjf_ybb" title="A failure snapshot is a partial snapshot that occurs automatically when the pipeline stops due to unexpected data. You can view the failure snapshot to troubleshoot the problem.">failure snapshot</a> when the pipeline fails due to
                                        a data-related exception. </li>

                                    <li class="li">You can now <a class="xref" href="../Pipeline_Monitoring/PipelineMonitoring_title.html#task_ut4_gvw_2cb" title="When needed, you can download a snapshot. You might download a snapshot from a production Data Collector so you can review it on a development Data Collector. Or you might download a snapshot to use the Dev Snapshot Replaying origin to read records from the downloaded file.">download snapshots through the UI</a> and the REST
                                        API.</li>

                                </ul>
</li>

                            <li class="li">Time zone enhancement - Time zones have been organized and updated
                                to use JDK 8 names. This should make it easier to select time zones
                                in stage properties. In the rare case that your pipeline uses a
                                format not supported by JDK 8, edit the pipeline to select a
                                compatible time zone.</li>

                        </ul>

                    </dd>

                
            </dl>
</div>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title32" id="concept_rrq_v3k_kbb">
 <h2 class="title topictitle2" id="ariaid-title32">What's New in 2.7.2.0</h2>

 <div class="body conbody">
  <div class="p"><span class="ph">Data Collector</span>
            version 2.7.2.0 includes the following new features:<dl class="dl">
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_rrq_v3k_kbb__ul_ibg_djk_kbb">
                            <li class="li"><a class="xref" href="../Origins/KafkaMultiConsumer.html#concept_ccs_fn4_x1b">New Kafka Multitopic Consumer origin</a> - A new origin that
                                reads messages from multiple Kafka topics. Creates multiple threads
                                to enable parallel processing in a multithreaded pipeline. </li>

                            <li class="li"><a class="xref" href="../Origins/KinConsumer.html#task_p4b_vv4_yr">Kinesis Consumer origin enhancement</a> - You can now
                                configure the origin to start reading messages from a specified
                                timestamp.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Destinations</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_rrq_v3k_kbb__ul_crw_frk_kbb">
                            <li class="li"><a class="xref" href="../Destinations/BigQuery.html#concept_hj4_brk_dbb">New
                                    Google BigQuery destination</a> - A new destination that
                                streams data into Google BigQuery.</li>

                        </ul>

                    </dd>

                
            </dl>
</div>

 </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title33" id="concept_rr2_mbz_w1b">
    <h2 class="title topictitle2" id="ariaid-title33">What's New in 2.7.1.1</h2>

    <div class="body conbody">
        <div class="p"><span class="ph">Data Collector</span>
            version 2.7.1.1 includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_rr2_mbz_w1b__ul_gr5_hvr_1bb">
                            <li class="li"><a class="xref" href="../Origins/AmazonS3.html#task_gfj_ssv_yq">Amazon S3 origin enhancement</a> - You can now specify a
                                Connection Timeout advanced property. </li>

                            <li class="li"><a class="xref" href="../Origins/MultiTableJDBCConsumer.html#concept_m43_zzm_dbb" title="The JDBC Multitable Consumer origin can read from views in addition to tables.">JDBC Multitable Consumer origin enhancement</a> - You can
                                now use the origin to read from views in addition to tables.</li>

                            <li class="li"><a class="xref" href="../Origins/OPCUAClient.html#task_bqt_mx3_h1b">OPC UA Client origin enhancement</a> - You can now configure
                                channel properties, such as the maximum chunk or message size.</li>

                            <li class="li"><a class="xref" href="../Origins/OracleCDC.html#task_ehh_mjj_tw">Oracle CDC Client origin enhancement </a>- You can now
                                configure a JDBC Fetch Size property to determine the minimum number
                                of records that the origin waits for before passing data to the
                                pipeline. When writing to the destination is slow, use the default
                                of 1 record to improve performance. Previously, the origin used the
                                Oracle JDBC driver default of 10 records.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Executor</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_rr2_mbz_w1b__ul_zzr_ztn_2bb">
                            <li class="li"><a class="xref" href="../Executors/MapRFSFileMeta.html#concept_ohx_r5h_z1b">New MapR FS File Metadata executor</a> - The new executor
                                can change file metadata, create an empty file, or remove a file or
                                directory in MapR each time it receives an event. </li>

                        </ul>

                    </dd>

                
            </dl>
</div>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title34" id="unique_1138101209">
    <h2 class="title topictitle2" id="ariaid-title34">What's New in 2.7.1.0</h2>

    <div class="body conbody">
        <div class="p"><span class="ph">Data Collector</span>
            version 2.7.1.0 includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Stage Libraries</dt>

                    <dd class="dd">Data Collector includes the following upgraded stage library:<ul class="ul" id="unique_1138101209__ul_s1w_cvr_1bb">
                            <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">Jython 2.7.1</a></li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">
                        <ul class="ul" id="unique_1138101209__ul_gr5_hvr_1bb">
                            <li class="li"><a class="xref" href="../Origins/AzureEventHub.html#concept_c1z_15q_1bb">New
                                    Azure Event Hub Consumer origin</a> - A multithreaded origin
                                that reads data from Microsoft Azure Event Hub.</li>

                            <li class="li"><a class="xref" href="../Origins/OPCUAClient.html#concept_p25_wm2_dbb">OPC UA Client origin enhancement</a> - You can now specify
                                node information in a file. Or have the origin browse for nodes to
                                use based on a specified root node.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Processors</dt>

                    <dd class="dd">
                        <ul class="ul" id="unique_1138101209__ul_rh5_3vr_1bb">
                            <li class="li"><a class="xref" href="../Processors/SchemaGenerator.html#concept_rfz_ks3_x1b">New Schema Generator processor</a> - A processor that
                                generates a schema for each record and writes the schema to a record
                                header attribute. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Destinations</dt>

                    <dd class="dd">
                        <ul class="ul" id="unique_1138101209__ul_s22_lvr_1bb">
                            <li class="li"><a class="xref" href="../Destinations/AzureEventHubProducer.html#concept_xq5_d5q_1bb">New Azure Event Hub Producer destination</a> - A destination
                                that writes data to Microsoft Azure Event Hub.</li>

                            <li class="li"><a class="xref" href="../Destinations/AzureIoTHub.html#concept_pnd_jkq_1bb">New Azure IoT Hub Producer destination</a> - A destination
                                that writes data to Microsoft Azure IoT Hub.</li>

                        </ul>

                    </dd>

                
            </dl>
</div>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title35" id="unique_184236386">
    <h2 class="title topictitle2" id="ariaid-title35">What's New in 2.7.0.0</h2>

    <div class="body conbody">
        <p class="p"><span class="ph">Data Collector</span>
            version 2.7.0.0 includes the following new features and enhancements:</p>

        <dl class="dl">
            
                <dt class="dt dlterm">Credential Stores</dt>

                <dd class="dd"><span class="ph">Data Collector</span> now has a <a class="xref" href="../Configuration/CredentialStores.html#concept_bt1_bpj_r1b">credential
                        store</a> API that integrates with the following credential store
                        systems:<ul class="ul" id="unique_184236386__ul_vgr_3cz_w1b">
                        <li class="li">Java keystore</li>

                        <li class="li">Hashicorp Vault</li>

                    </ul>
</dd>

                <dd class="dd ddexpand">
                    <p class="p">You define the credentials required by external systems - user names,
                        passwords, or access keys - in a Java keystore file or in Vault. Then you
                        use <a class="xref" href="../Expression_Language/Functions.html#concept_yvc_3qs_r1b">credential expression language functions</a> in JDBC stage
                        properties to retrieve those values, instead of directly entering credential
                        values in stage properties. </p>

                </dd>

                <dd class="dd ddexpand">
                    <div class="p">The following JDBC stages can use the new credential functions:<ul class="ul" id="unique_184236386__ul_ezm_4cz_w1b">
                            <li class="li">JDBC Multitable Consumer origin</li>

                            <li class="li">JDBC Query Consumer origin</li>

                            <li class="li">Oracle CDC Client origin</li>

                            <li class="li">SQL Server CDC Client origin</li>

                            <li class="li">SQL Server Change Tracking origin</li>

                            <li class="li">JDBC Lookup processor</li>

                            <li class="li">JDBC Tee processor</li>

                            <li class="li">JDBC Producer destination</li>

                            <li class="li">JDBC Query executor</li>

                        </ul>
</div>

                </dd>

            
            
                <dt class="dt dlterm">Publish Pipeline Metadata to Cloudera Navigator (Beta)</dt>

                <dd class="dd">
                    <p class="p"><span class="ph">Data Collector</span> now provides beta support for publishing metadata about running pipelines
                        to Cloudera Navigator. You can then use Cloudera Navigator to explore the
                        pipeline metadata, including viewing lineage diagrams of the metadata.</p>

                </dd>

                <dd class="dd ddexpand">Feel free to try out this feature in a development or test <span class="ph">Data Collector</span>, and send us your feedback. We are continuing to refine metadata publishing
                    as we gather input from the community and work with Cloudera.</dd>

            
            
                <dt class="dt dlterm">Stage Libraries</dt>

                <dd class="dd">
                    <div class="p">Data Collector includes the following new stage libraries:<ul class="ul" id="unique_184236386__ul_cxp_d2z_w1b">
                            <li class="li">Apache Kudu version 1.4.0</li>

                            <li class="li">Cloudera CDH version 5.12 distribution of Hadoop</li>

                            <li class="li">Cloudera version 5.12 distribution of Apache Kafka 2.1</li>

                            <li class="li">Google Cloud - Includes the Google BigQuery origin, Google Pub/Sub
                                Subscriber origin, and Google Pub/Sub Publisher destination.</li>

                            <li class="li">Java keystore credential store - For use with <a class="xref" href="../Configuration/CredentialStores.html#concept_bt1_bpj_r1b">credential stores</a>.</li>

                            <li class="li">Vault credential store - For use with <a class="xref" href="../Configuration/CredentialStores.html#concept_bt1_bpj_r1b">credential stores</a>.</li>

                        </ul>
</div>

                </dd>

            
            
                <dt class="dt dlterm">Data Collector Configuration</dt>

                <dd class="dd">
                    <ul class="ul" id="unique_184236386__ul_hc5_q2z_w1b">
                        <li class="li"><a class="xref" href="../Configuration/Vault-Overview.html#concept_bmq_gl1_mw">Access Hashicorp Vault secrets</a> - The Data Collector Vault
                            integration now relies on Vault's App Role authentication backend.
                            Previously, Data Collector relied on Vault's App ID authentication
                            backend. Hashicorp has deprecated the App ID authentication
                            backend.</li>

                        <li class="li"><a class="xref" href="../Configuration/DCConfig.html#concept_dcq_mpk_f1b">New Hadoop user impersonation property</a> - When you enable
                            Data Collector to impersonate the current Data Collector user when
                            writing to Hadoop, you can now also configure Data Collector to make the
                            username lowercase. This can be helpful with case-sensitive
                            implementations of LDAP.</li>

                        <li class="li"><a class="xref" href="../Configuration/DCConfig.html#task_lxk_kjw_1r" title="You can customize Data Collector by editing the Data Collector configuration file, sdc.properties.">New Java security properties</a> - The Data Collector
                            configuration file now includes properties with a "java.security."
                            prefix, which you can use to configure Java security properties.</li>

                        <li class="li"><a class="xref" href="../Configuration/DCConfig.html#task_lxk_kjw_1r" title="You can customize Data Collector by editing the Data Collector configuration file, sdc.properties.">New property to define the amount of time to cache DNS
                                lookups</a> - By default, the
                            java.security.networkaddress.cache.ttl property is set to 0 so that the
                            JVM uses the Domain Name Service (DNS) time to live value, instead of
                            caching the lookups for the lifetime of the JVM.</li>

                        <li class="li"><a class="xref" href="../Configuration/DCEnvironmentConfig.html#concept_cy3_r44_z1b">SDC_HEAPDUMP_PATH enhancement</a> - The new default file name,
                                <code class="ph codeph">$SDC_LOG/sdc_heapdump_${timestamp}.hprof</code>, includes
                            a timestamp so you can write multiple heap dump files to the specified
                            directory.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Dataflow Triggers</dt>

                <dd class="dd">
                    <ul class="ul" id="unique_184236386__ul_fg2_gfz_w1b">
                        <li class="li"><a class="xref" href="../Event_Handling/EventFramework-Title.html#concept_amg_2qr_t1b">Pipeline events</a> - The event framework now generates pipeline
                            lifecycle events when the pipeline stops and starts. You can pass each
                            pipeline event to an executor or to another pipeline for more complex
                            processing. Use pipeline events to trigger tasks before pipeline
                            processing begins or after it stops.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Origins</dt>

                <dd class="dd">
                    <ul class="ul" id="unique_184236386__ul_tpp_sfz_w1b">
                        <li class="li"><a class="xref" href="../Origins/BigQuery.html#concept_cg3_y3v_q1b">New Google
                                BigQuery origin</a> - An origin that executes a query job and
                            reads the result from Google BigQuery.</li>

                        <li class="li"><a class="xref" href="../Origins/PubSub.html#concept_pjw_qtl_r1b">New Google
                                Pub/Sub Subscriber origin</a> - A multithreaded origin that
                            consumes messages from a Google Pub/Sub subscription.</li>

                        <li class="li"><a class="xref" href="../Origins/OPCUAClient.html#concept_nmf_1ly_f1b">New OPC UA
                                Client origin</a> - An origin that processes data from an OPC UA
                            server.</li>

                        <li class="li"><a class="xref" href="../Origins/SQLServerCDC.html#concept_ut3_ywc_v1b">New SQL
                                Server CDC Client origin</a> - A multithreaded origin that reads
                            data from Microsoft SQL Server CDC tables. </li>

                        <li class="li"><a class="xref" href="../Origins/SQLServerChange.html#concept_ewq_b2s_r1b">New SQL
                                Server Change Tracking origin</a> - A multithreaded origin that
                            reads data from Microsoft SQL Server change tracking tables and
                            generates the latest version of each record.</li>

                        <li class="li"><a class="xref" href="../Origins/Directory.html#concept_ttg_vgn_qx">Directory origin event enhancements</a> - The Directory origin
                            can now generate no-more-data events when it completes processing all
                            available files and the batch wait time has elapsed without the arrival
                            of new files. Also, the File Finished event now includes the number of
                            records and files processed. </li>

                        <li class="li"><a class="xref" href="../Origins/HadoopFS-origin.html#concept_ogc_xzd_f1b" title="The Hadoop FS origin included in a cluster batch pipeline allows you to read from file systems other than HDFS using the Hadoop FileSystem interface.">Hadoop
                                FS origin enhancement</a> - The Hadoop FS origin now allows you
                            to read data from other file systems using the Hadoop FileSystem
                            interface. Use the Hadoop FS origin in cluster batch pipelines.</li>

                        <li class="li"><a class="xref" href="../Origins/HTTPClient.html#task_akl_rkz_5r">HTTP
                                Client origin enhancement</a> - The HTTP Client origin now allows
                            time functions and datetime variables in the request body. It also
                            allows you to specify the time zone to use when evaluating the request
                            body. </li>

                        <li class="li"><a class="xref" href="../Origins/HTTPServer.html#concept_anf_ss4_qy">HTTP Server origin enhancement</a> - The HTTP Server origin can
                            now process Avro files.</li>

                        <li class="li"><a class="xref" href="../Origins/JDBCConsumer.html#task_ryz_tkr_bs">JDBC Query Consumer origin enhancement</a> - You can now
                            configure the behavior for the origin when it encounters data of an
                            unknown data type.</li>

                        <li class="li"><a class="xref" href="../Origins/MultiTableJDBCConsumer.html#concept_zp3_wnw_4y">JDBC Multitable Consumer origin enhancements</a>:<ul class="ul" id="unique_184236386__ul_gnl_rhz_w1b">
                                <li class="li">You can now use the origin to perform multithreaded processing
                                    of partitions within a table. Use partition processing to handle
                                    even larger volumes of data. This enhancement also includes new
                                    JDBC header attributes.<p class="p">By default, all new pipelines use
                                        partition processing when possible. Upgraded pipelines use
                                        multithreaded table processing to preserve previous
                                        behavior.</p>
</li>

                                <li class="li">You can now configure the behavior for the origin when it
                                    encounters data of an unknown data type.</li>

                            </ul>
</li>

                        <li class="li"><a class="xref" href="../Origins/OracleCDC.html#concept_rs5_hjj_tw">Oracle CDC
                                Client origin enhancements</a>:<ul class="ul" id="unique_184236386__ul_ych_yhz_w1b">
                                <li class="li">The origin can now buffer data locally rather than utilizing
                                    Oracle LogMiner buffers.</li>

                                <li class="li">You can now specify the behavior when the origin encounters an
                                    unsupported field type - send to the pipeline, send to error, or
                                    discard.</li>

                                <li class="li">You can configure the origin to include null values passed from
                                    the LogMiner full supplemental logging. By default, the origin
                                    ignores null values.</li>

                                <li class="li">You now must select the target server time zone for the origin. </li>

                                <li class="li">You can now configure a query timeout for the origin.</li>

                                <li class="li">The origin now includes the row ID in the oracle.cdc.rowId
                                    record header attribute and can include the LogMiner redo query
                                    in the oracle.cdc.query record header attribute.</li>

                            </ul>
</li>

                        <li class="li"><a class="xref" href="../Origins/RabbitMQ.html#concept_rg5_yts_y1b">RabbitMQ Consumer origin enhancement</a> - When available, the
                            origin now provides attributes generated by RabbitMQ, such as
                            contentType, contentEncoding, and deliveryMode, as record header
                            attributes.</li>

                        <li class="li"><a class="xref" href="../Origins/TCPServer.html#concept_bqt_tl4_sz">TCP
                                Server origin enhancement</a> - The origin can now process
                            character-based data that includes a length prefix. </li>

                        <li class="li"><a class="xref" href="../Origins/UDP.html#concept_jhh_ryx_r1b">UDP Source
                                origin enhancement</a> - The origin can now process binary and
                            character-based raw data.</li>

                        <li class="li">New last-modified time record header attribute - <a class="xref" href="../Origins/Directory.html#concept_tlj_3g1_2z" title="The Directory origin creates record header attributes that include information about the originating file for the record.">Directory</a>, <a class="xref" href="../Origins/FileTail.html#concept_tlj_3g1_2z">File Tail</a>, and <a class="xref" href="../Origins/SFTP.html#concept_tlj_3g1_2z">SFTP/FTP Client</a> origins now include the last modified time
                            for the originating file for a record in an mtime record header
                            attribute.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Processors</dt>

                <dd class="dd">
                    <ul class="ul" id="unique_184236386__ul_py4_vmz_w1b">
                        <li class="li"><a class="xref" href="../Processors/DataParser.html#concept_xw3_4xk_r1b">New Data
                                Parser processor</a> - Use the new processor to extract NetFlow
                            or syslog messages as well as other supported data formats that are
                            embedded in a field. </li>

                        <li class="li"><a class="xref" href="../Processors/JSONParser.html#concept_bs1_4t3_yq">New JSON
                                Generator processor</a> - Use the new processor to serialize data
                            from a record field to a JSON-encoded string.</li>

                        <li class="li"><a class="xref" href="../Processors/KuduLookup.html#concept_a1x_3wl_p1b">New Kudu
                                Lookup processor</a> - Use the new processor to perform lookups
                            in Kudu to enrich records with additional data.</li>

                        <li class="li"><a class="xref" href="../Processors/HiveMetadata.html#concept_jv2_jjn_l1b">Hive Metadata processor enhancement</a> - You can now configure
                            custom record header attributes for metadata records.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Destinations</dt>

                <dd class="dd">
                    <ul class="ul" id="unique_184236386__ul_wdl_lnz_w1b">
                        <li class="li"><a class="xref" href="../Destinations/PubSubPublisher.html#concept_qsj_hk1_v1b">New Google Pub/Sub Publisher destination</a> - A destination
                            that publishes messages to Google Pub/Sub.</li>

                        <li class="li"><a class="xref" href="../Destinations/JMSProducer.html#concept_sfz_ww5_n1b">New
                                JMS Producer destination</a> - A destination that writes data to
                            JMS.</li>

                        <li class="li">Amazon S3 destination enhancements:<ul class="ul" id="unique_184236386__ul_ls3_d4z_w1b">
                                <li class="li">You can now use expressions in the <a class="xref" href="../Destinations/AmazonS3.html#concept_bnp_gwp_f1b">Bucket property</a> for the Amazon S3 destination. This
                                    enables you to write records dynamically based expression
                                    evaluation.</li>

                                <li class="li">The Amazon S3 object written <a class="xref" href="../Destinations/AmazonS3.html#concept_nly_sw2_px">event record</a> now includes the number of records
                                    written to the object.</li>

                            </ul>
</li>

                        <li class="li"><a class="xref" href="../Destinations/DataLakeStore.html#task_jfl_nf4_zx">Azure Data Lake Store destination enhancement</a> - The Client
                            ID and Client Key properties have been renamed Application ID and
                            Application Key to align with the updated property names in the new
                            Azure portal.</li>

                        <li class="li"><a class="xref" href="../Destinations/Cassandra.html#concept_ajh_vhp_x1b">Cassandra destination enhancement</a> - The destination now
                            supports Kerberos authentication if you have installed the DataStax
                            Enterprise Java driver. </li>

                        <li class="li"><a class="xref" href="../Destinations/Elasticsearch.html#task_uns_gtv_4r">Elasticsearch destination enhancement</a> - The destination can
                            now create parent-child relationships between documents in the same
                            index.</li>

                        <li class="li"><a class="xref" href="../Destinations/HiveMetastore.html#concept_gcr_z2t_zv">Hive
                                Metastore destination</a> - You can now configure the destination
                            to create custom record header attributes.</li>

                        <li class="li"><a class="xref" href="../Destinations/KProducer.html#concept_lww_3b3_kra">Kafka Producer destination enhancement</a> - The destination can
                            now write XML documents.</li>

                        <li class="li"><a class="xref" href="../Destinations/Solr.html#task_ld1_phr_wr">Solr
                                destination enhancement</a> - You can now configure the
                            destination to skip connection validation when the Solr configuration
                            file, <code class="ph codeph">solrconfig.xml</code>, does not define the default
                            search field (df) parameter.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Executors</dt>

                <dd class="dd">
                    <ul class="ul" id="unique_184236386__ul_gqx_fpz_w1b">
                        <li class="li"><a class="xref" href="../Executors/AmazonS3.html#concept_mvh_bnm_f1b">New Amazon
                                S3 executor</a> - Use the Amazon S3 executor to create new Amazon
                            S3 objects for the specified content or add tags to existing objects
                            each time it receives an event.</li>

                        <li class="li"><a class="xref" href="../Executors/HDFSMetadata.html#concept_yf2_hc4_x1b">HDFS File Metadata executor enhancement</a> - The executor can
                            now remove a file or directory when it receives an event.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Dataflow Performance Manager</dt>

                <dd class="dd">
                    <ul class="ul" id="unique_184236386__ul_dwx_lpz_w1b">
                        <li class="li"><a class="xref" href="../DPM/PipelineManagement.html#task_c4x_vff_p1b">Revert changes to published pipelines</a> - If you update a
                            published pipeline but decide not to publish the updates to DPM as a new
                            version, you can revert the changes made to the pipeline
                            configuration.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Pipelines</dt>

                <dd class="dd">
                    <ul class="ul" id="unique_184236386__ul_ngb_qpz_w1b">
                        <li class="li"><a class="xref" href="../Pipeline_Design/ErrorHandling.html#concept_pm4_txm_vq">Pipeline error handling enhancements</a>: <ul class="ul" id="unique_184236386__ul_rq1_tpz_w1b">
                                <li class="li">Use the new Error Record Policy to specify the version of the
                                    record to include in error records. </li>

                                <li class="li">You can now write error records to Amazon Kinesis Streams.</li>

                            </ul>
</li>

                        <li class="li"><a class="xref" href="../Pipeline_Design/RecordHeaderAttributes.html#concept_itf_55z_dz">Error records enhancement</a> - Error records now include the
                            user-defined stage label in the errorStageLabel header attribute.</li>

                        <li class="li"><a class="xref" href="../Pipeline_Maintenance/PipelineStates-Understanding.html#concept_s4p_ns5_nz">Pipeline state enhancements</a> - Pipelines can now display the
                            following new states: STARTING_ERROR, STOPPING_ERROR, and
                            STOP_ERROR.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Data Formats</dt>

                <dd class="dd">
                    <ul class="ul" id="unique_184236386__ul_btq_yqz_w1b">
                        <li class="li"><a class="xref" href="../Data_Formats/WritingXML.html#concept_t2m_hhx_41b">Writing
                                XML</a> - You can now use the Google Pub/Sub Publisher, JMS
                            Producer, and Kafka Producer destinations to write XML documents to
                            destination systems. Note the record structure requirement before you
                            use this data format. </li>

                        <li class="li">Avro:<ul class="ul" id="unique_184236386__ul_k4b_yrz_w1b">
                                <li class="li">Origins now write the Avro schema to an avroSchema record header
                                    attribute.</li>

                                <li class="li">Origins now include precision and scale <a class="xref" href="../Pipeline_Design/FieldAttributes.html#concept_xfm_wtp_1z" title="Field attributes are attributes that provide additional information about each field that you can use in pipeline logic, as needed.">field attributes</a> for every Decimal field.</li>

                                <li class="li">Data Collector now supports the time-based logical types added
                                    to Avro in version 1.8.</li>

                            </ul>
</li>

                        <li class="li">Delimited - Data Collector can now continue processing records with
                            delimited data when a row has more fields than the header. Previously,
                            rows with more fields than the header were sent to error. </li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Cluster Pipelines</dt>

                <dd class="dd">
                    <div class="p">This release includes the following <a class="xref" href="../Cluster_Mode/ClusterPipelines.html#concept_rjc_4m5_lx" title="Data Collector can run a cluster pipeline using cluster batch or cluster streaming execution mode.">Cluster Yarn Streaming enhancements</a>:<ul class="ul" id="unique_184236386__ul_b14_nsz_w1b">
                            <li class="li">Use a new Worker Count property to limit the number of worker nodes
                                used in Cluster Yarn Streaming pipelines. By default, a Data
                                Collector worker is spawned for each partition in the topic. </li>

                            <li class="li">You can now define Spark configuration properties to pass to the
                                spark-submit script.</li>

                        </ul>
</div>

                </dd>

            
            
                <dt class="dt dlterm">Expression Language</dt>

                <dd class="dd">
                    <div class="p">This release includes the following new functions:<ul class="ul" id="unique_184236386__ul_abf_qsz_w1b">
                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_yvc_3qs_r1b">credential:get()</a> - Returns credential values from a
                                credential store.</li>

                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_yvc_3qs_r1b">credential:getWithOptions()</a> - Returns credential values
                                from a credential store using additional options to communicate with
                                the credential store.</li>

                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_ndj_43v_1r" title="Error record functions provide information about error records. Use error functions to process error records.">record:errorStageLabel()</a> - Returns the user-defined name
                                of the stage that generated the error record.</li>

                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_ddw_ld1_1s">list:join()</a> - Merges elements in a List field into a
                                String field, using the specified separator between elements. </li>

                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_ddw_ld1_1s">list:joinSkipNulls()</a> - Merges elements in a List field
                                into a String field, using the specified separator between elements
                                and skipping null values.</li>

                        </ul>
</div>

                </dd>

                <dd class="dd ddexpand">
                    <div class="p">
                        <ul class="ul" id="unique_184236386__ul_k5c_rg4_z1b">
                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_ahp_f4v_1r">str:indexOf()</a> - Returns the index within a string of the
                                first occurrence of the specified subset of characters.</li>

                        </ul>

                    </div>

                </dd>

            
            
                <dt class="dt dlterm">Miscellaneous</dt>

                <dd class="dd">
                    <ul class="ul" id="unique_184236386__ul_y2k_htz_w1b">
                        <li class="li"><a class="xref" href="../Pipeline_Configuration/SimpleBulkEdit.html#concept_alb_b3y_cbb">Global bulk edit mode</a> - In any property where you would
                            previously click an Add icon to add additional configurations, you can
                            now switch to bulk edit mode to enter a list of configurations in JSON
                            format.</li>

                        <li class="li">Snapshot enhancement - Snapshots no longer produce empty batches when
                            waiting for data.</li>

                        <li class="li"><a class="xref" href="../Pipeline_Configuration/Webhooks.html#concept_rby_1rl_rz">Webhooks enhancement</a> - You can use several new pipeline
                            state notification parameters in webhooks.</li>

                    </ul>

                </dd>

            
        </dl>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title36" id="concept_avm_x1y_h1b">
 <h2 class="title topictitle2" id="ariaid-title36">What's New in 2.6.0.1</h2>

 <div class="body conbody">
  <div class="p"><span class="ph">Data Collector</span>
            version 2.6.0.1 includes the following enhancement:<ul class="ul" id="concept_avm_x1y_h1b__ul_x5t_gby_h1b">
                <li class="li"><a class="xref" href="../Origins/KinConsumer.html#concept_dt1_4mq_h1b">Kinesis
                        Consumer origin</a> - You can now reset the origin for Kinesis Consumer
                    pipelines. Resetting the origin for Kinesis Consumer differs from other origins,
                    so please note the requirement and guidelines.</li>

            </ul>
</div>

 </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title37" id="concept_bsw_cky_11b">
 <h2 class="title topictitle2" id="ariaid-title37">What's New in 2.6.0.0</h2>

 <div class="body conbody">
  <div class="p"><span class="ph">Data Collector</span>
            version 2.6.0.0 includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Installation</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_bsw_cky_11b__ul_s2s_lky_11b">
                            <li class="li"><a class="xref" href="../Installation/MapR-Prerequisites.html#concept_jgs_qpg_2v">MapR prerequisites</a> - You can now run the
                                    <code class="ph codeph">setup-mapr</code> command in interactive or
                                non-interactive mode. In interactive mode, the command prompts you
                                for the MapR version and home directory. In non-interactive mode,
                                you define the MapR version and home directory in environment
                                variables before running the command.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Stage Libraries</dt>

                    <dd class="dd">Data Collector now supports the following <a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">stage libraries</a>:<ul class="ul" id="concept_bsw_cky_11b__ul_zcv_4ky_11b">
                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p">Hortonworks version 2.6 distribution of Apache
                                    Hadoop</p>

                            </li>

                            <li class="li">Cloudera distribution of Spark 2.1</li>

                            <li class="li">MapR distribution of Spark 2.1</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Data Collector Configuration</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_bsw_cky_11b__ul_jys_cpm_d1b">
                            <li class="li"><a class="xref" href="../Configuration/DCConfig.html#task_lxk_kjw_1r" title="You can customize Data Collector by editing the Data Collector configuration file, sdc.properties.">New buffer size configuration</a> - You can now use a new
                                parser.limit configuration property to increase the Data Collector
                                parser buffer size. The parser buffer is used by the origin to
                                process many data formats, including Delimited, JSON, and XML. The
                                parser buffer size limits the size of the records that origins can
                                process. The Data Collector parser buffer size is 1048576 bytes by
                                default.</li>

                        </ul>

                    </dd>

                
            </dl>
<dl class="dl">
                
                    <dt class="dt dlterm">Drift Synchronization Solution for Hive</dt>

                    <dd class="dd"><ul class="ul" id="concept_bsw_cky_11b__ul_x5p_zky_11b">
                            <li class="li"><a class="xref" href="../Solutions/HiveDrift-Overview.html#concept_ndg_3zw_vz">Parquet support</a> - You can now use the <a class="xref" href="../Solutions/HiveDrift-Overview.html#concept_phk_bdf_2w" title="The Drift Synchronization Solution for Hive detects drift in incoming data and updates corresponding Hive tables.">Drift Synchronization Solution for Hive</a> to generate
                                Parquet files. Previously, the Data Synchronization Solution
                                supported only Avro data. This enhancement includes the following
                                    updates:<ul class="ul" id="concept_bsw_cky_11b__ul_dt1_sky_11b">
                                    <li class="li"><a class="xref" href="../Processors/HiveMetadata.html#concept_rz5_nft_zv">Hive Metadata processor</a> data format property -
                                        Use the new data format property to indicate the data format
                                        to use.</li>

                                    <li class="li">Parquet support in the <a class="xref" href="../Destinations/HiveMetastore.html#concept_gcr_z2t_zv">Hive Metastore destination</a> - The destination can
                                        now create and update Parquet tables in Hive. The
                                        destination no longer includes a data format property since
                                        that information is now configured in the Hive Metadata
                                        processor. </li>

                                </ul>
</li>

                        </ul>
See the documentation for implementation details and a <a class="xref" href="../Solutions/HiveDrift-Overview.html#concept_vl3_v2f_zz">Parquet case study</a>. </dd>

                
                
                    <dt class="dt dlterm">Multithreaded Pipelines</dt>

                    <dd class="dd">The <a class="xref" href="../Multithreaded_Pipelines/MultithreadedPipelines.html#concept_zpp_2xc_py">multithreaded framework</a> includes the following enhancements:<ul class="ul" id="concept_bsw_cky_11b__ul_a2d_4ry_11b">
                            <li class="li"><a class="xref" href="../Multithreaded_Pipelines/MultithreadedPipelines.html#concept_wcz_tpd_py">Origins for multithreaded pipelines</a> - You can now use
                                the following origins to create multithreaded pipelines:<ul class="ul" id="concept_bsw_cky_11b__ul_efl_4ry_11b">
                                    <li dir="ltr" class="li">
                                        <p dir="ltr" class="p"><a class="xref" href="../Origins/CoAPServer.html#concept_wfy_ghn_sz">CoAP Server origin</a></p>

                                    </li>

                                    <li dir="ltr" class="li">
                                        <p dir="ltr" class="p"><a class="xref" href="../Origins/TCPServer.html#concept_ppm_xb1_4z">TCP Server origin</a></p>

                                    </li>

                                </ul>
</li>

                            <li class="li">Multithreaded origin icons - The icons for multithreaded origins now
                                include the following multithreaded indicator: <img class="image" id="concept_bsw_cky_11b__image_al4_tpm_d1b" src="../Graphics/icon_Multithreaded.png" height="16" width="14" /><p class="p">For example, heres the updated Elasticsearch
                                    origin icon:</p>
<p class="p"><img class="image" id="concept_bsw_cky_11b__image_iml_ypm_d1b" src="../Graphics/Multithreaded-icon.png" height="40" width="63" /></p>
</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Dataflow Triggers / Event Framework</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_bsw_cky_11b__ul_ot4_ssy_11b">
                            <li class="li"><a class="xref" href="../Event_Handling/EventFramework-Title.html#concept_rxg_shn_lx">New executors</a> - You can now use the following executors
                                to perform tasks upon receiving an event:<ul class="ul" id="concept_bsw_cky_11b__ul_pd2_tsy_11b">
                                    <li class="li"><a class="xref" href="../Executors/Email.html#concept_sjs_sfp_qz">Email executor</a></li>

                                    <li class="li"><a class="xref" href="../Executors/Shell.html#concept_jsr_zpw_tz">Shell executor</a>
                                    </li>

                                </ul>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Dataflow Performance Manager (DPM)</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_bsw_cky_11b__ul_pht_hld_d1b">
                            <li class="li"><a class="xref" href="../DPM/AggregatedStatistics.html#concept_h2q_mb5_xw">Pipeline statistics</a> - You can now configure a pipeline
                                to <a class="xref" href="../DPM/AggregatedStatistics.html#concept_abc_1w1_c1b" title="When you write statistics directly to Control Hub, Control Hub does not generate a system pipeline for the job. Instead, the Data Collector or SDC Edge directly sends the statistics to Control Hub.">write statistics directly to DPM</a>. Write statistics
                                directly to DPM when you run a job for the pipeline on a single Data
                                Collector. <p class="p">When you run a job on multiple Data Collectors, a
                                    remote pipeline instance runs on each of the Data Collectors. To
                                    view aggregated statistics for the job within DPM, you must
                                    configure the pipeline to write the statistics to a Kafka
                                    cluster, Amazon Kinesis Streams, or SDC RPC.</p>
</li>

                            <li class="li"><a class="xref" href="../DPM/PipelineManagement.html#task_rxy_xqc_fx">Update
                                    published pipelines</a> - When you update a published
                                pipeline, Data Collector now displays a red asterisk next to the
                                pipeline name to indicate that the pipeline has been updated since
                                it was last published.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_bsw_cky_11b__ul_xb4_bty_11b">
                            <li class="li"><a class="xref" href="../Origins/CoAPServer.html#concept_wfy_ghn_sz">New CoAP
                                    Server origin</a> - An origin that listens on a CoAP endpoint
                                and processes the contents of all authorized CoAP requests. The
                                origin performs parallel processing and can generate multithreaded
                                pipelines. </li>

                            <li class="li"><a class="xref" href="../Origins/TCPServer.html#concept_ppm_xb1_4z">New TCP
                                    Server origin</a> - An origin that listens at the specified
                                ports, establishes TCP sessions with clients that initiate TCP
                                connections, and then processes the incoming data. The origin can
                                process NetFlow, syslog, and most Data Collector data formats as
                                separated records. You can configure custom acknowledgement messages
                                and use a new batchSize variable, as well as other expressions, in
                                the messages.</li>

                            <li class="li"><a class="xref" href="../Origins/SFTP.html#concept_g5p_5ks_b1b">SFTP/FTP Client origin enhancement</a> - You can now specify
                                the first file to process. This enables you to skip processing files
                                with earlier timestamps. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Processors</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_bsw_cky_11b__ul_dvb_jty_11b">
                            <li class="li"><a class="xref" href="../Processors/Groovy.html#concept_ldh_sct_gv">Groovy</a>, <a class="xref" href="../Processors/JavaScript.html#concept_n2p_jgf_lr">JavaScript</a>, and <a class="xref" href="../Processors/Jython.html#concept_a1h_lkf_lr">Jython
                                    Evaluator</a> processor enhancements:<ul class="ul" id="concept_bsw_cky_11b__ul_e4s_jty_11b">
                                    <li class="li">You can now include some methods of the sdcFunctions
                                        scripting object in the initialization and destroy scripts
                                        for the processors.</li>

                                    <li class="li">You can now use runtime parameters in the code developed for
                                        a Groovy Evaluator processor.</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Processors/HiveMetadata.html#concept_rz5_nft_zv">Hive
                                    Metadata processor enhancements</a>:<ul class="ul" id="concept_bsw_cky_11b__ul_v3b_lty_11b">
                                    <li class="li">The Hive Metadata processor can now <a class="xref" href="../Solutions/HiveDrift-Overview.html#concept_ndg_3zw_vz">process Parquet data as part of the Drift
                                            Synchronization Solution for Hive</a>. </li>

                                    <li class="li">You can now specify the data format to use: Avro or
                                        Parquet.</li>

                                    <li class="li">You can now configure an expression that defines comments
                                        for generated columns. </li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Processors/JDBCLookup.html#concept_ysc_ccy_hw">JDBC
                                    Lookup processor enhancements</a>:<ul class="ul" id="concept_bsw_cky_11b__ul_vls_4ty_11b">
                                    <li class="li">The JDBC Lookup processor can now return multiple values.
                                        You can now configure the lookup to return the first value
                                        or to return all matches as separate records. </li>

                                    <li class="li">When you <a class="xref" href="../Processors/JDBCLookup.html#concept_k3l_hrd_wz" title="When you monitor a pipeline that includes the JDBC Lookup processor, the Summary tab displays statistics about the queries that the JDBC Lookup processor performs. Use the statistics to help identify any performance bottlenecks encountered by the pipeline.">monitor a pipeline that includes the JDBC Lookup
                                            processor</a>, you can now view stage statistics
                                        about the number of queries the processor makes and the
                                        average time of the queries.</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Processors/Spark.html#concept_nbj_1jb_c1b">Spark Evaluator enhancement</a> - The Spark Evaluator now
                                supports Spark 2.x.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Destinations</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_bsw_cky_11b__ul_p5x_qty_11b">
                            <li class="li"><a class="xref" href="../Destinations/CoAPClient.html#concept_hw5_s3n_sz">New
                                    CoAP Client destination</a> - A destination that writes to a
                                CoAP endpoint.</li>

                            <li class="li"><a class="xref" href="../Destinations/HiveMetastore.html#concept_gcr_z2t_zv">Hive Metastore destination enhancements</a>:<ul class="ul" id="concept_bsw_cky_11b__ul_y5w_sty_11b">
                                    <li class="li">The destination can now <a class="xref" href="../Destinations/HiveMetastore.html#concept_wyr_5jv_hw">create and update Parquet tables in Hive</a>. </li>

                                    <li class="li">Also, the data format property has been removed. You now
                                        specify the data format in the <a class="xref" href="../Processors/HiveMetadata.html#concept_rz5_nft_zv">Hive Metadata processor.</a>
                                        <p class="p">Since the Hive Metastore previously supported only Avro
                                            data, there is no upgrade impact. </p>
</li>

                                </ul>
</li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Destinations/Kudu.html#concept_chy_xxg_4v">Kudu
                                        destination enhancement</a> - You can use the new
                                    Mutation Buffer Space property to set the buffer size that the
                                    Kudu client uses to write each batch.</p>

                            </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Executors</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_bsw_cky_11b__ul_xl4_b5y_11b">
                            <li class="li"><a class="xref" href="../Executors/Email.html#concept_sjs_sfp_qz">New Email
                                    executor</a> - Use to send custom emails upon receiving an
                                event. For a case study, see <a class="xref" href="../Solutions/SendEmail.html#concept_t2t_lp5_xz" title="This solution describes how to design a pipeline to send email notifications at different moments during pipeline processing.">Sending Email During Pipeline Processing</a>.<ul class="ul" id="concept_bsw_cky_11b__ul_ekw_b5y_11b">
                                    <li dir="ltr" class="li">
                                        <p class="p"><a class="xref" href="../Executors/Shell.html#concept_jsr_zpw_tz">New Shell executor</a> - Use to execute shell
                                            scripts upon receiving an event. </p>

                                    </li>

                                    <li dir="ltr" class="li">
                                        <p class="p"><a class="xref" href="../Executors/JDBCQuery.html#concept_j3r_gcv_sx">JDBC Query executor enhancement</a> - A new
                                            Batch Commit property allows the executor to commit to
                                            the database after each batch. Previously, the executor
                                            did not call commits by default.</p>

                                        <p class="p">For new pipelines, the property is enabled by default.
                                            For upgraded pipelines, the property is disabled to
                                            prevent changes in pipeline behavior. </p>

                                    </li>

                                    <li dir="ltr" class="li"><a class="xref" href="../Executors/Spark.html#concept_vbm_ywb_c1b">Spark executor enhancement</a> - The executor now
                                        supports Spark 2.x. </li>

                                </ul>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">REST API / Command Line Interface</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_bsw_cky_11b__ul_ktv_k5y_11b">
                            <li class="li">Offset management - Both the REST API and <a class="xref" href="../Administration/Administration_title.html#concept_ywx_d5x_pt" title="Data Collector provides a command line interface that includes a basic cli command. Use the command to perform some of the same actions that you can complete from the Data Collector UI. Data Collector must be running before you can use the cli command.">command line interface</a> can now retrieve the last-saved
                                offset for a pipeline and set the offset for a pipeline when it is
                                not running. Use these commands to implement pipeline failover using
                                an external storage system. Otherwise, pipeline offsets are managed
                                by Data Collector and there is no need to update the offsets.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Expression Language</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_bsw_cky_11b__ul_fm5_m5y_11b">
                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_ddw_ld1_1s">vault:read enhancement</a> - The vault:read function now
                                supports returning the value for a key nested in a map.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">General</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_bsw_cky_11b__ul_rlz_55y_11b">
                            <li class="li"><a class="xref" href="../Administration/Administration_title.html#concept_szj_3mw_xz">Support bundles</a> - You can now use Data Collector to
                                generate a support bundle. A support bundle is a ZIP file that
                                includes Data Collector logs, environment and configuration
                                information, pipeline JSON files, resource files, and pipeline
                                snapshots. <p class="p">You upload the generated file to the StreamSets
                                    support team so that we can use the information to troubleshoot
                                    your support tickets.</p>
</li>

                            <li class="li">
                                <div class="p"><a class="xref" href="../Pipeline_Configuration/SSL-TLS.html#concept_dd1_n3f_5z" title="Many stages can use SSL/TLS encryption to securely connect to the external system.">TLS property enhancements</a> - Stages that support
                                    SSL/TLS now provide the following enhanced set of properties
                                    that enable more specific configuration:<ul class="ul" id="concept_bsw_cky_11b__ul_v5g_w5y_11b">
                                        <li class="li">Keystore and truststore type - You can now choose
                                            between Java Keystore (JKS) and PKCS #12 (p-12).
                                            Previously, Data Collector only supported JKS.</li>

                                        <li class="li">Transport protocols - You can now specify the transport
                                            protocols that you want to allow. By default, Data
                                            Collector allows only TLSv1.2. </li>

                                        <li class="li">Cipher suites - You can now specify the cipher suites to
                                            allow. Data Collector provides a modern set of default
                                            cipher suites. Previously, Data Collector always allowed
                                            the default cipher suites for the JRE.</li>

                                    </ul>
To avoid upgrade impact, all SSL/TLS/HTTPS properties in
                                    existing pipelines are preserved during upgrade. </div>

                            </li>

                            <li class="li">
                                <p class="p"><a class="xref" href="../Cluster_Mode/ClusterPipelines.html#concept_hmh_kfn_1s" title="A cluster pipeline is a pipeline that runs in cluster execution mode. You can run a pipeline in standalone execution mode or cluster execution mode.">Cluster mode enhancement</a> - Cluster streaming mode
                                    now supports Spark 2.x. For information about using Spark 2.x
                                    stages with cluster mode, see <a class="xref" href="../Cluster_Mode/StageLimitations.html#concept_pdf_r5y_fz">Cluster Pipeline Limitations</a>.</p>

                            </li>

                            <li class="li"><a class="xref" href="../Pipeline_Design/DroppingUnwantedRecords.html#concept_msl_yd4_fs">Precondition enhancement</a> - Stages with user-defined
                                preconditions now process all preconditions before passing a record
                                to error handling. This allows error records to include all
                                precondition failures in the error message.</li>

                            <li class="li"><a class="xref" href="../Pipeline_Maintenance/PipelineMaintenance_title.html#task_fdf_hrr_5q">Pipeline import</a>/<a class="xref" href="../Pipeline_Maintenance/PipelineMaintenance_title.html#task_dtz_4tr_5q">export enhancement</a> - When you export multiple pipelines,
                                Data Collector now includes all pipelines in a single zip file. You
                                can also import multiple pipelines from a single zip file. </li>

                        </ul>

                    </dd>

                
            </dl>
</div>

 </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title38" id="concept_afr_hly_tz">
 <h2 class="title topictitle2" id="ariaid-title38">What's New in 2.5.1.0</h2>

 <div class="body conbody">
  <div class="p"><span class="ph">Data Collector</span>
            version 2.5.1.0 includes the following enhancement:<ul class="ul" id="concept_afr_hly_tz__ul_uwf_lly_tz">
                <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">New
                        stage library</a> - <span class="ph">Data Collector</span> now supports the Cloudera CDH version 5.11 distribution of Hadoop and the
                    Cloudera version 5.11 distribution of Apache Kafka 2.1. <p class="p">Upgrading to this
                        version can require updating existing pipelines. For details, see <a class="xref" href="../Upgrade/Upgrade-ExternalSystems.html#concept_spf_2gq_vz">Working with Cloudera CDH 5.11 or Later</a>.</p>

                </li>

            </ul>
</div>

 </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title39" id="concept_ddx_bpm_pz">
 <h2 class="title topictitle2" id="ariaid-title39">What's New in 2.5.0.0</h2>

 <div class="body conbody">
  <p class="p"><span class="ph">Data Collector</span>
            version 2.5.0.0 includes the following new features and enhancements:</p>

        <dl class="dl">
            
                <dt class="dt dlterm">Multithreaded Pipelines</dt>

                <dd class="dd">
                    <div class="p">The multithreaded framework includes the following enhancements:<ul class="ul" id="concept_ddx_bpm_pz__ul_gs3_3pm_pz">
                            <li class="li"><a class="xref" href="../Multithreaded_Pipelines/MultithreadedPipelines.html#concept_wcz_tpd_py">Origins for multithreaded pipelines</a> - You can now use
                                the following origins to create multithreaded pipelines:<ul class="ul" id="concept_ddx_bpm_pz__ul_hx3_mpm_pz">
                                    <li class="li">Elasticsearch origin</li>

                                    <li class="li">JDBC Multitable Consumer origin</li>

                                    <li class="li">Kinesis Consumer origin</li>

                                    <li class="li">WebSocket Server origin</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Multithreaded_Pipelines/MultithreadedPipelines.html#concept_fmg_pjd_mz">Maximum pipeline runners</a> - You can now configure a
                                maximum number of pipeline runners to use in a pipeline. Previously,
                                    <span class="ph">Data Collector</span> generated pipeline runners based on the number of threads created
                                by the origin. This allows you to tune performance and resource
                                usage. By default, <span class="ph">Data Collector</span> still generates runners based on the number of threads that the
                                origin uses. </li>

                            <li class="li"><a class="xref" href="../Multithreaded_Pipelines/MultithreadedPipelines.html#concept_np1_pkz_ry">Record Deduplicator processor enhancement</a> - The
                                processor can now deduplicate records across all pipeline runners in
                                a multithreaded pipeline.</li>

                            <li class="li">Pipeline validation enhancement - The pipeline now displays
                                duplicate errors generated by using multiple threads as one error
                                message.</li>

                            <li class="li">Log enhancement - Multithreaded pipelines now include the runner ID
                                in log information. </li>

                            <li class="li"><a class="xref" href="../Multithreaded_Pipelines/MultithreadedPipelines.html#concept_tdn_vwy_ry">Monitoring</a> - Monitoring now displays a histogram of
                                available pipeline runners, replacing the information previously
                                included in the Runtime Statistics list.</li>

                        </ul>
</div>

                </dd>

            
            
                <dt class="dt dlterm">Pipelines</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ddx_bpm_pz__ul_asz_cqm_pz">
                        <li class="li"><span class="ph">Data Collector</span>
                            <a class="xref" href="../Configuration/RolesandPermissions.html#concept_i1p_hzd_yy">pipeline permissions</a> change - With this release, pipeline
                            permissions are no longer enabled by default. To enable pipeline
                            permissions, edit the <code class="ph codeph">pipeline.access.control.enabled</code>
                            <span class="ph">Data Collector</span> configuration property.</li>

                        <li class="li"><a class="xref" href="../Executors/PipelineFinisher.html#concept_qzm_l4r_kz">Stop
                                pipeline execution</a> - You can configure pipelines to transfer
                            data and automatically stop execution based on an event such as reaching
                            the end of a table. The JDBC and Salesforce origins can generate events
                            when they reach the end of available data that the Pipeline Finisher
                            uses to stop the pipeline. Click <a class="xref" href="../Solutions/StopPipeline.html#concept_kff_ykv_lz" title="This solution describes how to design a pipeline that stops automatically after it finishes processing all available data.">here</a> for a case study. </li>

                        <li class="li"><a class="xref" href="../Pipeline_Configuration/RuntimeValues.html#concept_rjh_ntz_qr" title="Runtime parameters are parameters that you define in a pipeline and then call from within that same pipeline. When the pipeline runs, the value replaces the name of the runtime parameter. Use runtime parameters to define values for stage and pipeline properties.">Pipeline runtime parameters</a> - You can now define runtime
                            parameters when you configure a pipeline, and then call the parameters
                            from within that pipeline. When you start the pipeline from the user
                            interface, the command line, or the REST API, you specify the values to
                            use for those parameters. Use pipeline parameters to represent any stage
                            or pipeline property with a value that must change for each pipeline run
                            - such as batch sizes and timeouts, directories, or URI.<p class="p">In previous
                                versions, pipeline runtime parameters were named pipeline constants.
                                You defined the constant values in the pipeline, and could not pass
                                different values when you started the pipeline.</p>
</li>

                        <li class="li"><a class="xref" href="../Pipeline_Configuration/ConfiguringAPipeline.html#task_xlv_jdw_kq" title="Configure a pipeline to define the stream of data. After you configure the pipeline, you can start the pipeline.">Pipeline ID enhancement</a> - <span class="ph">Data Collector</span> now prefixes the pipeline ID with the alphanumeric characters entered
                            for the pipeline title. For example, if you enter Oracle To HDFS as
                            the pipeline title, then the pipeline ID has the following value:
                            OracleToHDFStad9f592-5f02-4695-bb10-127b2e41561c.</li>

                        <li class="li">Webhooks for pipeline state changes and alerts - You can now configure
                            pipeline state changes and metric and data alerts to call webhooks in
                            addition to sending email. For example, you can configure an incoming
                            webhook in Slack so that an alert can be posted to a Slack channel. Or,
                            you can configure a webhook to start another pipeline when the pipeline
                            state is changed to Finished or Stopped.</li>

                        <li class="li"><a class="xref" href="../Administration/Administration_title.html#concept_msh_k2q_yt" title="The manager command provides subcommands to start and stop a pipeline, view the status of all pipelines, and reset the origin for a pipeline. It can also be used to get the last-saved offset and to update the last-saved offset for a pipeline.">Force
                                a pipeline to stop from the command line</a> - If a pipeline
                            remains in a Stopping state, you can now use the command line to force
                            stop the pipeline immediately.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Stage Libraries</dt>

                <dd class="dd">
                    <p class="p"><span class="ph">Data Collector</span> now supports the Apache Kudu version 1.3.x. <a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">stage library</a>.</p>

                </dd>

            
            
                <dt class="dt dlterm">Salesforce Stages</dt>

                <dd class="dd">
                    <div class="p">The following Salesforce stages include several enhancements:<ul class="ul" id="concept_ddx_bpm_pz__ul_gsv_prm_pz">
                            <li class="li"><a class="xref" href="../Origins/Salesforce.html#concept_odf_vr3_rx">Salesforce origin</a> and <a class="xref" href="../Processors/SalesforceLookup.html#concept_k23_3rk_yx">Salesforce Lookup processor</a><ul class="ul" id="concept_ddx_bpm_pz__ul_k1d_5rm_pz">
                                    <li class="li">The origin and processor can use a proxy to connect to
                                        Salesforce.</li>

                                    <li class="li">You can now specify <code class="ph codeph">SELECT * FROM
                                            &lt;object&gt;</code> in a SOQL query. The origin or
                                        processor expands * to all fields in the Salesforce object
                                        that are accessible to the configured user. </li>

                                    <li class="li">The origin and processor generate Salesforce field
                                        attributes that provide additional information about each
                                        field, such as the data type of the Salesforce field.</li>

                                    <li class="li">The origin and processor can now additionally retrieve
                                        deleted records from the Salesforce recycle bin. </li>

                                    <li class="li">The origin can now generate events when it completes
                                        processing all available data.</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Destinations/Salesforce.html#concept_rlb_rt3_rx">Salesforce destination</a> - The destination can now use a
                                CRUD operation record header attribute to indicate the operation to
                                perform for each record. You can also configure the destination to
                                use a proxy to connect to Salesforce. </li>

                            <li class="li"><a class="xref" href="../Destinations/WaveAnalytics.html#concept_hlx_r53_rx">Wave Analytics destination</a> - You can now configure the
                                authentication endpoint and the API version that the destination
                                uses to connect to Salesforce Wave Analytics. You can also configure
                                the destination to use a proxy to connect to Salesforce.</li>

                        </ul>
</div>

                </dd>

            
            
                <dt class="dt dlterm">Origins</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ddx_bpm_pz__ul_z25_bsm_pz">
                        <li class="li"><a class="xref" href="../Origins/Elasticsearch.html#concept_f1q_vpm_2z">New
                                Elasticsearch origin</a> - An origin that reads data from an
                            Elasticsearch cluster. The origin uses the Elasticsearch scroll API to
                            read documents using a user-defined Elasticsearch query. The origin
                            performs parallel processing and can generate multithreaded pipelines. </li>

                        <li class="li"><a class="xref" href="../Origins/MQTTSubscriber.html#concept_ukz_3vt_lz">New MQTT
                                Subscriber origin</a> - An origin that subscribes to a topic on
                            an MQTT broker to read messages from the broker.</li>

                        <li class="li"><a class="xref" href="../Origins/WebSocketServer.html#concept_u2r_gpc_3z">New
                                WebSocket Server origin</a> - An origin that listens on a
                            WebSocket endpoint and processes the contents of all authorized
                            WebSocket requests. The origin performs parallel processing and can
                            generate multithreaded pipelines. </li>

                        <li class="li"><a class="xref" href="../Pipeline_Design/DevStages.html#concept_czx_ktn_ht">Dev
                                Data Generator origin enhancement</a> - When you configure the
                            origin to generate events to test event handling functionality, you can
                            now specify the event type to use.</li>

                        <li class="li"><a class="xref" href="../Origins/HTTPClient.html#concept_wk4_bjz_5r">HTTP Client
                                origin enhancements</a> - When using pagination, the origin can
                            include all response fields in the resulting record in addition to the
                            fields in the specified result field path. The origin can now also
                            process the following new data formats: Binary, Delimited, Log, and SDC
                            Record.</li>

                        <li class="li"><a class="xref" href="../Origins/HTTPServer.html#concept_thw_wtd_kz" title="For origins configured to require requests from allowed application IDs, configure the HTTP clients to include an allowed application ID in each request.">HTTP Server origin enhancement</a> - The origin requires that
                            HTTP clients include the application ID in all requests. You can now
                            configure HTTP clients to send data to a URL that includes the
                            application ID in a query parameter, rather than including the
                            application ID in request headers. </li>

                        <li class="li"><a class="xref" href="../Origins/MultiTableJDBCConsumer.html#concept_zp3_wnw_4y">JDBC Multitable Consumer origin enhancements</a> - The origin
                            now performs parallel processing and can generate multithreaded
                            pipelines. The origin can generate events when it completes processing
                            all available data. <p class="p">You can also configure the quote character to use
                                around table, schema, and column names in the query. And you can
                                configure the number of times a thread tries to read a batch of data
                                after receiving an SQL error.</p>
</li>

                        <li class="li"><a class="xref" href="../Origins/JDBCConsumer.html#concept_qhf_hjr_bs">JDBC Query
                                Consumer origin enhancements</a> - The origin can now generate
                            events when it completes processing all available data, and when it
                            successfully completes or fails to complete a query. <p class="p">To handle
                                transient connection or network errors, you can now specify how many
                                times the origin should retry a query before stopping the
                                pipeline.</p>
</li>

                        <li class="li"><a class="xref" href="../Origins/KConsumer.html#concept_msz_wnr_5q">Kinesis
                                Consumer origin enhancement</a> - The origin now performs
                            parallel processing and can generate multithreaded pipelines. </li>

                        <li class="li"><a class="xref" href="../Origins/MongoDB.html#concept_gzz_kdr_tz">MongoDB origin</a> and <a class="xref" href="../Origins/MongoDBOplog.html#concept_ovt_vpt_tz">MongoDB Oplog origin</a> enhancements - The origins can now use
                            LDAP authentication in addition to username/password authentication to
                            connect to MongoDB. You can also now include credentials in the MongoDB
                            connection string.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Processors</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ddx_bpm_pz__ul_apy_wsm_pz">
                        <li class="li"><a class="xref" href="../Processors/FieldOrder.html#concept_krp_5fv_vy">New Field
                                Order processor</a> - A processor that orders fields in a map or
                            list-map field and outputs the fields into a list-map or list root
                            field.</li>

                        <li class="li">Field Flattener enhancement - You can now flatten a field in place to
                            raise it to the parent level.</li>

                        <li class="li"><a class="xref" href="../Processors/Groovy.html#concept_ldh_sct_gv">Groovy</a>,
                                <a class="xref" href="../Processors/JavaScript.html#concept_n2p_jgf_lr">JavaScript</a>, and <a class="xref" href="../Processors/Jython.html#concept_a1h_lkf_lr">Jython
                                Evaluator</a> processor enhancement - You can now develop an
                            initialization script that the processor runs once when the pipeline
                            starts. Use an initialization script to set up connections or resources
                            required by the processor.<p class="p">You can also develop a destroy script that
                                the processor runs once when the pipeline stops. Use a destroy
                                script to close any connections or resources opened by the
                                processor.</p>
</li>

                        <li class="li"><a class="xref" href="../Processors/JDBCLookup.html#task_kbr_2cy_hw">JDBC Lookup enhancement</a> - Default value date formats. When
                            the default value data type is Date, use the following format:
                            yyyy/MM/dd . When the default value data type is Datetime, use the
                            following format: yyyy/MM/dd HH:mm:ss.</li>

                        <li class="li"><a class="xref" href="../Pipeline_Design/DevStages.html#concept_czx_ktn_ht">Record
                                Deduplicator processor enhancement</a> - The processor can now
                            deduplicate records across all pipeline runners in a multithreaded
                            pipeline.</li>

                        <li class="li"><a class="xref" href="../Processors/Spark.html#concept_cpx_1lm_zx">Spark Evaluator
                                processor enhancements</a> - The processor is now included in the
                            MapR 5.2 stage library. <p class="p">The processor also now provides beta support
                                of cluster mode pipelines. In a development or test environment, you
                                can use the processor in pipelines that process data from a Kafka or
                                MapR cluster in cluster streaming mode. Do not use the Spark
                                Evaluator processor in cluster mode pipelines in a production
                                environment.</p>
</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Destinations</dt>

                <dd class="dd">
                        <ul class="ul" id="concept_ddx_bpm_pz__ul_x13_lym_pz">
                            <li class="li"><a class="xref" href="../Destinations/HTTPClient.html#concept_khl_sg5_lz">New
                                    HTTP Client destination</a> - A destination that writes to an
                                HTTP endpoint.</li>

                            <li class="li"><a class="xref" href="../Destinations/MQTTPublisher.html#concept_odz_txt_lz">New MQTT Publisher destination</a> - A destination that
                                publishes messages to a topic on an MQTT broker.</li>

                            <li class="li"><a class="xref" href="../Destinations/WebSocketClient.html#concept_l4d_mjn_lz">New WebSocket Client destination</a> - A destination that
                                writes to a WebSocket endpoint.</li>

                            <li class="li"><a class="xref" href="../Destinations/DataLakeStore.html#concept_c2p_wzh_4z">Azure Data Lake Store destination enhancement</a> - You can
                                now configure an idle timeout for output files. </li>

                            <li class="li">Cassandra destination enhancements - The destination now supports
                            the Cassandra uuid and timeuuid <a class="xref" href="../Destinations/Cassandra.html#concept_bdg_bt3_ms">data types</a>. And you can now specify the Cassandra <a class="xref" href="../Destinations/Cassandra.html#concept_fky_vjd_mgb">batch type</a> to use: logged or unlogged. Previously, the
                            destination used the logged batch type.</li>

                            <li class="li"><a class="xref" href="../Destinations/JDBCProducer.html#concept_kvs_3hh_ht">JDBC Producer enhancements</a> - The origin now includes a
                                Schema Name property for entering the schema name. For information
                                about possible upgrade impact, see <a class="xref" href="../Upgrade/PostUpgrade.html#concept_cmh_ryd_pz">Configure JDBC Producer Schema Names</a>.<p class="p">You can also use
                                    the Enclose Object Name property to enclose the database/schema,
                                    table, and column names in quotation marks when writing to the
                                    database. </p>
</li>

                            <li class="li"><a class="xref" href="../Destinations/MapRDBJSON.html#task_wq3_wkj_dy">MapR DB JSON destination enhancement</a> - You can now enter an
                            expression that evaluates to the name of the MapR DB JSON table to write
                            to.</li>

                        <li class="li"><a class="xref" href="../Destinations/MongoDB.html#concept_ppl_3qt_tz">MongoDB destination enhancements</a> - The destination can now
                            use LDAP authentication in addition to username/password authentication
                            to connect to MongoDB. You can also now include credentials in the
                            MongoDB connection string.</li>

                        <li class="li"><a class="xref" href="../Destinations/SDC_RPCdest.html#task_nbl_r2x_dt">SDC RPC destination enhancements</a> - The Back Off Period value
                            that you enter now increases exponentially after each retry, until it
                            reaches the maximum wait time of 5 minutes. Previously, there was no
                            limit to the maximum wait time. The maximum value for the Retries per
                            Batch property is now unlimited - previously it was 10 retries.</li>

                        <li class="li"><a class="xref" href="../Destinations/Solr.html#concept_z2g_q1r_wr">Solr
                                destination enhancement</a> - You can now configure the action
                            that the destination takes when it encounters missing fields in the
                            record. The destination can discard the fields, send the record to
                            error, or stop the pipeline.</li>

                        </ul>

                </dd>

            
            
                <dt class="dt dlterm">Executors</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ddx_bpm_pz__ul_ojc_3zm_pz">
                        <li class="li"><a class="xref" href="../Executors/Spark.html#concept_cvy_vxb_1z">New Spark
                                executor</a> - The executor starts a Spark application on a YARN
                            or Databricks cluster each time it receives an event.</li>

                        <li class="li"><a class="xref" href="../Executors/PipelineFinisher.html#concept_qzm_l4r_kz">New
                                Pipeline Finisher executor</a> - The executor stops the pipeline
                            and transitions it to a Finished state when it receives an event. Can be
                            used with the JDBC Query Consumer, JDBC Multitable Consumer, and
                            Salesforce origins to perform batch processing of available data.</li>

                        <li class="li"><a class="xref" href="../Executors/HDFSMetadata.html#concept_wgj_slk_fx">HDFS File
                                Metadata executor enhancement</a> - The executor can now create
                            an empty file upon receiving an event. The executor can also generate a
                            file-created event when generating events. </li>

                        <li class="li"><a class="xref" href="../Executors/MapReduce.html#concept_bj2_zlk_fx">MapReduce
                                executor enhancement</a> - When starting the provided Avro to
                            Parquet job, the executor can now overwrite any temporary files created
                            from a previous run of the job.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Functions</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ddx_bpm_pz__ul_ayq_pzm_pz">
                        <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_ahp_f4v_1r">New escape XML functions</a> - Three new string functions enable
                            you to escape and unescape XML.</li>

                        <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_dvg_nqn_wx">New pipeline user function</a> - A new pipeline user function
                            enables you to determine the user who started the pipeline. </li>

                        <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_ddw_ld1_1s">New function to generate UUIDs</a> - A new function that enables
                            you generate UUIDs.</li>

                        <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_ddw_ld1_1s">New function returns the number of available processors</a> -
                            The runtime:availableProcessors() function returns the number of
                            processors available to the Java virtual machine.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">General Enhancements</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ddx_bpm_pz__ul_ksn_tzm_pz">
                        <li class="li"><a class="xref" href="../Configuration/DCConfig.html#concept_pmr_sy5_nz">Data Collector Hadoop impersonation enhancement</a> - You can
                            use the
                                <code class="ph codeph">stage.conf_hadoop.always.impersonate.current.user</code>
                            <span class="ph">Data Collector</span> configuration property to ensure that <span class="ph">Data Collector</span> uses the current <span class="ph">Data Collector</span> user to read from or write to Hadoop systems. <div class="p">When enabled, you
                                cannot configure alternate users in the following Hadoop-related
                                    stages:<ul class="ul" id="concept_ddx_bpm_pz__ul_ztn_xzm_pz">
                                    <li class="li">Hadoop FS origin and destination</li>

                                    <li class="li">MapR FS origin and destination</li>

                                    <li class="li">HBase lookup and destination</li>

                                    <li class="li">MapR DB destination</li>

                                    <li class="li">HDFS File Metadata executor</li>

                                    <li class="li">Map Reduce executor</li>

                                </ul>
</div>
</li>

                        <li class="li">Stage precondition property enhancement - Records that do not meet all
                            preconditions for a stage are now processed based on error handling
                            configured in the stage. Previously, they were processed based on error
                            handling configured for the pipeline. See <a class="xref" href="../Upgrade/PostUpgrade.html#concept_gk3_s5l_nz">Evaluate Precondition Error Handling</a> for information about
                            upgrading.</li>

                        <li class="li">XML parsing enhancements - You can include field XPath expressions and
                            namespaces in the record with the <a class="xref" href="../Data_Formats/XMLDFormat.html#concept_w3k_1ch_qz">Include Field XPaths property</a>. And use the new <a class="xref" href="../Data_Formats/XMLDFormat.html#concept_jll_4wh_qz">Output Field Attributes property</a> to write XML attributes and
                            namespace declarations to field attributes rather than including them in
                            the record as fields. </li>

                        <li class="li"><a class="xref" href="../Getting_Started/GettingStarted_Title.html#task_r3q_fnx_pr">Wrap long lines in properties</a> - You can now configure <span class="ph">Data Collector</span> to wrap long lines of text that you enter in properties, instead of
                            displaying the text with a scroll bar.</li>

                    </ul>

                </dd>

            
        </dl>

 </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title40" id="concept_cf2_sdz_fz">
 <h2 class="title topictitle2" id="ariaid-title40">What's New in 2.4.1.0</h2>

 <div class="body conbody">
  <div class="p"><span class="ph">Data Collector</span>
            version 2.4.1.0 includes the following new features and enhancements:<ul class="ul" id="concept_cf2_sdz_fz__ul_xdl_tdz_fz">
                <li class="li"><a class="xref" href="../Origins/Salesforce.html#concept_owv_nj5_2z">Salesforce origin enhancement</a> - When the origin processes existing
                    data and is not subscribed to notifications, it can now repeat the specified
                    query at regular intervals. The origin can repeat a full or incremental
                    query.</li>

                <li class="li"><a class="xref" href="../Administration/Administration_title.html#task_gbm_s3k_br">Log data
                        display</a> - You can stop and restart the automatic display of the most
                    recent log data on the Data Collector Logs page.</li>

                <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_qkr_trf_sw" title="Use time functions to return the current time or to transform datetime data.">New
                        time function</a> - The <code class="ph codeph">time:createDateFromStringTZ</code>
                    function enables creating Date objects adjusted for time zones from string
                    datetime values.</li>

                <li class="li">New stage library stage-type icons - The stage library now displays icons to
                    differentiate between different stage types.</li>

            </ul>
<div class="note note"><span class="notetitle">Note:</span> The Hive Drift Solution is now known as the "Drift Synchronization Solution
                for Hive" in the documentation.</div>
</div>

 </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title41" id="concept_kzc_4sd_yy">
 <h2 class="title topictitle2" id="ariaid-title41">What's New in 2.4.0.0</h2>

 <div class="body conbody">
  <p class="p"><span class="ph">Data Collector</span>
            version 2.4.0.0 includes the following new features and enhancements:</p>

        <div class="p">
            <dl class="dl">
                
                    <dt class="dt dlterm">Pipeline Sharing and Permissions</dt>

                    <dd class="dd">Data Collector now provides pipeline-level permissions. Permissions
                        determine the access level that users and groups have on pipelines. To
                        create a multitenant environment, create groups of users and then share
                        pipelines with the groups to grant different levels of access.</dd>

                    <dd class="dd ddexpand">With this change, only the pipeline owner and users with the Admin role can
                        view a pipeline by default. If upgrading from a previous version of Data
                        Collector, see the following post-upgrade task, <a class="xref" href="../Upgrade/PostUpgrade.html#concept_zbn_fpw_xy">Configure Pipeline Permissions</a>.</dd>

                    <dd class="dd ddexpand">This feature includes the following components:<ul class="ul" id="concept_kzc_4sd_yy__ul_qhv_ds1_cz">
                            <li class="li"><a class="xref" href="../Configuration/RolesandPermissions.html#concept_i1p_hzd_yy">Pipeline permissions</a> - Pipelines now have read, write,
                                and execute permissions. Pipeline permissions overlay existing Data
                                Collector roles to provide greater security. For information, see
                                    <a class="xref" href="../Configuration/RolesandPermissions.html#concept_k1r_prc_yy">Roles and Permissions</a>.</li>

                            <li class="li"><a class="xref" href="../Pipeline_Maintenance/PipelineMaintenance_title.html#concept_jrg_1vy_wy">Pipeline sharing</a> - The pipeline owner and users with the
                                Admin role can configure pipeline permissions for users and
                                groups.</li>

                            <li class="li"><a class="xref" href="../Configuration/DCConfig.html#task_lxk_kjw_1r" title="You can customize Data Collector by editing the Data Collector configuration file, sdc.properties.">Data Collector pipeline access control property</a> - You
                                can enable and disable the use of pipeline permissions with the
                                pipeline.access.control.enabled configuration property. By default,
                                this property is enabled.</li>

                            <li class="li"><a class="xref" href="../Configuration/RolesandPermissions.html#concept_p11_msc_1z">Permissions transfer</a> - You can transfer all pipeline
                                permissions associated with a user or group to a different user or
                                group. Use pipeline transfer to easily migrate permissions after
                                registering with DPM or after a user or group becomes obsolete.</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Dataflow Performance Manager (DPM)</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_kzc_4sd_yy__ul_f3t_zs1_cz">
                            <li class="li"><a class="xref" href="../DPM/RegisterSDCwithDPM.html#task_a4y_v1g_xw" title="For a Data Collector tarball installation, you can register the Data Collector with Control Hub using the Data Collector UI.">Register Data Collectors with DPM</a> - If Data Collector
                                uses file-based authentication and if you register the Data
                                Collector from the Data Collector UI, you can now create DPM user
                                accounts and groups during the registration process.</li>

                            <li class="li"><a class="xref" href="../DPM/AggregatedStatistics.html#concept_c53_pzp_yy">Aggregated statistics for DPM</a> - When working with DPM,
                                you can now configure a pipeline to write aggregated statistics to
                                SDC RPC. Write statistics to SDC RPC for development purposes only.
                                For a production environment, use a Kafka cluster or Amazon Kinesis
                                Streams to aggregate statistics.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_kzc_4sd_yy__ul_ad3_kt1_cz">
                            <li class="li"><a class="xref" href="../Pipeline_Design/DevStages.html#concept_czx_ktn_ht">Dev SDC RPC with Buffering origin</a> - A new development
                                stage that receives records from an SDC RPC destination, temporarily
                                buffering the records to disk before passing the records to the next
                                stage in the pipeline. Use as the origin in an SDC RPC destination
                                pipeline.</li>

                            <li class="li"><a class="xref" href="../Origins/AmazonS3.html#task_gfj_ssv_yq">Amazon S3 origin enhancement</a> - You can configure a new
                                File Pool Size property to determine the maximum number of files
                                that the origin stores in memory for processing after loading and
                                sorting all files present on S3.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Other</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_kzc_4sd_yy__ul_hsj_tt1_cz">
                            <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">New stage libraries</a> - This release supports the
                                following new stage libraries:<ul class="ul" id="concept_kzc_4sd_yy__ul_xrv_5t1_cz">
                                    <li dir="ltr" class="li">Kudu versions 1.1 and 1.2</li>

                                    <li class="li">
                                        <p dir="ltr" class="p">Cloudera CDH version 5.10 distribution of
                                            Hadoop</p>

                                    </li>

                                    <li dir="ltr" class="li">Cloudera version 5.10 distribution of Apache Kafka
                                        2.1</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Configuration/ExternalLibs.html#concept_pdv_qlw_ft">Install external libraries using the Data Collector user
                                    interface</a> - You can now use the Data Collector user
                                interface to install external libraries to make them available to
                                stages. For example, you can install JDBC drivers for stages that
                                use JDBC connections. Or, you can install external libraries to call
                                external Java code from the Groovy, Java, and Jython Evaluator
                                processors.</li>

                            <li class="li"><a class="xref" href="../Configuration/DCConfig.html#task_lxk_kjw_1r" title="You can customize Data Collector by editing the Data Collector configuration file, sdc.properties.">Custom header enhancement</a> - You can now use HTML in the
                                ui.header.title configuration property to configure a custom header
                                for the <span class="ph">Data Collector</span> UI. This allows you to specify the look and feel for any text
                                that you use, and to include small images in the header. </li>

                            <li class="li"><a class="xref" href="../Processors/Groovy.html#task_asl_bpt_gv">Groovy enhancement</a> - You can configure the processor to
                                use the invokedynamic bytecode instruction.</li>

                            <li class="li">Pipeline renaming - You can now rename a pipeline by clicking
                                directly on the pipeline name when editing the pipeline, in addition
                                to editing the Title general pipeline property.</li>

                        </ul>

                    </dd>

                
            </dl>

        </div>

 </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title42" id="concept_bml_dbt_wy">
 <h2 class="title topictitle2" id="ariaid-title42">What's New in 2.3.0.1</h2>

 <div class="body conbody">
  <div class="p"><span class="ph">Data Collector</span>
            version 2.3.0.1 includes the following new features and enhancements:<ul class="ul" id="concept_bml_dbt_wy__ul_srs_hbt_wy">
                <li class="li"><a class="xref" href="../Origins/OracleCDC.html#concept_rs5_hjj_tw">Oracle CDC Client
                        origin enhancement</a> - The origin can now track and adapt to schema
                    changes when reading the dictionary from redo logs. When using the dictionary in
                    redo logs, the origin can also generate events for each DDL that it reads. </li>

                <li class="li"><a class="xref" href="../Configuration/DCConfig.html#task_lxk_kjw_1r" title="You can customize Data Collector by editing the Data Collector configuration file, sdc.properties.">New
                        Data Collector property</a> - The http.enable.forwarded.requests property
                    in the Data Collector configuration file enables handling X-Forwarded-For,
                    X-Forwarded-Proto, X-Forwarded-Port request headers issued by a reverse proxy or
                    load balancer.</li>

                <li class="li"><a class="xref" href="../Origins/MongoDB.html#concept_kx3_zrs_ns">MongoDB
                        origin enhancement</a>  The origin now supports using any string field
                    as the offset field. </li>

            </ul>
</div>

 </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title43" id="concept_yym_xqt_5y">
    <h2 class="title topictitle2" id="ariaid-title43">What's New in 2.3.0.0</h2>

    <div class="body conbody">
        <div class="p"><span class="ph">Data Collector</span>
            version 2.3.0.0 includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Multithreaded Pipelines</dt>

                    <dd class="dd">You can use a multithreaded origin to generate <a class="xref" href="../Multithreaded_Pipelines/MultithreadedPipelines.html#concept_zpp_2xc_py">multithreaded pipelines</a> to perform parallel processing. <p dir="ltr" class="p">The new multithreaded framework includes the following
                            changes:</p>
<ul class="ul" id="concept_yym_xqt_5y__ul_slg_mrt_5y">
                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p"><a class="xref" href="../Origins/HTTPServer.html#concept_s2p_5hb_4y">HTTP
                                        Server origin</a> - Listens on an HTTP endpoint and
                                    processes the contents of all authorized HTTP POST requests. Use
                                    the HTTP Server origin to receive high volumes of HTTP POST
                                    requests using multiple threads.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p"><a class="xref" href="../Pipeline_Design/DevStages.html#concept_czx_ktn_ht">Enhanced Dev Data Generator origin</a> - Can create
                                    multiple threads for testing multithreaded pipelines.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p"><a class="xref" href="../Multithreaded_Pipelines/MultithreadedPipelines.html#concept_tdn_vwy_ry">Enhanced runtime statistics</a> - Monitoring a pipeline
                                    displays aggregated runtime statistics for all threads in the
                                    pipeline. You can also view the number of runners, i.e. threads
                                    and pipeline instances, being used.</p>

                            </li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">CDC/CRUD Enhancements</dt>

                    <dd class="dd">With this release, certain Data Collector stages enable you to easily
                        process change data capture (CDC) or transactional data in a pipeline. The
                        sdc.operation.type record header attribute is now used by all CDC-enabled
                        origins and CRUD-enabled stages:<p dir="ltr" class="p"><a class="xref" href="../Pipeline_Design/CDC-Overview.html#concept_iws_mhd_ty">CDC-enabled origins</a>:</p>
<ul class="ul" id="concept_yym_xqt_5y__ul_vb4_prt_5y">
                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p">The MongoDB Oplog and Salesforce origins are now
                                    enabled for processing changed data by including the CRUD
                                    operation type in the sdc.operation.type record header
                                    attribute. </p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p">Though previously CDC-enabled, the Oracle CDC Client and JDBC
                                    Query Consumer for Microsoft SQL Server now include CRUD
                                    operation type in the sdc.operation.type record header
                                    attribute. </p>

                                <p class="p">Previous operation type header attributes are still supported for
                                    backward-compatibility. </p>

                            </li>

                        </ul>
<p dir="ltr" class="p"><a class="xref" href="../Pipeline_Design/CDC-Overview.html#concept_lfb_phd_ty">CRUD-enabled stages</a>:</p>
<ul class="ul" id="concept_yym_xqt_5y__ul_wb4_prt_5y">
                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p">The JDBC Tee processor and JDBC Producer can now
                                    process changed data based on CRUD operations in record headers.
                                    The stages also include a default operation and unsupported
                                    operation handling. </p>

                            </li>

                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p">The MongoDB and Elasticsearch destinations now look for
                                    the CRUD operation in the sdc.operation.type record header
                                    attribute. The Elasticsearch destination includes a default
                                    operation and unsupported operation handling.</p>

                            </li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Multitable Copy</dt>

                    <dd class="dd">You can use the new <a class="xref" href="../Origins/MultiTableJDBCConsumer.html#concept_zp3_wnw_4y">JDBC
                            Multitable Consumer origin</a> when you need to copy multiple tables
                        to a destination system or for database replication. The JDBC Multitable
                        Consumer origin reads database data from multiple tables through a JDBC
                        connection. The origin generates SQL queries based on the table
                        configurations that you define.</dd>

                
                
                    <dt class="dt dlterm">Configuration</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_yym_xqt_5y__ul_up5_ntt_5y">
                            <li dir="ltr" class="li"><a class="xref" href="../Configuration/Authentication.html#concept_wgy_rxt_5x" title="If your organization does not use LDAP and you want to enable multiple users to access Data Collector, you might configure Data Collector to use file-based authentication.">Groups for file-based authentication</a> - If you use
                                file-based authentication, you can now create groups of users when
                                multiple users use Data Collector. You configure groups in the
                                associated realm.properties file located in the Data Collector
                                configuration directory, $SDC_CONF. <p class="p">If you use file-based
                                    authentication, you can also now view all user accounts granted
                                    access to the Data Collector, including the roles and groups
                                    assigned to each user.</p>
</li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Configuration/Authentication.html#concept_x2j_5ts_g5" title="If your organization uses LDAP and you want multiple users to access Data Collector, you can configure Data Collector to use LDAP authentication. After you configure LDAP authentication, users log in to Data Collector using their LDAP username and password.">LDAP authentication enhancements</a> - You can now
                                    configure Data Collector to use StartTLS to make secure
                                    connections to an LDAP server. You can also configure the
                                    userFilter property to define the LDAP user attribute used to
                                    log in to Data Collector. For example, a username, uid, or email
                                    address.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../DPM/RegisterSDCwithDPM.html#concept_dmr_df5_5y" title="You can configure each registered Data Collector to use an authenticated HTTP or HTTPS proxy server for outbound requests made to Control Hub. Define the proxy properties in the SDC_JAVA_OPTS environment variable.">Proxy
                                        configuration for outbound requests</a> - You can now
                                    configure Data Collector to use an authenticated HTTP proxy for
                                    outbound requests to Dataflow Performance Manager (DPM).</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Configuration/DCEnvironmentConfig.html#concept_kqh_lj3_vx" title="You can define the Java garbage collector that Data Collector uses. By default, Data Collector uses the Concurrent Mark Sweep (CMS) garbage collector.">Java garbage collector logging</a> - Data Collector now
                                    enables logging for the Java garbage collector by default. Logs
                                    are written to $SDC_LOG/gc.log. You can disable the logging if
                                    needed. </p>

                            </li>

                            <li dir="ltr" class="li">Heap dump for out of memory errors - Data Collector now
                                produces a heap dump file by default if it encounters an out of
                                memory error. You can configure the location of the heap dump file
                                or you can disable this default behavior. </li>

                            <li dir="ltr" class="li"><a class="xref" href="../Administration/Administration_title.html#task_lkv_g2f_wy" title="If the Data Collector logs do not provide enough troubleshooting information, you can modify the log level to display messages at another severity level.">Modifying the log level</a> - You can now use the Data
                                Collector UI to modify the log level to display messages at another
                                severity level.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Pipelines</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_yym_xqt_5y__ul_lzx_xtt_5y">
                            <li dir="ltr" class="li">Pipeline renaming - You can now rename pipelines by
                                editing the Title general pipeline property.</li>

                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p"><a class="xref" href="../Pipeline_Design/FieldAttributes.html#concept_xfm_wtp_1z" title="Field attributes are attributes that provide additional information about each field that you can use in pipeline logic, as needed.">Field attributes</a> - Data Collector now supports
                                    field-level attributes. Use the Expression Evaluator to add
                                    field attributes.</p>

                            </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_yym_xqt_5y__ul_ljw_15t_5y">
                            <li dir="ltr" class="li"><a class="xref" href="../Origins/HTTPServer.html#concept_s2p_5hb_4y">New HTTP Server origin</a> - A multithreaded origin that
                                listens on an HTTP endpoint and processes the contents of all
                                authorized HTTP POST requests. Use the HTTP Server origin to read
                                high volumes of HTTP POST requests using multiple threads. </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Origins/HTTPtoKafka.html#concept_izh_mqd_dy">New
                                        HTTP to Kafka origin</a> - Listens on a HTTP endpoint and
                                    writes the contents of all authorized HTTP POST requests
                                    directly to Kafka. Use to read high volumes of HTTP POST
                                    requests and write them to Kafka. </p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Origins/MapRDBJSON.html#concept_ywh_k15_3y">New
                                        MapR DB JSON origin</a> - Reads JSON documents from MapR
                                    DB JSON tables.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Origins/MongoDBOplog.html#concept_mjn_yqw_4y">New
                                        MongoDB Oplog origin</a> - Reads entries from a MongoDB
                                    Oplog. Use to process change information for data or database
                                    operations. </p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Origins/Directory.html#concept_xd5_5z4_4y" title="Use a file name pattern to define the files that the Directory origin processes. You can use either a glob pattern or a regular expression to define the file name pattern.">Directory origin enhancement</a> - You can use regular
                                    expressions in addition to glob patterns to define the file name
                                    pattern to process files. </p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Origins/HTTPClient.html#concept_c13_zz1_5y">HTTP Client origin enhancement</a> - You can now
                                    configure the origin to use the OAuth 2 protocol to connect to
                                    an HTTP service.</p>

                            </li>

                            <li dir="ltr" class="li"><a class="xref" href="../Origins/JDBCConsumer.html#concept_qhf_hjr_bs">JDBC
                                    Query Consumer origin enhancements</a> - The JDBC Consumer
                                origin has been renamed to the JDBC Query Consumer origin. The
                                origin functions the same as in previous releases. It reads database
                                data using a user-defined SQL query through a JDBC connection. You
                                can also now configure the origin to enable auto-commit mode for the
                                JDBC connection and to disable validation of the SQL query.</li>

                            <li class="li"><a class="xref" href="../Origins/MongoDB.html#concept_bk4_2rs_ns">MongoDB
                                    origin enhancements</a> - You can now use a nested field as
                                the offset field. The origin supports reading the MongoDB BSON
                                timestamp for MongoDB versions 2.6 and later. And you can configure
                                the origin to connect to a single MongoDB server or node. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Processors</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_yym_xqt_5y__ul_z35_cwt_5y">
                            <li dir="ltr" class="li"><a class="xref" href="../Processors/FieldTypeConverter.html#task_g23_2tq_wq">Field Type Converter processor enhancement</a> - You can now
                                configure the processor to convert timestamp data in a long field to
                                a String data type. Previously, you had to use one Field Type
                                Converter processor to convert the long field to a datetime, and
                                then use another processor to convert the datetime field to a
                                string.</li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Processors/HTTPClient.html#concept_ghx_ypr_fw">HTTP Client processor enhancements</a> - You can now
                                    configure the processor to use the OAuth 2 protocol to connect
                                    to an HTTP service. You can also configure a rate limit for the
                                    processor, which defines the maximum number of requests to make
                                    per second.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Processors/JDBCLookup.html#concept_ysc_ccy_hw">JDBC Lookup processor enhancements</a> - You can now
                                    configure the processor to enable auto-commit mode for the JDBC
                                    connection. You can also configure the processor to use a
                                    default value if the database does not return a lookup value for
                                    a column.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Processors/SalesforceLookup.html#concept_k23_3rk_yx">Salesforce Lookup processor enhancement</a> - You can
                                    now configure the processor to use a default value if Salesforce
                                    does not return a lookup value for a field.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p"><a class="xref" href="../Processors/XMLParser.html#concept_dtt_q5q_k5">XML
                                        Parser enhancement</a> - A new Multiple Values Behavior
                                    property allows you to specify the behavior when you define a
                                    delimiter element and the document includes more than one value:
                                    Return the first value as a record, return one record with a
                                    list field for each value, or return all values as records.</p>

                            </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Destinations</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_yym_xqt_5y__ul_uys_hwt_5y">
                            <li dir="ltr" class="li"><a class="xref" href="../Destinations/MapRDBJSON.html#concept_i4h_2kj_dy">New
                                    MapR DB JSON destination</a> - Writes data as JSON documents
                                to MapR DB JSON tables.</li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Destinations/DataLakeStore.html#concept_jzm_kf4_zx">Azure Data Lake Store destination</a> enhancement - You
                                    can now use the destination in cluster batch pipelines. You can
                                    also process binary and protobuf data, use record header
                                    attributes to write records to files and roll files, and
                                    configure a file suffix and the maximum number of records that
                                    can be written to a file. </p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Destinations/Elasticsearch.html#concept_u5t_vpv_4r">Elasticsearch destination enhancement</a> - The
                                    destination now uses the Elasticsearch HTTP API. With this API,
                                    the Elasticsearch version 5 stage library is compatible with all
                                    versions of Elasticsearch. Earlier stage library versions have
                                    been removed. Elasticsearch is no longer supported on Java 7.
                                    Youll need to verify that Java 8 is installed on the Data
                                    Collector machine and remove this stage from the blacklist
                                    property in $SDC_CONF/sdc.properties before you can use it. </p>

                                <p class="p">You can also now configure the destination to perform any of the
                                    following CRUD operations: create, update, delete, or index.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Destinations/HiveMetastore.html#concept_x4p_fyc_rx">Hive Metastore destination enhancement</a> - New table
                                    events now include information about columns and partitions in
                                    the table.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Destinations/HadoopFS-destination.html#concept_uv2_vfb_vy">Hadoop FS</a>, <a class="xref" href="../Destinations/LocalFS.html#concept_s5n_ggc_vy">Local FS</a>, and <a class="xref" href="../Destinations/MapRFS.html#concept_vvr_ngc_vy">MapR FS</a> destination enhancement - The destinations
                                    now support recovery after an unexpected stop of the pipeline by
                                    renaming temporary files when the pipeline restarts.</p>

                            </li>

                            <li dir="ltr" class="li">Redis destination enhancement - You can now configure a
                                timeout for each key that the destination writes to Redis.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Executors</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_yym_xqt_5y__ul_zqk_4wt_5y">
                            <li dir="ltr" class="li">
                                <div class="p" dir="ltr"><a class="xref" href="../Executors/HiveQuery.html#concept_kjw_llk_fx">Hive
                                        Query executor enhancements</a>: <ul class="ul" id="concept_yym_xqt_5y__ul_gf2_qwt_5y">
                                        <li class="li">The executor can now execute multiple queries for each
                                            event that it receives.</li>

                                        <li class="li">It can also generate event records each time it
                                            processes a query.</li>

                                    </ul>
</div>

                            </li>

                        </ul>

                        <ul class="ul" id="concept_yym_xqt_5y__ul_brk_4wt_5y">
                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p"><a class="xref" href="../Executors/JDBCQuery.html#concept_j3r_gcv_sx">JDBC
                                        Query executor enhancement</a> - You can now configure
                                    the executor to enable auto-commit mode for the JDBC
                                    connection.</p>

                            </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Data Formats</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_yym_xqt_5y__ul_hpw_zwt_5y">
                            <li class="li"><a class="xref" href="../Data_Formats/WholeFile.html#concept_prp_jzd_py">Whole File enhancement</a> - You can now specify a transfer
                                rate to help control the resources used to process whole files. You
                                can specify the rate limit in all origins that process whole
                                files.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Expression Language</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_yym_xqt_5y__ul_hq3_2xt_5y">
                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_dvg_nqn_wx">New pipeline functions</a> - You can use the following new
                                pipeline functions to return pipeline information:<ul class="ul" id="concept_yym_xqt_5y__ul_mls_bp1_cz">
                                    <li class="li">pipeline:id() - Returns the pipeline ID, a UUID that is
                                        automatically generated and used by Data Collector to
                                        identify the pipeline. <div class="note note"><span class="notetitle">Note:</span> The existing pipeline:name()
                                            function now returns the pipeline ID instead of the
                                            pipeline name since pipeline ID is the correct way to
                                            identify a pipeline.</div>
</li>

                                    <li class="li">
                                        <p class="p">pipeline:title() - Returns the pipeline title or
                                            name.</p>

                                    </li>

                                </ul>
</li>

                            <li dir="ltr" class="li">
                                <div class="p" dir="ltr"><a class="xref" href="../Expression_Language/Functions.html#concept_p1z_ggv_1r" title="Use record functions to determine information about a record, such as the stage that created it or whether a field exists in the record.">New record functions</a> - You can use the following new
                                    record functions to work with field attributes:<ul class="ul" id="concept_yym_xqt_5y__ul_k4h_kxt_5y">
                                        <li class="li">record:fieldAttribute (&lt;field path&gt;, &lt;attribute
                                            name&gt;) - Returns the value of the specified field
                                            attribute. </li>

                                        <li class="li">record:fieldAttributeOrDefault (&lt;field path&gt;,
                                            &lt;attribute name&gt;, &lt;default value&gt;) - Returns the
                                            value of the specified field attribute. Returns the
                                            default value if the attribute does not exist or
                                            contains no value.</li>

                                    </ul>
</div>

                            </li>

                            <li dir="ltr" class="li">
                                <div class="p" dir="ltr"><a class="xref" href="../Expression_Language/Functions.html#concept_ahp_f4v_1r">New string functions</a> - You can use the following new
                                    string functions to transform string data: <ul class="ul" id="concept_yym_xqt_5y__ul_knh_mxt_5y">
                                        <li class="li">str:urlEncode (&lt;string&gt;, &lt;encoding&gt;) - Returns a
                                            URL encoded string from a decoded string using the
                                            specified encoding format. </li>

                                        <li class="li">str:urlDecode (&lt;string&gt;, &lt;encoding&gt;) - Returns a
                                            decoded string from a URL encoded string using the
                                            specified encoding format.</li>

                                    </ul>
</div>

                            </li>

                            <li dir="ltr" class="li">
                                <div class="p" dir="ltr"><a class="xref" href="../Expression_Language/Functions.html#concept_qkr_trf_sw" title="Use time functions to return the current time or to transform datetime data.">New time functions</a> - You can use the following new
                                    time functions to transform datetime data: <ul class="ul" id="concept_yym_xqt_5y__ul_yk5_pxt_5y">
                                        <li class="li">time:dateTimeToMilliseconds (&lt;Date object&gt;) -
                                            Converts a Date object to an epoch or UNIX time in
                                            milliseconds. </li>

                                        <li class="li">time:extractDateFromString(&lt;string&gt;, &lt;format
                                            string&gt;) - Extracts a Date object from a String, based
                                            on the specified date format. </li>

                                        <li class="li">time:extractStringFromDateTZ (&lt;Date object&gt;,
                                            &lt;timezone&gt;, &lt;format string&gt;) - Extracts a string
                                            value from a Date object based on the specified date
                                            format and time zone.</li>

                                    </ul>
</div>

                            </li>

                            <li dir="ltr" class="li">
                                <div class="p" dir="ltr"><a class="xref" href="../Expression_Language/Functions.html#concept_ddw_ld1_1s">New and enhanced miscellaneous functions</a> - You can
                                    use the following new and enhanced miscellaneous functions: <ul class="ul" id="concept_yym_xqt_5y__ul_m2x_nxt_5y">
                                        <li class="li">offset:column(&lt;position&gt;) - Returns the value of the
                                            positioned offset column for the current table.
                                            Available only in the additional offset column
                                            conditions of the JDBC Multitable Consumer origin. </li>

                                        <li class="li">every function - You can now use the function with the
                                            hh() datetime variable in directory templates. This
                                            allows you to create directories based on the specified
                                            interval for hours.</li>

                                    </ul>
</div>

                            </li>

                        </ul>

                    </dd>

                
            </dl>
</div>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title44" id="concept_wbf_dgk_fy">
 <h2 class="title topictitle2" id="ariaid-title44">What's New in 2.2.1.0</h2>

 <div class="body conbody">
        <div class="p"><span class="ph">Data Collector</span>
            version 2.2.1.0 includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Processors</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_wbf_dgk_fy__ul_y1c_1pm_3y">
                            <li class="li">New <a class="xref" href="../Processors/FieldZip.html#concept_o3b_t1k_yx">Field Zip processor</a> - Merges two List fields or two
                                List-Map fields in the same record.</li>

                            <li class="li">New <a class="xref" href="../Processors/SalesforceLookup.html#concept_k23_3rk_yx">Salesforce Lookup processor</a> - Performs lookups in a
                                Salesforce object and passes the lookup values to fields. Use the
                                Salesforce Lookup to enrich records with additional data.</li>

                            <li class="li"><a class="xref" href="../Processors/ValueReplacer.html#concept_ppg_ztk_3y">Value Replacer</a> enhancement - You can now replace field
                                values with nulls using a condition.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Destination</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_wbf_dgk_fy__ul_vlq_dpm_3y">
                            <li class="li"><a class="xref" href="../Destinations/DataLakeStore.html#concept_wsz_qj4_zx">Whole file support in the Azure Data Lake Store
                                    destination</a> - You can now use the whole file data format
                                to stream whole files to Azure Data Lake Store. </li>

                        </ul>

                    </dd>

                
            </dl>
</div>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title45" id="concept_oyv_zfk_fy">
 <h2 class="title topictitle2" id="ariaid-title45">What's New in 2.2.0.0</h2>

 <div class="body conbody">
  <div class="p"><span class="ph">Data Collector</span> version
            2.2.0.0 includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Event Framework</dt>

                    <dd class="dd">The Data Collector event framework enables the pipeline to trigger tasks in
                        external systems based on actions that occur in the pipeline, such as
                        running a MapReduce job after the pipeline writes a file to HDFS. You can
                        also use the event framework to store event information, such as when an
                        origin starts or completes reading a file. </dd>

                    <dd class="dd ddexpand">For details, see the <a class="xref" href="../Event_Handling/EventFramework-Title.html#concept_cph_5h4_lx">Event Framework chapter</a>. </dd>

                    <dd class="dd ddexpand">The event framework includes the following new features and enhancements:<ul class="ul" id="concept_oyv_zfk_fy__ul_ojf_xgk_fy">
                            <li class="li"><a class="xref" href="../Executors/Executors-overview.html#concept_stt_2lk_fx">New executor stages</a>. A new type of stage that performs
                                tasks in external systems upon receiving an event. This release
                                includes the following executors:<ul class="ul" id="concept_oyv_zfk_fy__ul_bn4_ygk_fy">
                                    <li class="li"><a class="xref" href="../Executors/HDFSMetadata.html#concept_wgj_slk_fx">HDFS File Metadata executor</a> - Changes file
                                        metadata such as the name, location, permissions, and ACLs. </li>

                                    <li class="li"><a class="xref" href="../Executors/HiveQuery.html#concept_kjw_llk_fx">Hive Query executor</a> - Runs a Hive or Impala
                                        query. </li>

                                    <li class="li"><a class="xref" href="../Executors/JDBCQuery.html#concept_j3r_gcv_sx">JDBC Query executor</a> - Runs a SQL query. </li>

                                    <li class="li"><a class="xref" href="../Executors/MapReduce.html#concept_bj2_zlk_fx">MapReduce executor</a> - Runs a custom MapReduce job
                                        or an Avro to Parquet MapReduce job. </li>

                                </ul>
</li>

                            <li class="li">Event generation. The following stages now generate events that you
                                can use in a pipeline: <ul class="ul" id="concept_oyv_zfk_fy__ul_sxd_bhk_fy">
                                    <li class="li"><a class="xref" href="../Origins/Directory.html#concept_ttg_vgn_qx">Directory</a> and <a class="xref" href="../Origins/FileTail.html#concept_gwn_c32_px">File Tail</a> origins - Generate events when they
                                        start and complete reading a file.</li>

                                    <li class="li"><a class="xref" href="../Destinations/AmazonS3.html#concept_aqq_tt2_px">Amazon S3 destination</a> - Generates events when it
                                        completes writing to an object or streaming a whole file. </li>

                                    <li class="li"><a class="xref" href="../Destinations/HadoopFS-destination.html#concept_bvb_rxj_px">Hadoop FS</a>, <a class="xref" href="../Destinations/LocalFS.html#concept_in1_fcm_px">Local FS</a>, and <a class="xref" href="../Destinations/MapRFS.html#concept_bqd_3qb_rx">MapR FS</a> destinations - Generate events when they
                                        close an output file or complete streaming a whole file. </li>

                                    <li class="li"><a class="xref" href="../Processors/Groovy.html#concept_qcz_ssq_1y">Groovy Evaluator</a>, <a class="xref" href="../Processors/JavaScript.html#concept_mkv_wgh_cy">JavaScript Evaluator</a>, and <a class="xref" href="../Processors/Jython.html#concept_zhd_chh_cy">Jython Evaluator</a> processors - Can run scripts
                                        that generate events. </li>

                                    <li class="li"><a class="xref" href="../Executors/HDFSMetadata.html#concept_vhl_mfj_rx">HDFS File Metadata executor</a> - Generates events
                                        when it changes file metadata. </li>

                                    <li class="li"><a class="xref" href="../Executors/MapReduce.html#concept_e1s_sm5_sx">MapReduce executor</a> - Generates events when it
                                        starts a MapReduce job.</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Pipeline_Design/DevStages.html#concept_czx_ktn_ht">Dev stages</a>. You can use the following stages to develop
                                and test event handling: <ul class="ul" id="concept_oyv_zfk_fy__ul_ysc_2hk_fy">
                                    <li class="li">Dev Data Generator enhancement - You can now configure the
                                        Dev Data Generator to generate event records as well as data
                                        records. You can also specify the number of records in a
                                        batch. </li>

                                    <li class="li">To Event - Generates event records using the incoming record
                                        as the body of the event record.</li>

                                </ul>
</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Installation</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_q3x_pvm_3y">
                            <li class="li"><a class="xref" href="../Installation/InstallationAndConfig.html#concept_vzg_n2p_kq" title="Install Data Collector on a machine that meets the following minimum requirements. To run pipelines in cluster execution mode, each node in the cluster must meet the minimum requirements.">Installation requirements</a>:<ul class="ul" id="concept_oyv_zfk_fy__ul_zh5_qvm_3y">
                                    <li class="li">Java requirement - Oracle Java 7 is supported but now
                                        deprecated. Oracle announced the end of public updates for
                                        Java 7 in April 2015. StreamSets recommends migrating to
                                        Java 8, as Java 7 support will be removed in a future Data
                                        Collector release. </li>

                                    <li class="li">File descriptors requirement - Data Collector now requires a
                                        minimum of 32,768 open file descriptors. </li>

                                </ul>
</li>

                        </ul>

                        <ul class="ul" id="concept_oyv_zfk_fy__ul_jtf_ghk_fy">
                            <li class="li"><a class="xref" href="../Installation/CoreInstall_Overview.html#concept_vvw_p3m_s5" title="Users with a StreamSets enterprise account can use the Data Collector core installation.">Core installation</a> includes the basic stage library only
                                - The core RPM and tarball installations now include the basic stage
                                library only, to allow Data Collector to use less disk space.
                                Install additional stage libraries using the Package Manager for
                                tarball installations or the command line for RPM and tarball
                                installations. <p class="p">Previously, the core installation also included
                                    the Groovy, Jython, and statistics stage libraries.</p>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Configuration</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_v5w_mhk_fy">
                            <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">New stage libraries</a>. Data Collector now supports the
                                following stage libraries: <ul class="ul" id="concept_oyv_zfk_fy__ul_oxv_nhk_fy">
                                    <li class="li">Apache Kudu version 1.0.x - Earlier Kudu versions are no
                                        longer supported. </li>

                                    <li class="li">Cloudera CDH version 5.9 distribution of Apache Hadoop. </li>

                                    <li class="li">Cloudera version 5.9 distribution of Apache Kafka 2.0. </li>

                                    <li class="li">Elasticsearch version 5.0.x. </li>

                                    <li class="li">Google Cloud Bigtable. </li>

                                    <li class="li">Hortonworks HDP version 2.5 distribution of Apache Hadoop. </li>

                                    <li class="li">MySQL Binary Log. </li>

                                    <li class="li">Salesforce. </li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Configuration/Authentication.html#concept_x2j_5ts_g5" title="If your organization uses LDAP and you want multiple users to access Data Collector, you can configure Data Collector to use LDAP authentication. After you configure LDAP authentication, users log in to Data Collector using their LDAP username and password.">LDAP authentication</a> - If you use LDAP authentication,
                                you can now configure Data Collector to connect to multiple LDAP
                                servers. You can also configure Data Collector to support an LDAP
                                deployment where members are defined by uid or by full DN. </li>

                            <li class="li"><a class="xref" href="../Configuration/DCEnvironmentConfig.html#concept_kqh_lj3_vx" title="You can define the Java garbage collector that Data Collector uses. By default, Data Collector uses the Concurrent Mark Sweep (CMS) garbage collector.">Java garbage collector</a> - Data Collector now uses the
                                Concurrent Mark Sweep (CMS) garbage collector by default. You can
                                configure Data Collector to use a different garbage collector by
                                modifying Java configuration options in the Data Collector
                                environment configuration file.</li>

                            <li class="li">Environment variables for <a class="xref" href="../Configuration/DCEnvironmentConfig.html#concept_vrx_4fg_qr">Java configuration options</a>. Data Collector now uses
                                three environment variables to define Java configuration options:
                                    <ul class="ul" id="concept_oyv_zfk_fy__ul_wym_thk_fy">
                                    <li class="li">SDC_JAVA_OPTS - Includes configuration options for all Java
                                        versions. SDC_JAVA7_OPTS - Includes configuration options
                                        used only when Data Collector is running Java 7.</li>

                                    <li class="li">SDC_JAVA8_OPTS - Includes configuration options used only
                                        when Data Collector is running Java 8. </li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Getting_Started/GettingStarted_Title.html#task_r3q_fnx_pr">New time zone property</a> - You can configure the Data
                                Collector UI to use UTC, the browser time zone, or the Data
                                Collector time zone. The time zone property affects how dates and
                                times display in the UI. The default is the browser time zone.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_kq1_whk_fy">
                            <li class="li">New <a class="xref" href="../Origins/MySQLBinaryLog.html#concept_kqg_1yh_xx">MySQL Binary Log origin</a> - Reads MySQL binary logs to
                                generate records with change data capture information. </li>

                            <li class="li">New <a class="xref" href="../Origins/Salesforce.html#concept_odf_vr3_rx">Salesforce origin </a>- Reads data from Salesforce. The
                                origin can execute a SOQL query to read existing data from
                                Salesforce. The origin can also subscribe to the Force.com Streaming
                                API to receive notifications for changes to Salesforce data. </li>

                            <li class="li"><a class="xref" href="../Origins/Directory.html#concept_qpt_rg3_cy" title="When using the Last Modified Timestamp read order, the Directory origin can read files in subdirectories of the specified file directory.">Directory origin</a> enhancement - You can configure the
                                Directory origin to read files from all subdirectories when using
                                the last-modified timestamp for the read order. </li>

                            <li class="li"><a class="xref" href="../Origins/JDBCConsumer.html#task_ryz_tkr_bs">JDBC Query Consumer</a> and <a class="xref" href="../Origins/OracleCDC.html#task_ehh_mjj_tw">Oracle CDC Client</a> origin enhancement - You can now
                                configure the transaction isolation level that the JDBC Query
                                Consumer and Oracle CDC Client origins use to connect to the
                                database. Previously, the origins used the default transaction
                                isolation level configured for the database.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Processors</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_wfg_13k_fy">
                            <li class="li">New <a class="xref" href="../Processors/Spark.html#concept_cpx_1lm_zx">Spark
                                    Evaluator processor</a> - Processes data based on a Spark
                                application that you develop. Use the Spark Evaluator processor to
                                develop a Spark application that performs custom processing within a
                                pipeline. </li>

                            <li class="li"><a class="xref" href="../Processors/FieldFlattener.html#concept_vpx_zc1_xx">Field Flattener processor</a> enhancements - In addition to
                                flattening the entire record, you can also now use the Field
                                Flattener processor to flatten specific list or map fields in the
                                record. </li>

                            <li class="li"><a class="xref" href="../Processors/FieldTypeConverter.html#concept_sym_c4g_xx" title="You can use the Field Type Converter processor to change the scale of decimal fields. For example, you might have a decimal field with the value 12345.6789115, and you'd like to decrease the scale to 4 so that the value is 12345.6789.">Field Type Converter processor</a> enhancements - You can
                                now use the Field Type Converter processor to change the scale of a
                                decimal field. Or, if you convert a field with another data type to
                                the Decimal data type, you can configure the scale to use in the
                                conversion. </li>

                            <li class="li"><a class="xref" href="../Processors/ListPivoter.html#concept_ekg_313_qw">Field
                                    Pivoter processor</a> enhancements - The List Pivoter
                                processor has been renamed to the Field Pivoter processor. You can
                                now use the processor to pivot data in a list, map, or list-map
                                field. You can also use the processor to save the field name of the
                                first-level item in the pivoted field. </li>

                            <li class="li"><a class="xref" href="../Processors/JDBCLookup.html#task_kbr_2cy_hw">JDBC Lookup</a> and <a class="xref" href="../Processors/JDBCTee.html#task_qpj_ncy_hw">JDBC Tee</a> processor enhancement - You can now configure
                                the transaction isolation level that the JDBC Lookup and JDBC Tee
                                processors use to connect to the database. Previously, the origins
                                used the default transaction isolation level configured for the
                                database. </li>

                            <li class="li">Scripting processor enhancements - The <a class="xref" href="../Processors/Groovy.html#concept_qcz_ssq_1y">Groovy Evaluator</a>, <a class="xref" href="../Processors/JavaScript.html#concept_mkv_wgh_cy">JavaScript Evaluator</a>, and <a class="xref" href="../Processors/Jython.html#concept_zhd_chh_cy">Jython Evaluator</a> processors can generate event records
                                and work with record header attributes. The sample scripts now
                                include examples of both and a new tip for generating unique record
                                IDs. </li>

                            <li class="li"><a class="xref" href="../Processors/XMLFlattener.html#task_pmb_l55_sv" title="Configure an XML Flattener to flatten XML data embedded in a string field.">XML Flattener processor</a> enhancement - You can now
                                configure the XML Flattener processor to write the flattened data to
                                a new output field. Previously, the processor wrote the flattened
                                data to the same field. </li>

                            <li class="li"><a class="xref" href="../Processors/XMLParser.html#concept_dtt_q5q_k5">XML
                                    Parser processor</a> enhancement. You can now generate
                                records from XML documents using simplified XPath expressions. This
                                enables reading records from deeper within XML documents. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Destination</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_v5j_l3k_fy">
                            <li class="li">New <a class="xref" href="../Destinations/DataLakeStore.html#concept_jzm_kf4_zx">Azure Data Lake Store destination</a> - Writes data to
                                Microsoft Azure Data Lake Store. </li>

                            <li class="li">New <a class="xref" href="../Destinations/Bigtable.html#concept_pl5_tmq_tx">Google Bigtable destination</a> - Writes data to Google
                                Cloud Bigtable. </li>

                            <li class="li">New <a class="xref" href="../Destinations/Salesforce.html#concept_rlb_rt3_rx">Salesforce destination</a> - Writes data to Salesforce. New
                                Wave Analytics destination. Writes data to Salesforce Wave
                                Analytics. The destination creates a dataset with external data. </li>

                            <li class="li"><a class="xref" href="../Destinations/AmazonS3.html#task_pxb_j3r_rt">Amazon S3 destination</a> change - The AWS KMS Key ID
                                property has been renamed AWS KMS Key ARN. Data Collector upgrades
                                existing pipelines seamlessly. </li>

                            <li class="li">File suffix enhancement. You can now configure a file suffix, such
                                as txt or json, for output files generated by <a class="xref" href="../Destinations/HadoopFS-destination.html#concept_awl_4km_zq">Hadoop FS</a>, <a class="xref" href="../Destinations/LocalFS.html#concept_zvc_bv5_1r">Local
                                    FS</a>, <a class="xref" href="../Destinations/MapRFS.html#concept_spv_xlc_fv">MapR
                                    FS</a>, and the <a class="xref" href="../Destinations/AmazonS3.html#concept_avx_bnq_rt">Amazon
                                    S3</a> destinations. </li>

                            <li class="li"><a class="xref" href="../Destinations/JDBCProducer.html#concept_kvs_3hh_ht">JDBC Producer</a> destination enhancement - You can now
                                configure the transaction isolation level that the JDBC Producer
                                destination uses to connect to the database. Previously, the
                                destination used the default transaction isolation level configured
                                for the database. </li>

                            <li class="li"><a class="xref" href="../Destinations/Kudu.html#concept_dvg_vvj_wx">Kudu destination</a> enhancement - You can now configure the
                                destination to perform one of the following write operations:
                                insert, update, delete, or upsert.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Data Formats</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_r3d_dkk_fy">
                            <li class="li"><a class="xref" href="../Data_Formats/XMLDFormat.html#concept_lty_42b_dy">XML processing</a> enhancement - You can now generate
                                records from XML documents using <a class="xref" href="../Data_Formats/XMLDFormat.html#concept_zw2_mfk_dy">simplified XPath expressions</a> with origins that process
                                XML data and the XML Parser processor. This enables reading records
                                from deeper within XML documents. </li>

                            <li class="li">Consolidated data format properties - You now configure the data
                                format and related properties on a new Data Format tab. Previously,
                                data formats had individual configuration tabs, e.g., Avro,
                                Delimited, Log. <p class="p">Related properties, such as Charset, Compression
                                    Format, and Ignore Control Characters now appear on the Data
                                    Format tab as well. </p>
</li>

                            <li class="li"><a class="xref" href="../Data_Formats/WholeFile.html#concept_ojv_sr4_vx">Checksum generation for whole files</a> - Destinations that
                                stream whole files can now generate checksums for the files so you
                                can confirm the accurate transmission of the file.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Pipeline Maintenance</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_h24_3kk_fy">
                            <li class="li">Add labels to pipelines from the Home page - You can now add labels
                                to multiple pipelines from the Data Collector Home page. Use labels
                                to group similar pipelines. For example, you might want to group
                                pipelines by database schema or by the test or production
                                environment. </li>

                            <li class="li">Reset the origin for multiple pipelines from the Home page - You can
                                now reset the origin for multiple pipelines at the same time from
                                the Data Collector Home page.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Rules and Alerts</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_sfk_lkk_fy">
                            <li class="li"><a class="xref" href="../Alerts/RulesAlerts_title.html#concept_ky2_g4f_qv">Metric
                                    rules and alerts</a> enhancements - The gauge metric type can
                                now provide alerts based on the number of input, output, or error
                                records for the last processed batch.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Expression Language Functions</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_bwr_nkk_fy">
                            <li class="li">New <a class="xref" href="../Expression_Language/Functions.html#concept_kxj_nyl_5x">file functions </a>- You can use the following new file
                                functions to work with file paths:<ul class="ul" id="concept_oyv_zfk_fy__ul_xdq_4kk_fy">
                                    <li class="li">file:fileExtension(&lt;filepath&gt;) - Returns the file
                                        extension from a path. </li>

                                    <li class="li">file:fileName(&lt;filepath&gt;) - Returns a file name from a
                                        path. </li>

                                    <li class="li">file:parentPath(&lt;filepath&gt;) - Returns the parent path of
                                        the specified file or directory. </li>

                                    <li class="li">file:pathElement(&lt;filepath&gt;, &lt;integer&gt;) - Returns the
                                        portion of the file path specified by a positive or negative
                                        integer. </li>

                                    <li class="li">file:removeExtension(&lt;filepath&gt;) - Removes the file
                                        extension from a path. </li>

                                </ul>
</li>

                            <li class="li">New <a class="xref" href="../Expression_Language/Functions.html#concept_dvg_nqn_wx">pipeline functions</a> - You can use the following new
                                pipeline functions to determine information about a pipeline: <ul class="ul" id="concept_oyv_zfk_fy__ul_lc5_rkk_fy">
                                    <li class="li">pipeline:name() - Returns the pipeline name. </li>

                                    <li class="li">pipeline:version() - Returns the pipeline version when the
                                        pipeline has been published to Dataflow Performance Manager
                                        (DPM). </li>

                                </ul>
</li>

                            <li class="li">New <a class="xref" href="../Expression_Language/Functions.html#concept_qkr_trf_sw" title="Use time functions to return the current time or to transform datetime data.">time functions</a> - You can use the following new time
                                functions to transform datetime data:<ul class="ul" id="concept_oyv_zfk_fy__ul_znz_skk_fy">
                                    <li class="li"> time:extractLongFromDate(&lt;Date object&gt;, &lt;string&gt;) -
                                        Extracts a long value from a Date object, based on the
                                        specified date format. </li>

                                    <li class="li">time:extractStringFromDate(&lt;Date object&gt;, &lt;string&gt;) -
                                        Extracts a string value from a Date object, based on the
                                        specified date format. </li>

                                    <li class="li">time:millisecondsToDateTime(&lt;long&gt;) - Converts an epoch
                                        or UNIX time in milliseconds to a Date object. </li>

                                </ul>
</li>

                        </ul>

                    </dd>

                
            </dl>
</div>

 </div>

</article>
</article>
</article></main></div>
                        
                        
                        
                    </div>
                    
                </div>
            </div>
        </div> <nav class="navbar navbar-default wh_footer">
  <div class=" footer-container text-center ">
    <!-- Copyright 2018 StreamSets Inc. --><!-- SDC google analytics --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-60917135-3', 'auto');
  ga('send', 'pageview');
</script>
  </div>
</nav>

        
        <div id="go2top">
            <span class="glyphicon glyphicon-chevron-up"></span>
        </div>
        
        <!-- The modal container for images -->
        <div id="modal_img_large" class="modal">
            <span class="close glyphicon glyphicon-remove"></span>
            <!-- Modal Content (The Image) -->
            <img class="modal-content" id="modal-img" />
            <!-- Modal Caption (Image Text) -->
            <div id="caption"></div>
        </div>
        
        <script src="../../../oxygen-webhelp/lib/bootstrap/js/bootstrap.min.js" type="text/javascript"></script>
         Apache License, Version 2.0.
    </body>
</html>