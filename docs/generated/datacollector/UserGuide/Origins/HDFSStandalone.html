
<!DOCTYPE html
  SYSTEM "about:legacy-compat">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />        
      <meta name="copyright" content="(C) Copyright 2005" /><meta name="DC.rights.owner" content="(C) Copyright 2005" /><meta name="DC.Type" content="concept" /><meta name="DC.Title" content="Hadoop FS Standalone" /><meta name="abstract" content="The Hadoop FS Standalone origin reads files in HDFS. The origin can use multiple threads to enable the parallel processing of files. The files to be processed must all share a file name pattern and be fully written. You can also configure the origin to read from Azure HDInsight." /><meta name="description" content="The Hadoop FS Standalone origin reads files in HDFS. The origin can use multiple threads to enable the parallel processing of files. The files to be processed must all share a file name pattern and be fully written. You can also configure the origin to read from Azure HDInsight." /><meta name="DC.Relation" scheme="URI" content="../../../datacollector/UserGuide/Origins/Origins_title.html" /><meta name="DC.Format" content="XHTML" /><meta name="DC.Identifier" content="concept_djz_pdm_hdb" /><link rel="stylesheet" type="text/css" href="../../../oxygen-webhelp/resources/css/commonltr.css?buildId=@@WEBHELP_BUILD_NUMBER@@"><!----></link><title>Hadoop FS Standalone</title><!--  Generated with Oxygen version @@WEBHELP_VERSION@@, build number @@WEBHELP_BUILD_NUMBER@@.  --><link rel="stylesheet" type="text/css" href="../../../oxygen-webhelp/resources/css/webhelp_topic.css?buildId=@@WEBHELP_BUILD_NUMBER@@"><!----></link><link rel="stylesheet" type="text/css" href="../../../oxygen-webhelp/resources/skins/skin.css?buildId=@@WEBHELP_BUILD_NUMBER@@" /><script type="text/javascript"><!--
          
          var prefix = "../../../index.html";
          
          --></script><script type="text/javascript" src="../../../oxygen-webhelp/resources/js/jquery-3.1.1.min.js"><!----></script><script type="text/javascript" src="../../../oxygen-webhelp/resources/js/jquery.cookie.js"><!----></script><script type="text/javascript" src="../../../oxygen-webhelp/resources/js/jquery.highlight-3.js"><!----></script><script type="text/javascript" charset="utf-8" src="../../../oxygen-webhelp/resources/js/log.js?buildId=@@WEBHELP_BUILD_NUMBER@@"><!----></script><script type="text/javascript" charset="utf-8" src="../../../oxygen-webhelp/resources/js/webhelp_topic.js?buildId=@@WEBHELP_BUILD_NUMBER@@"><!----></script>
  <!--
  Copyright 2018 StreamSets Inc.
  
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
  <script type="text/javascript">
    <!--
    var parentWindow = window.name;
    console.log("1. " + parentWindow);
    if (!(parentWindow == "frm" || parentWindow == "contentwin")) {
        var currHash = window.location.hash.substr(1);
        console.log("2. " + currHash);
        console.log("3. " + currHash.indexOf("datacollector/UserGuide/"));

        if ( currHash.indexOf("datacollector/UserGuide/") == -1 ) {
            window.location.hash = "#datacollector/UserGuide/" + currHash;
        }
    }
-->
  </script>
</head>
<body onload="highlightSearchTerm()" class="frmBody">
<table class="nav"><tbody><tr><td colspan="2"><div id="printlink"><a href="javascript:window.print();" title="Print this page"></a></div><div id="permalink"><a href="#" title="Link to this page"></a></div></td></tr><tr><td style="width:75%;"><span class="topic_breadcrumb_links"><span class="topic_breadcrumb_link"><a class="navheader_parent_path" href="../../../datacollector/UserGuide/Origins/Origins_title.html" title="Origins">Origins</a></span></span></td><td><span id="topic_navigation_links" class="navheader">
<span class="navparent"><a class="link" href="../../../datacollector/UserGuide/Origins/Origins_title.html" title="Origins"><span class="navheader_label">Parent topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Origins</span></a></span>  </span></td></tr></tbody></table>
<div class="nested0" id="concept_djz_pdm_hdb">
    <h1 class="title topictitle1">Hadoop FS Standalone</h1>

    
    <div class="body conbody"><p class="shortdesc">The Hadoop FS Standalone origin reads files in HDFS. The origin can use multiple
        threads to enable the parallel processing of files. The files to be processed must all share
        a file name pattern and be fully written. You can also configure the origin to read from
        Azure HDInsight.</p>

        <p class="p">Use the Hadoop FS Standalone origin
            only in pipelines configured for standalone execution mode. To read from HDFS in cluster
            execution mode, use the <a class="xref" href="HadoopFS-origin.html#concept_lw2_tnm_vs" title="The Hadoop FS origin reads data from the Hadoop Distributed File System (HDFS) or from other file systems using the Hadoop FileSystem interface.">Hadoop FS
                origin</a>.</p>

        <p class="p">When you configure the Hadoop FS Standalone origin, you define the directory to use, read
            order, file name pattern, file name pattern mode, and the first file to process. You can
            use glob patterns or regular expressions to define the file name pattern that you want
            to use. </p>

        <p class="p">When using the last-modified timestamp read order, you can configure the origin to read
            from subdirectories. To use multiple threads for processing, specify the number of
            threads to use. You can also enable reading compressed files. After processing a file,
            the Hadoop FS Standalone origin can keep, archive, or delete the file. </p>

        <p class="p">When the pipeline stops, the Hadoop FS Standalone origin notes where it stops reading.
            When the pipeline starts again, the origin continues processing from where it stopped by
            default. You can reset the origin to process all requested files. </p>

        <p class="p">The origin generates record header attributes that enable you to use the origins of a
            record in pipeline processing. </p>

        <p class="p">When necessary, you can enable Kerberos authentication and
                  specify a Hadoop user. You can also use Hadoop configuration files and add other
                  Hadoop configuration properties as needed. </p>

        <p class="p">The origin can generate events for an event stream. For
                  more information about dataflow triggers and the event framework, see <a class="xref" href="../Event_Handling/EventFramework-Title.html#concept_cph_5h4_lx">Dataflow Triggers Overview</a>. </p>

    </div>

<div class="related-links"></div>
<div class="topic concept nested1" id="concept_exc_22h_ldb">
    <h2 class="title topictitle2">File Name Pattern and Mode</h2>

    
    <div class="body conbody"><p class="shortdesc">Use a file name pattern to define the files that the Hadoop FS Standalone origin
        processes. <span class="ph">You can use either a glob pattern or a regular expression to define
                the file name pattern. </span></p>

        <p class="p">The Hadoop FS Standalone origin <span class="ph">processes files based on the file name pattern mode, file
                name pattern, and specified directory. For example, if you specify a
                    <samp class="ph codeph">/logs/weblog/ </samp>directory, glob mode, and <samp class="ph codeph">*.json</samp>
                as the file name pattern, the origin processes all files with the "json" extension
                in the <samp class="ph codeph">/logs/weblog/</samp> directory.</span></p>

        <p class="p">The origin processes files in order based on the specified read
            order.</p>

        <p class="p">For more information about glob syntax, see <a class="xref" href="https://en.wikipedia.org/wiki/Glob_(programming)#Syntax" target="_blank">https://en.wikipedia.org/wiki/Glob_(programming)#Syntax</a>. For more information about regular expressions, see <a class="xref" href="../Apx-RegEx/RegEx-Title.html#concept_vd4_nsc_gs" title="A regular expression, also known as regex, describes a pattern for a string.">Regular Expressions Overview</a>.</p>

    </div>

</div>
<div class="topic concept nested1" id="concept_ogz_q3h_ldb">
    <h2 class="title topictitle2">Read Order</h2>

    <div class="body conbody">
        <p class="p">The Hadoop FS Standalone origin reads files in
            ascending order based on the timestamp or file name:</p>

        <div class="p">
            <dl class="dl">
                
                    <dt class="dt dlterm">Last Modified Timestamp</dt>

                    <dd class="dd">The Hadoop FS Standalone origin can read files
                        in ascending order based on the last modified timestamp associated with the
                        file. When the origin reads from a secondary location - not the directory
                        where the files are created and written - the last-modified timestamp should
                        be when the file is moved to the directory to be processed.</dd>

                    <dd class="dd">
                        <div class="note tip"><span class="tiptitle">Tip:</span> Avoid moving files using commands that preserve the
                            existing timestamp, such as <samp class="ph codeph">cp -p</samp>. Preserving the
                            existing timestamp can be problematic in some cases, such as moving
                            files across time zones.</div>

                    </dd>

                    <dd class="dd">When ordering based on timestamp, any files with the same timestamp are read
                        in lexicographically ascending order based on the file names.</dd>

                    <dd class="dd">For example, when reading files with the <samp class="ph codeph">log*.json</samp> file
                        name pattern, the origin reads the following files in the following
                        order:</dd>

                    <dd class="dd">
                        <table cellpadding="4" cellspacing="0" summary="" id="concept_ogz_q3h_ldb__simpletable_jmm_1g4_xv" border="0" class="simpletable"><tr class="strow">
                                <td valign="top" class="stentry" width="41.66666666666667%">
                                    <samp class="ph codeph"><span class="ph uicontrol">File Name</span></samp>
                                </td>

                                <td valign="top" class="stentry" width="58.333333333333336%">
                                    <samp class="ph codeph"><span class="ph uicontrol">Last Modified</span></samp>
                                </td>

                            </tr>
<tr class="strow">
                                <td valign="top" class="stentry">
                                    <samp class="ph codeph">log-1.json</samp>
                                </td>

                                <td valign="top" class="stentry">
                                    <samp class="ph codeph">APR 24 2016 14:03:35</samp>
                                </td>

                            </tr>
<tr class="strow">
                                <td valign="top" class="stentry">
                                    <samp class="ph codeph">log-903.json</samp>
                                </td>

                                <td valign="top" class="stentry">
                                    <samp class="ph codeph">APR 24 2016 14:05:03</samp>
                                </td>

                            </tr>
<tr class="strow">
                                <td valign="top" class="stentry">
                                    <samp class="ph codeph">log-2.json</samp>
                                </td>

                                <td valign="top" class="stentry">
                                    <samp class="ph codeph">APR 24 2016 14:45:11</samp>
                                </td>

                            </tr>
<tr class="strow">
                                <td valign="top" class="stentry">
                                    <samp class="ph codeph">log-3.json</samp></td>

                                <td valign="top" class="stentry">
                                    <samp class="ph codeph">APR 24 2016 14:45:11</samp></td>

                            </tr>
</table>

                    </dd>

                    <dd class="dd">Notice, the log-2.json and log-3.json files have identical timestamps, and
                        so are processed in lexicographically ascending order based on their file
                        names.</dd>

                
                
                    <dt class="dt dlterm">Lexicographically Ascending File Names</dt>

                    <dd class="dd">The Hadoop FS Standalone origin can read files in lexicographically
                        ascending order based on file names. Note that lexicographically ascending
                        order reads the numbers 1 through 11 as follows:</dd>

                    <dd class="dd">
                        <pre class="pre codeblock">1, 10, 11, 2, 3, 4... 9</pre>

                    </dd>

                    <dd class="dd">For example, when reading files with the <samp class="ph codeph">web*.log</samp> file name
                        pattern, the Hadoop FS Standalone origin reads the following files in the
                        following
                        order:<pre class="pre codeblock">web-1.log
web-10.log
web-11.log
web-2.log
web-3.log
web-4.log
web-5.log
web-6.log
web-7.log
web-8.log
web-9.log</pre>
</dd>

                    <dd class="dd">To read these files in logical and lexicographically ascending order, you
                        might add leading zeros to the file naming convention as
                        follows:<pre class="pre codeblock">web-0001.log
web-0002.log
web-0003.log
...
web-0009.log
web-0010.log
web-0011.log</pre>
</dd>

                
            </dl>

        </div>

    </div>

</div>
<div class="topic concept nested1" id="concept_fgx_1jh_ldb">
    <h2 class="title topictitle2">Multithreaded Processing</h2>

    
    <div class="body conbody"><p class="shortdesc">The Hadoop FS Standalone origin uses multiple concurrent threads to process data
        based on the Number of Threads property. </p>

        <p class="p">Each thread reads data from a single
            file, and each file can have a maximum of one thread read from it at a time. The file
            read order is based on the configuration for the <a class="xref" href="HDFSStandalone.html#concept_ogz_q3h_ldb">Read Order
                property</a>.</p>

        <p class="p">As the pipeline runs, <span class="ph">each thread connects to the origin system
                        and creates a batch of data, and passes the batch to an available pipeline
                        runner. A pipeline runner is a <dfn class="term">sourceless pipeline instance</dfn> -
                        an instance of the pipeline that includes all of the processors and
                        destinations in the pipeline and performs all pipeline processing after the
                        origin.</span></p>

        <p class="p">
            <span class="ph"> Each pipeline runner processes one batch at a time,
                        just like a pipeline that runs on a single thread. When the flow of data
                        slows, the pipeline runners wait idly until they are needed.</span></p>

        <p class="p"><span class="ph">Multithreaded pipelines preserve the order of
                        records within each batch, just like a single-threaded pipeline. But since
                        batches are processed by different pipeline instances, the order that
                        batches are written to destinations is not ensured.</span></p>

        <p class="p">For example, say you configure the origin to read files from a directory using 5 threads
            and the Last Modified Timestamp read order. When you start the pipeline, <span class="ph">the origin creates five
                              threads, and <span class="ph">Data Collector</span>
                              creates a matching number of pipeline runners.</span>
        </p>

        <p class="p">The Hadoop FS Standalone origin assigns a thread to each of the five oldest files in the
            directory. Each thread processes its assigned file, passing batches of data to the
            origin. <span class="ph">Upon receiving data, the origin passes a batch to
                              each of the pipeline runners for processing.</span>
        </p>

        <p class="p">After each thread completes processing a file, it continues to the next file based on the
            last-modified timestamp, until all files are processed. </p>

        <p class="p">For more information about multithreaded pipelines, see <a class="xref" href="../Multithreaded_Pipelines/MultithreadedPipelines.html#concept_zpp_2xc_py">Multithreaded Pipeline Overview</a>.</p>

    </div>

</div>
<div class="topic concept nested1" id="concept_bwq_nyj_mdb">
    <h2 class="title topictitle2">Reading from Subdirectories</h2>

    
    <div class="body conbody"><p class="shortdesc">When using the Last Modified Timestamp read order, the Hadoop FS Standalone origin
        can read files in subdirectories of the specified file directory. </p>

        <p class="p">When you configure the origin to read from
            subdirectories, it reads files from all subdirectories. It reads files in ascending
            order based on timestamp, regardless of the location of the file within the
            directory.</p>

        <div class="p">For example, you configure the Hadoop FS Standalone origin to read from the /logs/ file
            directory, select the Last Modified Timestamp read order, and enable reading from
            subdirectories. The Hadoop FS Standalone origin reads the following files in the
            following order based on timestamp, even though the files are written to different
            subdirectories. <table cellpadding="4" cellspacing="0" summary="" id="concept_bwq_nyj_mdb__simpletable_jmm_1g4_xv" border="0" class="simpletable"><tr class="strow">
                    <td valign="top" class="stentry" width="33.333333333333336%">
                        <samp class="ph codeph"><span class="ph uicontrol">File Name</span></samp>
                    </td>

                    <td valign="top" class="stentry" width="33.333333333333336%">
                        <samp class="ph codeph"><span class="ph uicontrol">Directory</span></samp>
                    </td>

                    <td valign="top" class="stentry" width="33.333333333333336%">
                        <samp class="ph codeph"><span class="ph uicontrol">Last Modified Timestamp</span></samp>
                    </td>

                </tr>
<tr class="strow">
                    <td valign="top" class="stentry">
                        <samp class="ph codeph">log-1.json</samp>
                    </td>

                    <td valign="top" class="stentry">
                        <samp class="ph codeph">/logs/west/</samp>
                    </td>

                    <td valign="top" class="stentry">
                        <samp class="ph codeph">APR 24 2016 14:03:35</samp>
                    </td>

                </tr>
<tr class="strow">
                    <td valign="top" class="stentry">
                        <samp class="ph codeph">log-0054.json</samp>
                    </td>

                    <td valign="top" class="stentry">
                        <samp class="ph codeph">/logs/east/</samp>
                    </td>

                    <td valign="top" class="stentry">
                        <samp class="ph codeph">APR 24 2016 14:05:03</samp>
                    </td>

                </tr>
<tr class="strow">
                    <td valign="top" class="stentry">
                        <samp class="ph codeph">log-0055.json </samp>
                    </td>

                    <td valign="top" class="stentry">
                        <samp class="ph codeph">/logs/west/</samp>
                    </td>

                    <td valign="top" class="stentry">
                        <samp class="ph codeph">APR 24 2016 14:45:11</samp>
                    </td>

                </tr>
<tr class="strow">
                    <td valign="top" class="stentry">
                        <samp class="ph codeph">log-2.json</samp>
                    </td>

                    <td valign="top" class="stentry">
                        <samp class="ph codeph">/logs/</samp>
                    </td>

                    <td valign="top" class="stentry">
                        <samp class="ph codeph">APR 24 2016 14:45:11</samp>
                    </td>

                </tr>
</table>
</div>

    </div>

<div class="topic concept nested2" id="concept_w3r_qhk_mdb">
    <h3 class="title topictitle3">Post-Processing Subdirectories</h3>

    
    <div class="body conbody"><p class="shortdesc">When the Hadoop FS Standalone origin reads from subdirectories, it uses the
        subdirectory structure when archiving files during post-processing. </p>

        <p class="p">You can archive files when the origin
            completes processing a file or when it cannot fully process a file.</p>

        <div class="p">For example, say you configure the origin to archive processed files to a "processed"
            archive directory. After successfully reading the files in the example above, it writes
            them to the following directories:<table cellpadding="4" cellspacing="0" summary="" id="concept_w3r_qhk_mdb__simpletable_jmm_1g4_xv" border="0" class="simpletable"><tr class="strow">
                    <td valign="top" class="stentry" width="50%">
                        <samp class="ph codeph"><span class="ph uicontrol">File Name</span></samp>
                    </td>

                    <td valign="top" class="stentry" width="50%">
                        <samp class="ph codeph"><span class="ph uicontrol">Archive Directory</span></samp>
                    </td>

                </tr>
<tr class="strow">
                    <td valign="top" class="stentry">
                        <samp class="ph codeph">log-1.json</samp>
                    </td>

                    <td valign="top" class="stentry">
                        <samp class="ph codeph">/processed/logs/west/</samp>
                    </td>

                </tr>
<tr class="strow">
                    <td valign="top" class="stentry">
                        <samp class="ph codeph">log-0054.json</samp>
                    </td>

                    <td valign="top" class="stentry">
                        <samp class="ph codeph">/processed/logs/east/</samp>
                    </td>

                </tr>
<tr class="strow">
                    <td valign="top" class="stentry">
                        <samp class="ph codeph">log-0055.json </samp>
                    </td>

                    <td valign="top" class="stentry">
                        <samp class="ph codeph">/processed/logs/west/</samp>
                    </td>

                </tr>
<tr class="strow">
                    <td valign="top" class="stentry">
                        <samp class="ph codeph">log-2.json</samp>
                    </td>

                    <td valign="top" class="stentry">
                        <samp class="ph codeph">/processed/logs/</samp>
                    </td>

                </tr>
</table>
</div>

    </div>

</div>
</div>
<div class="topic concept nested1" id="concept_a2q_4kh_ldb">
    <h2 class="title topictitle2">First File for Processing</h2>

    
    <div class="body conbody"><p class="shortdesc">Configure a first file for processing when you want Hadoop FS Standalone to ignore
        one or more existing files in the directory.</p>

        <p class="p">When you define a first file to process, the
            Hadoop FS Standalone origin starts processing with the specified file and continues
            based on the read order and file name pattern. When you do not specify a first file, the
            origin processes all files in the directory that match the file name pattern.
                </p>

        <p class="p">For example, say the Hadoop FS Standalone origin reads files based on last-modified
            timestamp. To ignore all files older than a particular file, use that file name as the
            first file to process.</p>

        <p class="p">Similarly, say you have the origin reading files based on lexicographically ascending
            file names, and the file directory includes the following files: web_001.log,
            web_002.log, web_003.log. </p>

        <p class="p">If you configure web_002.log as the first file, the origin reads web_002.log and
            continues to web_003.log. It skips web_001.log. </p>

    </div>

</div>
<div class="topic concept nested1" id="concept_vfk_yrv_ldb">
    <h2 class="title topictitle2">Reading from Azure HDInsight</h2>

    
    <div class="body conbody"><p class="shortdesc">You can use the HDP stage libraries to access Azure blob storage using the WASB
        protocol. This enables the Hadoop FS Standalone origin to read from Azure HDInsight. </p>

        <p class="p">To read from an Azure
            HDInsight cluster, <span class="ph">Data Collector</span> can
            be installed anywhere. It can be installed on a node in the HDInsight cluster or outside
            of the cluster entirely. </p>

        <div class="p">To read from HDInsight, configure the Hadoop FS Standalone origin as follows: <ol class="ol" id="concept_vfk_yrv_ldb__ol_kvn_mnf_sw">
                <li class="li">On the <span class="keyword wintitle">General</span> tab of the Hadoop FS Standalone origin, for
                    the <span class="ph uicontrol">Stage Library</span> property, select the HDP stage library
                    version 2.4 or later. </li>

                <li class="li">Configure Azure credentials in one of the following ways:<ul class="ul" id="concept_vfk_yrv_ldb__ul_hzr_q2g_sw">
                        <li class="li">If the Azure credentials are defined in the HDFS configuration file
                                <samp class="ph codeph">core-site.xml</samp>, configure the origin to access the
                                file.<ol class="ol" type="a" id="concept_vfk_yrv_ldb__ol_sjt_rmd_rx">
                                <li class="li">On the <span class="ph uicontrol">Hadoop FS</span> tab, configure the
                                        <span class="ph uicontrol">Hadoop FS Configuration Directory</span>
                                    property to point to the directory that includes the file. </li>

                            </ol>
</li>

                        <li class="li">If the credentials are not defined in the <samp class="ph codeph">core-site.xml</samp>
                            file, use a Hadoop FS configuration property to pass the Azure
                                credentials:<ol class="ol" type="a" id="concept_vfk_yrv_ldb__ol_yxb_1ht_px">
                                <li class="li">In the origin, on the <span class="keyword wintitle">Hadoop FS</span> tab, click
                                    the <span class="ph uicontrol">Add</span> icon to add a new Hadoop FS
                                    configuration property. <p class="p">You can use <a class="xref" href="../Pipeline_Configuration/SimpleBulkEdit.html#concept_alb_b3y_cbb">simple or bulk edit mode</a> to add configuration
                                        properties.</p>
</li>

                                <li class="li">Enter the following property name, using the Azure storage
                                    account name for &lt;storage account
                                    name&gt;:<pre class="pre codeblock">fs.azure.account.key.&lt;storage account name&gt;.blob.core.windows.net</pre>
For
                                    example, if the storage account name is "sdchd", then enter the
                                    following name for the property:
                                        <pre class="pre codeblock">fs.azure.account.key.sdchd.blob.core.windows.net</pre>
<div class="note tip"><span class="tiptitle">Tip:</span> You can find the Azure storage account name on
                                        the <span class="keyword wintitle">Access Keys</span> page in the Microsoft
                                        Azure portal. To view the page in the Microsoft Azure
                                        portal, click <span class="ph menucascade"><span class="ph uicontrol">All Resources</span> &gt; <span class="ph uicontrol">Storage Account</span> &gt; <span class="ph uicontrol">Access Keys</span></span>. A page like the following appears, with the
                                        storage account name and access keys:</div>
<p class="p"><img class="image" id="concept_vfk_yrv_ldb__image_v3c_j3t_px" src="../Graphics/Azure-AccessKeys.png" height="425" width="602" /></p>
</li>

                                <li class="li">For the value of the Hadoop FS Configuration property, enter an
                                    access key value for the Azure storage account. You can use any
                                    valid key.<div class="p">
                                        <div class="note tip"><span class="tiptitle">Tip:</span> The account key value also displays on the
                                                <span class="keyword wintitle">Access Keys</span> page. For example,
                                            on the image above, you could use either the key1 or
                                            key2 value. </div>

                                    </div>
</li>

                            </ol>
</li>

                    </ul>
</li>

                <li class="li">In the origin, on the <span class="keyword wintitle">Hadoop FS</span> tab, configure the
                        <span class="ph uicontrol">Hadoop FS URI</span> property using the following
                        structure:<pre class="pre codeblock">&lt;wasb[s]&gt;://&lt;container name&gt;@&lt;storage account name&gt;.blob.core.windows.net/&lt;path to files&gt;</pre>
<p class="p">In
                        the URI, &lt;container name&gt; is the Azure container name. And &lt;storage
                        account name&gt; is the same Azure storage account name that you used for the
                        Hadoop FS configuration property. </p>
<div class="p">For example, for a
                            <samp class="ph codeph">sdc-hd</samp> container in a storage account named
                            <samp class="ph codeph">sdchd</samp>, with all files in a "files" directory, you would
                        define the Hadoop FS URI as
                        follows:<pre class="pre codeblock">wasbs://sdc-hd@sdchd.blob.core.windows.net/files</pre>
</div>
<div class="p">
                        <div class="note tip"><span class="tiptitle">Tip:</span> You can find the container name and storage account name on
                            the <span class="keyword wintitle">Essentials</span> page in the Microsoft Azure portal.
                            For a standard storage account, in the Microsoft Azure portal, click <span class="ph menucascade"><span class="ph uicontrol">All Resources</span> &gt; <span class="ph uicontrol">Storage Account</span> &gt; <span class="ph uicontrol">Overview</span> &gt; <span class="ph uicontrol">Blobs</span></span>. For a blob storage account, click <span class="ph menucascade"><span class="ph uicontrol">All Resources</span> &gt; <span class="ph uicontrol">Storage Account</span> &gt; <span class="ph uicontrol">Overview</span></span>. </div>

                    </div>
<p class="p">A page like the following displays with the container name and storage
                        account name:</p>
<p class="p"><img class="image" id="concept_vfk_yrv_ldb__image_r2s_snt_px" src="../Graphics/Azure-BlobService.png" height="374" width="498" /></p>
Though the host name for the
                    Hadoop FS URI is <samp class="ph codeph">&lt;storage account
                        name&gt;.blob.core.windows.net</samp>, you can alternatively use the host
                    name of the Azure blob service endpoint as the hostname for the Hadoop FS
                    URI.</li>

            </ol>
</div>

        <div class="section" id="concept_vfk_yrv_ldb__section_ond_csv_ldb"><h3 class="title sectiontitle">Example</h3>
            
            <p class="p">The following image shows how to configure the Hadoop FS Standalone origin to read
                from HDInsight using the Azure account information in the examples above:</p>

            <p class="p"><img class="image" id="concept_vfk_yrv_ldb__image_ufb_rtv_ldb" src="../Graphics/AzureHDInsight-HDFSStandalone.png" height="223" width="666" /></p>

        </div>

    </div>

</div>
<div class="topic concept nested1" id="concept_asp_lfh_ldb">
    <h2 class="title topictitle2">Record Header Attributes</h2>

    
    <div class="body conbody"><p class="shortdesc">The Hadoop FS Standalone origin <span class="ph">creates record header
                attributes that include <span class="ph" id="d52767e19">information about the originating file for
                    the record</span>.</span>
    </p>

        <p class="p">When the origin <span class="ph">processes Avro data, it includes the Avro schema in
                        an avroSchema record header attribute.</span></p>

        <p class="p"><span class="ph" id="concept_asp_lfh_ldb__d48578e20">You can use the record:attribute or
                record:attributeOrDefault functions to access the information in the attributes. For
                more information about working with record header attributes, see <a class="xref" href="../Pipeline_Design/RecordHeaderAttributes.html#concept_rd2_ghz_dz">Working with Header Attributes</a>.</span></p>

        <div class="p">The Hadoop FS Standalone origin <span class="ph">creates the following record header
                attributes:</span><ul class="ul" id="concept_asp_lfh_ldb__ul_nsd_xfb_j1b">
                <li class="li">avroSchema - When processing Avro data, provides the
                              Avro schema.</li>

                <li class="li">baseDir - Base directory containing the file where the record originated.</li>

            </ul>
<ul class="ul" id="concept_asp_lfh_ldb__ul_wqg_nk1_2z">
                <li class="li" id="concept_asp_lfh_ldb__d52767e31">filename - Provides the name of the file where the record
                    originated.</li>

                <li class="li" id="concept_asp_lfh_ldb__d52767e34">file - Provides the file path and file name where the record
                    originated.</li>

                <li class="li" id="concept_asp_lfh_ldb__d52767e37">mtime - Provides the last-modified time for the file.</li>

                <li class="li" id="concept_asp_lfh_ldb__d52767e40">offset - Provides the file offset in bytes. The file offset is
                    the location in the file where the record originated.</li>

            </ul>
<ul class="ul" id="concept_asp_lfh_ldb__ul_ebw_1hh_ldb">
                <li class="li">atime - Provides the last accessed time.</li>

                <li class="li">isDirectory - Indicates if the file is a directory.</li>

                <li class="li">isSymbolicLink - Indicates if the file is a symbolic link.</li>

                <li class="li">size - Provides the file size.</li>

                <li class="li">owner - Provides the file owner.</li>

                <li class="li">group - Provides the group associated with the file.</li>

                <li class="li">blocksize - Provides the block size of the file.</li>

                <li class="li">replication - Provides the replication of the file. </li>

                <li class="li">isEncrypted - Indicates if the file is encrypted.</li>

            </ul>
</div>

    </div>

</div>
<div class="topic concept nested1" id="concept_zlx_5rh_ldb">
    <h2 class="title topictitle2">Event Generation</h2>

    <div class="body conbody">
        <p class="p">The Hadoop FS Standalone
            origin <span class="ph">can generate events that you can use in an event stream. When
                        you enable event generation, the origin generates event records each time
                        the origin starts or completes reading a file.</span> It can also generate events when it completes processing all available data and the
            configured batch wait time has elapsed.</p>

        <div class="p">Hadoop FS Standalone events can be used in any logical way. For example: <ul class="ul" id="concept_zlx_5rh_ldb__ul_vgr_wrh_ldb">
                <li class="li">With the Pipeline Finisher executor to
                              stop the pipeline and transition the pipeline to a Finished state when
                              the origin completes processing available data.<p class="p">When you restart a
                                    pipeline stopped by the Pipeline Finisher executor, the origin
                                    continues processing from the last-saved offset unless you reset
                                    the origin.</p>
<p class="p">For an example, see <a class="xref" href="../Event_Handling/EventFramework-Title.html#concept_kff_ykv_lz">Case Study: Stop the Pipeline</a>.</p>
</li>

                <li class="li">With the Email executor to send a custom email
                              after receiving an event.<p class="p">For an example, see <a class="xref" href="../Event_Handling/EventFramework-Title.html#concept_t2t_lp5_xz">Case Study: Sending Email</a>.</p>
</li>

            </ul>
<ul class="ul" id="concept_zlx_5rh_ldb__ul_xcz_dx4_vz">
                <li class="li">With a destination to store event information.
                                    <p class="p">For an example, see <a class="xref" href="../Event_Handling/EventFramework-Title.html#concept_ocb_nnl_px">Case Study: Event Storage</a>.</p>
</li>

            </ul>
</div>

        <p class="p"><span class="ph">For more information about dataflow
                        triggers and the event framework, see <a class="xref" href="../Event_Handling/EventFramework-Title.html#concept_cph_5h4_lx">Dataflow Triggers Overview</a>.</span></p>

    </div>

<div class="topic concept nested2" id="concept_p5x_2d3_ldb">
    <h3 class="title topictitle3">Event Records</h3>

    <div class="body conbody">
        <div class="p">Event records generated by the Hadoop
            FS Standalone origin have the following event-related record header attributes. Record
            header attributes are stored as String values:
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="concept_p5x_2d3_ldb__table_brz_3gp_qx" class="table" frame="border" border="1" rules="all">
                    
                    
                    <thead class="thead" align="left">
                        <tr>
                            <th class="entry" valign="top" width="30%" id="d278575e1055">Record Header Attribute</th>

                            <th class="entry" valign="top" width="70%" id="d278575e1058">Description</th>

                        </tr>

                    </thead>

                    <tbody class="tbody">
                        <tr>
                            <td class="entry" valign="top" width="30%" headers="d278575e1055 ">sdc.event.type</td>

                            <td class="entry" valign="top" width="70%" headers="d278575e1058 ">Event type. Uses one of the following types:<ul class="ul" id="concept_p5x_2d3_ldb__ul_m12_mgp_qx">
                                    <li class="li">new-file - Generated when the origin starts processing a new
                                        file. </li>

                                    <li class="li">finished-file - Generated when the origin completes
                                        processing a file.</li>

                                    <li class="li">no-more-data - Generated after the origin completes
                                        processing all available files and the number of seconds
                                        configured for Batch Wait Time has elapsed.</li>

                                </ul>
</td>

                        </tr>

                        <tr>
       <td class="entry" valign="top" width="30%" headers="d278575e1055 ">sdc.event.version</td>

       <td class="entry" valign="top" width="70%" headers="d278575e1058 ">An integer that indicates the version of the event record type.</td>

      </tr>

                        <tr>
       <td class="entry" valign="top" width="30%" headers="d278575e1055 ">sdc.event.creation_timestamp</td>

       <td class="entry" id="concept_p5x_2d3_ldb__d47442e2127" valign="top" width="70%" headers="d278575e1058 ">Epoch timestamp when the stage created the event.
       </td>

      </tr>

                    </tbody>

                </table>
</div>
</div>

        <p class="p">The Hadoop FS Standalone origin can generate the following types of event records: </p>

        <dl class="dl">
            
                <dt class="dt dlterm">new-file</dt>

                <dd class="dd">The Hadoop FS Standalone origin generates a new-file event record when it starts
                    processing a new file. </dd>

                <dd class="dd">New-file event records have the sdc.event.type set to new-file and include the
                    following field:
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="concept_p5x_2d3_ldb__table_vmc_l42_px" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr>
                                    <th class="entry" valign="top" width="30%" id="d278575e1133">Event Record Field</th>

                                    <th class="entry" valign="top" width="70%" id="d278575e1136">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d278575e1133 ">filepath</td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e1136 ">Path and name of the file that the origin started or
                                        finished processing.</td>

                                </tr>

                            </tbody>

                        </table>
</div>
</dd>

            
            
                <dt class="dt dlterm">finished-file</dt>

                <dd class="dd">The Hadoop FS Standalone origin generates a finished-file event record when it
                    finishes processing a file.</dd>

                <dd class="dd">Finished-file event records have the sdc.event.type set to finished-file and
                    include the following fields:
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="concept_p5x_2d3_ldb__table_msq_qzn_j1b" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr>
                                    <th class="entry" valign="top" width="30%" id="d278575e1178">Event Record Field</th>

                                    <th class="entry" valign="top" width="70%" id="d278575e1181">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d278575e1178 ">filepath</td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e1181 ">Path and name of the file that the origin started or
                                        finished processing.</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d278575e1178 ">record-count</td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e1181 ">Number of records successfully generated from the
                                        file.</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d278575e1178 ">error-count</td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e1181 ">Number of error records generated from the file. </td>

                                </tr>

                            </tbody>

                        </table>
</div>
</dd>

            
            
                <dt class="dt dlterm">no-more-data</dt>

                <dd class="dd">The Hadoop FS Standalone origin generates a no-more-data event record when the
                    origin completes processing all available records and the number of seconds
                    configured for Batch Wait Time elapses without any new files appearing to be
                    processed. </dd>

                <dd class="dd">No-more-data event records generated by the Hadoop FS Standalone origin have the
                    sdc.event.type set to no-more-data and include the following fields:
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="concept_p5x_2d3_ldb__table_fcv_xx4_j1b" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr>
                                    <th class="entry" valign="top" width="30%" id="d278575e1241">Event Record Field</th>

                                    <th class="entry" valign="top" width="70%" id="d278575e1244">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr>
       <td class="entry" valign="top" width="30%" headers="d278575e1241 ">record-count</td>

       <td class="entry" valign="top" width="70%" headers="d278575e1244 ">Number of records successfully generated since the pipeline started or since the last
        no-more-data event was created. </td>

      </tr>

                                <tr>
       <td class="entry" valign="top" width="30%" headers="d278575e1241 ">error-count</td>

       <td class="entry" valign="top" width="70%" headers="d278575e1244 ">Number of error records generated since the pipeline started or since the last
        no-more-data event was created.</td>

      </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d278575e1241 ">file-count</td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e1244 ">Number of files the origin attempted to process. Can
                                        include files that were unable to be processed or were not
                                        fully processed.</td>

                                </tr>

                            </tbody>

                        </table>
</div>
</dd>

            
        </dl>

    </div>

</div>
</div>
<div class="topic concept nested1" id="concept_jbn_md3_ldb">
    <h2 class="title topictitle2">Buffer Limit and Error Handling</h2>

    
    <div class="body conbody"><p class="shortdesc">The Hadoop FS Standalone origin <span class="ph">passes each record to a buffer. The size of the buffer determines
                the maximum size of the record that can be processed. Decrease the buffer limit when
                memory on the <span class="ph">Data Collector</span>
                machine is limited. Increase the buffer limit to process larger records when memory
                is available.</span></p>

        <div class="p">When a record is larger than the specified limit, the
            Hadoop FS Standalone origin <span class="ph">processes the source file based on the stage error
            handling:</span><dl class="dl">
                
                    <dt class="dt dlterm">Discard</dt>

                    <dd class="dd">The origin discards the record and all remaining records in the file, and
                        then continues processing the next file.</dd>

                
                
                    <dt class="dt dlterm">Send to Error </dt>

                    <dd class="dd">With a buffer limit error, the origin cannot send the record to the pipeline
                        for error handling because it is unable to fully process the record.
                            <p class="p">Instead, the origin creates a message stating that a buffer overrun
                            error occurred. The message includes the file and offset where the
                            buffer overrun error occurred. The information displays in the pipeline
                                history<span class="ph"> and displays as an alert when you monitor
                                the pipeline</span>. </p>
<p class="p">If an error directory is configured for
                            the stage, the origin moves the file to the error directory and
                            continues processing the next file. </p>
</dd>

                
                
                    <dt class="dt dlterm">Stop Pipeline</dt>

                    <dd class="dd">The origin stops the pipeline and creates a message stating that a buffer
                        overrun error occurred. The message includes the file and offset where the
                        buffer overrun error occurred. The information displays <span class="ph">as
                            an alert and </span>in the pipeline history. </dd>

                    <dd class="dd">
                        <div class="note note" id="concept_jbn_md3_ldb__d51603e112"><span class="notetitle">Note:</span> You can also check the <span class="ph">Data Collector</span> log file for error details. </div>

                    </dd>

                
            </dl>
</div>

    </div>

</div>
<div class="topic concept nested1" id="concept_vvd_4zp_ldb">
    <h2 class="title topictitle2">Kerberos Authentication</h2>

    <div class="body conbody">
        <p class="p"><span class="ph">You can use Kerberos authentication to connect to HDFS. When you
                use Kerberos authentication, <span class="ph">Data Collector</span>
                uses the Kerberos principal and keytab to connect to HDFS. By default, <span class="ph">Data Collector</span>
                uses the user account who started it to connect.</span></p>

        <p class="p"><span class="ph">The Kerberos principal and keytab are defined in the <span class="ph">Data Collector</span>
                configuration file, <samp class="ph codeph">$SDC_CONF/sdc.properties</samp>. To use Kerberos
                authentication, configure all Kerberos properties in the <span class="ph">Data Collector</span>
                configuration file, and then enable Kerberos in the</span> Hadoop FS Standalone origin.</p>

        <p class="p"><span class="ph">For more information about enabling Kerberos authentication
                        for <span class="ph">Data Collector</span>, see <a class="xref" href="../Configuration/DCConfig.html#concept_hnm_n4l_xs" title="You can use Kerberos authentication to connect to external systems as well as YARN clusters.">Kerberos Authentication</a>.</span></p>

    </div>

</div>
<div class="topic concept nested1" id="concept_zx5_yzp_ldb">
    <h2 class="title topictitle2">HDFS Properties and Configuration Files</h2>

    <div class="body conbody">
        <div class="p">You can configure the Hadoop FS
            Standalone origin to use individual HDFS properties or HDFS configuration
                files:<dl class="dl">
                
                    <dt class="dt dlterm">HDFS configuration files</dt>

                    <dd class="dd">You can use the following HDFS configuration files with the Hadoop FS
                        Standalone origin:<ul class="ul" id="concept_zx5_yzp_ldb__ul_qhn_ytr_bt">
                        <li class="li">core-site.xml</li>

                        <li class="li">hdfs-site.xml </li>

                  </ul>
</dd>

                    <dd class="dd">To use HDFS configuration files: <ol class="ol" id="concept_zx5_yzp_ldb__ol_rb2_2nr_bt">
                            <li class="li">Store the files or a symlink to the files in the <span class="ph">Data Collector</span> resources directory. </li>

                            <li class="li">In the Hadoop FS Standalone origin, configure the Hadoop FS
                                Configuration Directory property to specify the location of the
                                files. </li>

                        </ol>
<div class="note note"><span class="notetitle">Note:</span>  For a Cloudera Manager installation, Data Collector
                            automatically creates a symlink to the files named
                                <samp class="ph codeph">hadoop-conf</samp>. Enter <samp class="ph codeph">hadoop-conf</samp> for
                            the location of the files in the Hadoop FS Standalone
                        origin.</div>
</dd>

                
                
                    <dt class="dt dlterm">Individual properties</dt>

                    <dd class="dd">You can configure individual HDFS properties in the origin. To add an HDFS
                        property, you specify the exact property name and the value. The Hadoop FS
                        Standalone origin does not validate the property names or
                            values.<div class="note note"><span class="notetitle">Note:</span> Individual properties override properties defined in the
                            HDFS configuration file. </div>
</dd>

                
            </dl>
</div>

    </div>

</div>
<div class="topic concept nested1" id="concept_ub1_zwp_ldb">
    <h2 class="title topictitle2">HDFS User</h2>

    <div class="body conbody">
        <p class="p"><span class="ph"><span class="ph">Data Collector</span>
                        can either use the currently logged in <span class="ph">Data Collector</span> user or a
                        user configured in the</span> Hadoop FS Standalone origin to read from HDFS. </p>

        <p class="p">A <span class="ph">Data Collector</span> configuration property can be set that requires using the currently logged in
                        <span class="ph">Data Collector</span> user. When
                  this property is not set, you can specify a user in the origin. For more
                  information about Hadoop impersonation and the Data Collector property, see <a class="xref" href="../Configuration/DCConfig.html#concept_pmr_sy5_nz" title="When Data Collector is registered with Control Hub, you can configure Data Collector to use an abbreviated version of the Control Hub user ID to impersonate a Hadoop user.">Hadoop Impersonation Mode</a>. </p>

        <p class="p">Note that the origin <span class="ph">uses a different user account to
                        connect to HDFS. <span class="ph" id="concept_ub1_zwp_ldb__d47352e3612">By default, <span class="ph">Data Collector</span> uses
                              the user account who started it to connect to external systems. When
                              using Kerberos, <span class="ph">Data Collector</span> uses
                              the Kerberos principal.</span>
                  </span></p>

        <div class="p">To configure a user in the Hadoop FS Standalone origin to read from HDFS, perform the
            following tasks:<ol class="ol" id="concept_ub1_zwp_ldb__ul_mb1_xpt_ls">
                <li class="li">On Hadoop, configure the user as a proxy user and
                              authorize the user to impersonate a Hadoop user. <p class="p">For more
                                    information, see the Hadoop documentation. </p>
</li>

                <li class="li">In the Hadoop FS Standalone origin, on the <span class="keyword wintitle">Hadoop FS</span> tab,
                    configure the <span class="ph uicontrol">HDFS User</span> property.</li>

            </ol>
</div>

    </div>

</div>
<div class="topic concept nested1" id="concept_vym_zd3_ldb">
    <h2 class="title topictitle2">Data Formats</h2>

    <div class="body conbody">
        <div class="p">The Hadoop FS Standalone origin processes data
            differently based on the data format. The origin processes the following types of data:<dl class="dl">
                
                              <dt class="dt dlterm">Avro</dt>

                              <dd class="dd">Generates a record for every Avro record. Includes a "precision"
                                    and "scale" field attribute for each Decimal field. For more
                                    information about field attributes, see <a class="xref" href="../Pipeline_Design/FieldAttributes.html#concept_xfm_wtp_1z" title="Field attributes are attributes that provide additional information about each field that you can use in pipeline logic, as needed.">Field Attributes</a>.</dd>

                              <dd class="dd">The origin writes the Avro schema to an avroSchema record header
                                    attribute. For more information about record header attributes,
                                    see <a class="xref" href="../Pipeline_Design/RecordHeaderAttributes.html#concept_wn2_jcz_dz">Record Header Attributes</a>. </dd>

                              <dd class="dd">The origin expects each file to contain the Avro schema and uses
                                    the schema to process the Avro data.</dd>

                              <dd class="dd">The origin reads files compressed by Avro-supported compression
                                    codecs without requiring additional configuration. </dd>

                        
                
                              <dt class="dt dlterm">Delimited</dt>

                              <dd class="dd">Generates a record for each delimited line. You can use the
                                    following delimited format types:<ul class="ul" id="concept_vym_zd3_ldb__ul_evg_nsl_mcb">
                        <li class="li"><span class="ph uicontrol">Default CSV</span> - File that includes comma-separated
                              values. Ignores empty lines in the file.</li>

                        <li class="li"><span class="ph uicontrol">RFC4180 CSV</span> - Comma-separated file that strictly
                              follows RFC4180 guidelines.</li>

                        <li class="li"><span class="ph uicontrol">MS Excel CSV</span> - Microsoft Excel comma-separated
                              file.</li>

                        <li class="li"><span class="ph uicontrol">MySQL CSV</span> - MySQL comma-separated file.</li>

                        <li class="li"><span class="ph uicontrol">Postgres CSV</span> - Postgres comma-separated
                              file.</li>

                        <li class="li"><span class="ph uicontrol">Postgres Text</span> - Postgres text file.</li>

                        <li class="li"><span class="ph uicontrol">Tab-Separated Values</span> - File that includes
                              tab-separated values.</li>

                        <li class="li"><span class="ph uicontrol">Custom</span> - File that uses user-defined delimiter,
                              escape, and quote characters.</li>

                  </ul>
</dd>

                              <dd class="dd">You can use a list or list-map root field type for delimited data,
                                    optionally including the header information when available. For
                                    more information about the root field types, see <a class="xref" href="../Data_Formats/DelimitedDataRootFieldTypes.html#concept_zcg_bm4_fs">Delimited Data Root Field Type</a>.</dd>

                              <dd class="dd">When using a header line, you can allow processing records with
                                    additional columns. The additional columns are named using a
                                    custom prefix and integers in sequential increasing order, such
                                    as _extra_1, _extra_2. When you disallow additional columns when
                                    using a header line, records that include additional columns are
                                    sent to error.</dd>

                              <dd class="dd">You can also replace a string constant with null values.</dd>

                              <dd class="dd">When a record exceeds the user-defined maximum record length, the
                                    origin cannot continue processing data in the file. Records
                                    already processed from the file are passed to the pipeline. The
                                    behavior of the origin is then based on the error handling
                                    configured for the stage:<ul class="ul" id="concept_vym_zd3_ldb__d47352e1247">
                                          <li class="li">Discard - The origin continues processing with the
                                                next file, leaving the partially-processed file in
                                                the directory. </li>

                                          <li class="li">To Error - The origin continues processing with the
                                                next file. If a post-processing error directory is
                                                configured for the stage, the origin moves the
                                                partially-processed file to the error directory.
                                                Otherwise, it leaves the file in the directory.</li>

                                          <li class="li">Stop Pipeline - The origin stops the pipeline. </li>

                                    </ul>
</dd>

                        
                
                              <dt class="dt dlterm">JSON</dt>

                              <dd class="dd">Generates a record for each JSON object. You can process JSON
                                    files that include multiple JSON objects or a single JSON
                                    array.</dd>

                              <dd class="dd">When an object exceeds the maximum object length defined for the
                                    origin, the origin cannot continue processing data in the file.
                                    Records already processed from the file are passed to the
                                    pipeline. The behavior of the origin is then based on the error
                                    handling configured for the stage:<ul class="ul" id="concept_vym_zd3_ldb__d47352e1282">
                                          <li class="li">Discard - The origin continues processing with the
                                                next file, leaving the partially-processed file in
                                                the directory. </li>

                                          <li class="li">To Error - The origin continues processing with the
                                                next file. If a post-processing error directory is
                                                configured for the stage, the origin moves the
                                                partially-processed file to the error directory.
                                                Otherwise, it leaves the file in the directory.</li>

                                          <li class="li">Stop Pipeline - The origin stops the pipeline. </li>

                                    </ul>
</dd>

                        
                
                              <dt class="dt dlterm">Log</dt>

                              <dd class="dd">Generates a record for every log line. </dd>

                              <dd class="dd">When a line exceeds the user-defined maximum line length, the
                                    origin truncates longer lines. </dd>

                              <dd class="dd">You can include the processed log line as a field in the record.
                                    If the log line is truncated, and you request the log line in
                                    the record, the origin includes the truncated line.</dd>

                              <dd class="dd">You can define the <a class="xref" href="../Data_Formats/LogFormats.html#concept_tr1_spd_sr" title="When you use an origin to read log data, you define the format of the log files to be read.">log format</a> or type to be read.</dd>

                        
                
                        <dt class="dt dlterm">Protobuf</dt>

                        <dd class="dd">Generates a record for every protobuf message. </dd>

                        <dd class="dd">Protobuf messages must match the specified message type and be described
                              in the descriptor file. </dd>

                        <dd class="dd">When the data for a record exceeds 1 MB, the origin cannot continue
                              processing data in the file. The origin handles the file based on file
                              error handling properties and continues reading the next file. </dd>

                        <dd class="dd">For information about generating the descriptor file, see <a class="xref" href="../Data_Formats/Protobuf-Prerequisites.html" title="Perform the following prerequisites before reading or writing protobuf data.">Protobuf Data Format Prerequisites</a>.</dd>

                  
                
                              <dt class="dt dlterm">SDC Record</dt>

                              <dd class="dd">Generates a record for every record. Use to process records
                                    generated by a <span class="ph">Data Collector</span>
                                    pipeline using the SDC Record data format.</dd>

                              <dd class="dd">For error records, the origin provides the original record as read
                                    from the origin in the original pipeline, as well as error
                                    information that you can use to correct the record. </dd>

                              <dd class="dd">When processing error records, the origin expects the error file
                                    names and contents as generated by the original pipeline.</dd>

                        
                
                              <dt class="dt dlterm">Text</dt>

                              <dd class="dd">Generates a record for each line of text or for each section of
                                    text based on a custom delimiter.</dd>

                              <dd class="dd">When a line or section exceeds the maximum line length defined for
                                    the origin, the origin truncates it. The origin adds a boolean
                                    field named Truncated to indicate if the line was
                                    truncated.</dd>

                              <dd class="dd">For more information about processing text with a custom
                                    delimiter, see <a class="xref" href="../Data_Formats/TextCDelim.html#concept_lg2_gcg_jx">Text Data Format with Custom Delimiters</a>.</dd>

                        
                
                              <dt class="dt dlterm">Whole File</dt>

                              <dd class="dd">Streams whole files from the origin system to the destination
                                    system. You can specify a transfer rate or use all available
                                    resources to perform the transfer. </dd>

                              <dd class="dd">The origin generates two fields: one for a file reference and one
                                    for file information. For more information, see <a class="xref" href="../Data_Formats/WholeFile.html#concept_nfc_qkh_xw">Whole File Data Format</a>.</dd>

                        
                
                              <dt class="dt dlterm">XML</dt>

                              <dd class="dd">Generates records based on a user-defined delimiter element. Use
                                    an XML element directly under the root element or define a
                                    simplified XPath expression. If you do not define a delimiter
                                    element, the origin treats the XML file as a single record. </dd>

                              <dd class="dd">Generated records include XML attributes and namespace
                                    declarations as fields in the record by default. You can
                                    configure the stage to include them in the record as field
                                    attributes. </dd>

                              <dd class="dd">You can include XPath information for each parsed XML element and
                                    XML attribute in field attributes. This also places each
                                    namespace in an xmlns record header attribute. </dd>

                              <dd class="dd">
                                    <div class="note note"><span class="notetitle">Note:</span> <span class="ph">Field attributes and record header attributes are
                        written to destination systems automatically only when you use the SDC RPC
                        data format in destinations. For more information about working with field
                        attributes and record header attributes, and how to include them in records,
                        see <a class="xref" href="../Pipeline_Design/FieldAttributes.html#concept_xfm_wtp_1z" title="Field attributes are attributes that provide additional information about each field that you can use in pipeline logic, as needed.">Field Attributes</a> and <a class="xref" href="../Pipeline_Design/RecordHeaderAttributes.html#concept_wn2_jcz_dz">Record Header Attributes</a>.</span></div>

                              </dd>

                              <dd class="dd">When a record exceeds the user-defined maximum record length, the
                                    origin cannot continue processing data in the file. Records
                                    already processed from the file are passed to the pipeline. The
                                    behavior of the origin is then based on the error handling
                                    configured for the stage:<ul class="ul" id="concept_vym_zd3_ldb__d47352e1563">
                                          <li class="li">Discard - The origin continues processing with the
                                                next file, leaving the partially-processed file in
                                                the directory. </li>

                                          <li class="li">To Error - The origin continues processing with the
                                                next file. If a post-processing error directory is
                                                configured for the stage, the origin moves the
                                                partially-processed file to the error directory.
                                                Otherwise, it leaves the file in the directory.</li>

                                          <li class="li">Stop Pipeline - The origin stops the pipeline. </li>

                                    </ul>
</dd>

                              <dd class="dd">Use the XML data format to process valid XML documents. For more
                                    information about XML processing, see <a class="xref" href="../Data_Formats/XMLDFormat.html#concept_lty_42b_dy">Reading and Processing XML Data</a>. <div class="note tip"><span class="tiptitle">Tip:</span> <span class="ph" id="concept_vym_zd3_ldb__d47352e1580">If
                                                you want to process invalid XML documents, you can
                                                try using the text data format with custom
                                                delimiters. For more information, see <a class="xref" href="../Data_Formats/TextCDelim.html#concept_okt_kmg_jx">Processing XML Data with Custom Delimiters</a>.</span>
                                    </div>
</dd>

                        
            </dl>
</div>

    </div>

</div>
<div class="topic task nested1" id="task_l3t_sdm_hdb">
    <h2 class="title topictitle2">Configuring a Hadoop FS Standalone Origin</h2>

    <div class="body taskbody">
        <div class="section context" id="task_l3t_sdm_hdb__context_wzt_zn3_ldb">Configure a Hadoop FS
            Standalone origin to read files in HDFS.</div>

        <ol class="ol steps" id="task_l3t_sdm_hdb__steps_tvn_b5v_yq"><li class="li step stepexpand">
                <span class="ph cmd">In the Properties panel, on the <span class="keyword wintitle">General</span> tab, configure the
                    following properties:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_l3t_sdm_hdb__table_ac1_hss_5x" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr>
                                    <th class="entry" valign="top" width="30%" id="d278575e1951">General Property</th>

                                    <th class="entry" valign="top" width="70%" id="d278575e1954">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr>
       <td class="entry" valign="top" width="30%" headers="d278575e1951 ">Name</td>

       <td class="entry" valign="top" width="70%" headers="d278575e1954 ">Stage name.</td>

      </tr>

                                <tr>
       <td class="entry" valign="top" width="30%" headers="d278575e1951 ">Description</td>

       <td class="entry" valign="top" width="70%" headers="d278575e1954 ">Optional description.</td>

      </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d278575e1951 ">Produce Events <a class="xref" href="HDFSStandalone.html#concept_zlx_5rh_ldb">
                                            <img class="image" id="task_l3t_sdm_hdb__image_plp_tp2_px" src="../Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e1954 ">Generates event records when events occur. Use for event
        handling. <a class="xref" href="../Event_Handling/EventFramework-Title.html#concept_cph_5h4_lx">
         <img class="image" id="task_l3t_sdm_hdb__d47442e798" src="../../../reusable-content/datacollector/../../datacollector/UserGuide/Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                </tr>

                                <tr>
       <td class="entry" valign="top" width="30%" headers="d278575e1951 ">On Record Error <a class="xref" href="../Pipeline_Design/ErrorHandling.html#concept_atr_j4y_5r">
         <img class="image" id="task_l3t_sdm_hdb__d47442e807" src="../../../reusable-content/datacollector/../../datacollector/UserGuide/Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

       <td class="entry" valign="top" width="70%" headers="d278575e1954 ">Error record handling for the stage: <ul class="ul" id="task_l3t_sdm_hdb__d47442e811">
         <li class="li">Discard - Discards the record.</li>

         <li class="li">Send to Error - Sends the record to the pipeline for error handling.</li>

         <li class="li">Stop Pipeline - Stops the pipeline. </li>

        </ul>
</td>

      </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">On the <span class="keyword wintitle">Hadoop FS</span> tab, configure the following
                    properties:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_l3t_sdm_hdb__table_rst_t4d_br" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr>
                                    <th class="entry" valign="top" width="33.33333333333333%" id="d278575e2047">Hadoop FS Property</th>

                                    <th class="entry" valign="top" width="66.66666666666666%" id="d278575e2050">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr>
                            <td class="entry" valign="top" width="33.33333333333333%" headers="d278575e2047 ">Hadoop FS URI</td>

                            <td class="entry" valign="top" width="66.66666666666666%" headers="d278575e2050 ">HDFS URI.</td>

                        </tr>

                                <tr>
                                    <td class="entry" valign="top" width="33.33333333333333%" headers="d278575e2047 ">HDFS User <a class="xref" href="HDFSStandalone.html#concept_ub1_zwp_ldb">
                                            <img class="image" id="task_l3t_sdm_hdb__image_byg_yqg_xs" src="../Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                    <td class="entry" valign="top" width="66.66666666666666%" headers="d278575e2050 ">The HDFS user to use to access HDFS. When
                                using this property, make sure HDFS is configured
                                    appropriately.<p class="p">When not configured, the pipeline uses the
                                    currently logged in <span class="ph">Data Collector</span> user. </p>
<p class="p">Not configurable when <span class="ph">Data Collector</span> is configured to use the currently logged in <span class="ph">Data Collector</span> user. <span class="ph">For more information, see <a class="xref" href="../Configuration/DCConfig.html#concept_pmr_sy5_nz" title="When Data Collector is registered with Control Hub, you can configure Data Collector to use an abbreviated version of the Control Hub user ID to impersonate a Hadoop user.">Hadoop Impersonation Mode</a>.</span></p>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="33.33333333333333%" headers="d278575e2047 ">Kerberos Authentication <a class="xref" href="HDFSStandalone.html#concept_vvd_4zp_ldb">
                                            <img class="image" id="task_l3t_sdm_hdb__image_a5x_jzn_vs" src="../Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                    <td class="entry" valign="top" width="66.66666666666666%" headers="d278575e2050 ">Uses Kerberos credentials to connect to HDFS.
                                    <p class="p">When selected, uses the Kerberos principal and keytab defined
                                    in the <span class="ph">Data Collector</span> configuration file,
                                    <samp class="ph codeph">$SDC_CONF/sdc.properties</samp>. </p>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="33.33333333333333%" headers="d278575e2047 ">Hadoop FS Configuration Directory <a class="xref" href="HDFSStandalone.html#concept_zx5_yzp_ldb">
                                            <img class="image" id="task_l3t_sdm_hdb__image_br4_fgs_5r" src="../Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                    <td class="entry" valign="top" width="66.66666666666666%" headers="d278575e2050 ">Location of the HDFS configuration
                                    files.<p class="p">For a Cloudera Manager installation, enter
                                        <samp class="ph codeph">hadoop-conf</samp>. For all other installations,
                                    use a directory or symlink within the <span class="ph">Data Collector</span> resources directory.</p>
<div class="p">You can use the following files
                                    with the Hadoop FS destination:<ul class="ul" id="task_l3t_sdm_hdb__ul_qnc_jtt_bt">
                        <li class="li">core-site.xml</li>

                        <li class="li">hdfs-site.xml </li>

                  </ul>
</div>
<div class="note note"><span class="notetitle">Note:</span> Properties in the configuration files are
                                    overridden by individual properties defined in the
                                stage.</div>
</td>

                                </tr>

                                <tr>
                            <td class="entry" valign="top" width="33.33333333333333%" headers="d278575e2047 ">Hadoop FS Configuration</td>

                            <td class="entry" valign="top" width="66.66666666666666%" headers="d278575e2050 ">Additional HDFS properties to use. <p class="p">To add properties, click
                                        <span class="ph uicontrol">Add</span> and define the property name and
                                    value. Use the property names and values as expected by
                                    HDFS.</p>
</td>

                        </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">On the <span class="ph uicontrol">Files</span> tab, configure the following
                    properties:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_l3t_sdm_hdb__table_tm4_vck_5q" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr>
                                    <th class="entry" valign="top" width="27.77777777777778%" id="d278575e2208">File Property</th>

                                    <th class="entry" valign="top" width="72.22222222222221%" id="d278575e2211">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr>
                                    <td class="entry" valign="top" width="27.77777777777778%" headers="d278575e2208 ">File Directory</td>

                                    <td class="entry" valign="top" width="72.22222222222221%" headers="d278575e2211 ">The HDFS directory where source files are stored. Enter
                                        an absolute path.</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="27.77777777777778%" headers="d278575e2208 ">Number of Threads <a class="xref" href="HDFSStandalone.html#concept_fgx_1jh_ldb" title="The Hadoop FS Standalone origin uses multiple concurrent threads to process data based on the Number of Threads property.">
                                            <img class="image" id="task_l3t_sdm_hdb__image_trk_mj4_qbb" src="../Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                    <td class="entry" valign="top" width="72.22222222222221%" headers="d278575e2211 ">
                                <p class="p">Number of threads the origin generates and uses for multithreaded
                                    processing. </p>

                                <p class="p">Default is 1.</p>

                            </td>

                                </tr>

                                <tr>
                            <td class="entry" valign="top" width="27.77777777777778%" headers="d278575e2208 ">File Name Pattern Mode</td>

                            <td class="entry" valign="top" width="72.22222222222221%" headers="d278575e2211 ">Indicates whether the file name pattern uses glob patterns or
                                regular expressions.</td>

                        </tr>

                                <tr>
                                    <td class="entry" valign="top" width="27.77777777777778%" headers="d278575e2208 ">File Name Pattern <a class="xref" href="HDFSStandalone.html#concept_exc_22h_ldb" title="Use a file name pattern to define the files that the Hadoop FS Standalone origin processes. You can use either a glob pattern or a regular expression to define the file name pattern.">
                                            <img class="image" id="task_l3t_sdm_hdb__image_lwb_zcp_4y" src="../Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                    <td class="entry" valign="top" width="72.22222222222221%" headers="d278575e2211 ">Pattern of the file names to process.
                                Use glob patterns or regular expressions based on the specified file
                                name pattern mode. </td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="27.77777777777778%" headers="d278575e2208 ">Read Order <a class="xref" href="HDFSStandalone.html#concept_ogz_q3h_ldb">
                                            <img class="image" id="task_l3t_sdm_hdb__image_f2g_d54_xv" src="../Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                    <td class="entry" valign="top" width="72.22222222222221%" headers="d278575e2211 ">The order to use when reading
                                    files:<ul class="ul" id="task_l3t_sdm_hdb__d51603e234">
                                    <li class="li">Last-Modified Timestamp - Reads files in ascending order
                                        based on the last-modified timestamp. When files have
                                        matching timestamps, reads files in lexicographically
                                        ascending order based on file names.</li>

                                    <li class="li">Lexicographically Ascending File Names - Reads files in
                                        lexicographically ascending order based on file name.</li>

                                </ul>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="27.77777777777778%" headers="d278575e2208 ">Process Subdirectories <a class="xref" href="HDFSStandalone.html#concept_bwq_nyj_mdb" title="When using the Last Modified Timestamp read order, the Hadoop FS Standalone origin can read files in subdirectories of the specified file directory.">
                                            <img class="image" id="task_l3t_sdm_hdb__image_n4r_2l3_cy" src="../Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                    <td class="entry" valign="top" width="72.22222222222221%" headers="d278575e2211 ">Reads files in any subdirectory
                                of the specified file directory. Reads files in ascending order
                                based on the last-modified timestamp, regardless of the location
                                within the file directory. <p class="p">Uses the subdirectory for any
                                    configured post-processing directories.</p>
<p class="p">Available only
                                    when using the Last Modified Timestamp read order. </p>
</td>

                                </tr>

                                <tr>
                            <td class="entry" valign="top" width="27.77777777777778%" headers="d278575e2208 ">Batch Size (recs)</td>

                            <td class="entry" valign="top" width="72.22222222222221%" headers="d278575e2211 ">Number of records to pass through the pipeline at one time.
                                Honors values up to the <span class="ph">Data Collector</span> maximum batch size. <p class="p">Default is 1000. The <span class="ph">Data Collector</span> default is 1000.</p>
</td>

                        </tr>

                                <tr>
                                    <td class="entry" valign="top" width="27.77777777777778%" headers="d278575e2208 ">Batch Wait Time (secs) <a class="xref" href="Origins_overview.html#concept_ypd_vgr_5q">
                                            <img class="image" id="task_l3t_sdm_hdb__image_mgp_2q3_br" src="../Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                    <td class="entry" valign="top" width="72.22222222222221%" headers="d278575e2211 ">Number of seconds to wait before sending
                                a partial or empty batch.</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="27.77777777777778%" headers="d278575e2208 ">First File to Process <a class="xref" href="HDFSStandalone.html#concept_a2q_4kh_ldb" title="Configure a first file for processing when you want Hadoop FS Standalone to ignore one or more existing files in the directory.">
                                            <img class="image" id="task_l3t_sdm_hdb__image_tcz_pj2_br" src="../Graphics/icon_moreInfo.png" height="12" width="12" /></a>
                                    </td>

                                    <td class="entry" valign="top" width="72.22222222222221%" headers="d278575e2211 ">Name of the first file to process.
                                    <p class="p">When you do not enter a first file name, the origin reads all
                                    files in the directory with the specified file name
                                pattern.</p>
</td>

                                </tr>

                                <tr>
                            <td class="entry" valign="top" width="27.77777777777778%" headers="d278575e2208 ">Max Files Soft Limit </td>

                            <td class="entry" valign="top" width="72.22222222222221%" headers="d278575e2211 ">
                                <p class="p">Maximum number of files that the origin can add to the processing
                                    queue at one time. This value is a soft limit - meaning that the
                                    origin can temporarily exceed it. </p>

                                <p class="p">If the origin exceeds this soft limit, the origin starts the
                                    spooling period timer. If the number of files in the processing
                                    queue goes below the soft limit, the origin adds more files from
                                    the directory to the queue. If the number of files in the
                                    processing queue remains above the soft limit after the
                                    configured spooling period expires, no more files are added to
                                    the queue until the queue goes below the soft limit.</p>

                                <p class="p">Configure the soft limit to the expected maximum number of files
                                    in the directory.</p>

                                <p class="p">Default is 1000.</p>

                            </td>

                        </tr>

                                <tr>
                            <td class="entry" valign="top" width="27.77777777777778%" headers="d278575e2208 ">Spooling Period (secs)</td>

                            <td class="entry" valign="top" width="72.22222222222221%" headers="d278575e2211 ">Number of seconds to continue adding files to the processing
                                queue after the maximum files soft limit has been exceeded. When the
                                spooling period expires, no additional files are added to the
                                processing queue until the queue goes below the soft
                                    limit.<p class="p">Default is 5 seconds.</p>
</td>

                        </tr>

                                <tr>
                                    <td class="entry" valign="top" width="27.77777777777778%" headers="d278575e2208 ">Buffer Limit (KB) <a class="xref" href="HDFSStandalone.html#concept_jbn_md3_ldb" title="The Hadoop FS Standalone origin passes each record to a buffer. The size of the buffer determines the maximum size of the record that can be processed. Decrease the buffer limit when memory on the Data Collector machine is limited. Increase the buffer limit to process larger records when memory is available.">
                                            <img class="image" id="task_l3t_sdm_hdb__image_mfn_hwx_5r" src="../Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                    <td class="entry" valign="top" width="72.22222222222221%" headers="d278575e2211 ">Maximum buffer size. The buffer size
                                determines the size of the record that can be processed. <p class="p">Decrease
                                    when memory on the <span class="ph">Data Collector</span> machine is limited. Increase to process larger records when
                                    memory is available. </p>
</td>

                                </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">On the <span class="ph uicontrol">Post Processing</span> tab, configure the following
                    properties:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_l3t_sdm_hdb__table_eqz_brp_ldb" class="table" frame="border" border="1" rules="all">
                    
                    
                    <thead class="thead" align="left">
                        <tr>
                            <th class="entry" valign="top" width="22.22222222222222%" id="d278575e2453">Post Processing Property</th>

                            <th class="entry" valign="top" width="77.77777777777779%" id="d278575e2456">Description</th>

                        </tr>

                    </thead>

                    <tbody class="tbody">
                        <tr>
                            <td class="entry" valign="top" width="22.22222222222222%" headers="d278575e2453 ">Error Directory</td>

                            <td class="entry" valign="top" width="77.77777777777779%" headers="d278575e2456 ">The directory for files that cannot be fully processed due to
                                data handling errors. <p class="p">When you specify an error directory, files
                                    that cannot be fully processed are moved to this directory.
                                    </p>
<p class="p">Use to manage files for error handling and reprocessing.
                                </p>
</td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="22.22222222222222%" headers="d278575e2453 ">File Post Processing</td>

                            <td class="entry" valign="top" width="77.77777777777779%" headers="d278575e2456 ">The action taken after processing a file: <ul class="ul" id="task_l3t_sdm_hdb__d51603e429">
                                    <li class="li">None - Keeps the file in place.</li>

                                    <li class="li">Archive - Moves the file to the archive directory.</li>

                                    <li class="li">Delete - Deletes the file.</li>

                                </ul>
</td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="22.22222222222222%" headers="d278575e2453 ">Archiving Directory</td>

                            <td class="entry" valign="top" width="77.77777777777779%" headers="d278575e2456 ">The directory for files that are fully processed. <p class="p">When you
                                    specify an archiving directory, files are moved to this
                                    directory after being fully processed.</p>
Use to archive
                                processed files.</td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="22.22222222222222%" headers="d278575e2453 ">Archive Retention Time (mins)</td>

                            <td class="entry" valign="top" width="77.77777777777779%" headers="d278575e2456 ">Number of minutes processed files are saved in the archive
                                directory. Use 0 to keep archived files indefinitely.</td>

                        </tr>

                    </tbody>

                </table>
</div>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">On the <span class="keyword wintitle">Data Format</span> tab, configure the following
                    property:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_l3t_sdm_hdb__table_hvy_pt3_vx" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr>
                                    <th class="entry" valign="top" width="30%" id="d278575e2544">Data Format Property</th>

                                    <th class="entry" valign="top" width="70%" id="d278575e2547">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d278575e2544 ">Data Format <a class="xref" href="HDFSStandalone.html#concept_vym_zd3_ldb">
                                            <img class="image" id="task_l3t_sdm_hdb__image_w4w_q3p_ht" src="../Graphics/icon_moreInfo.png" height="12" width="12" />
                                        </a></td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e2547 ">Data format for source files. Use one of the following
         formats:<ul class="ul" id="task_l3t_sdm_hdb__d47442e1271">
         <li class="li">Avro</li>

         <li class="li">Delimited</li>

         <li class="li">JSON</li>

         <li class="li">Log</li>

         <li class="li">Protobuf</li>

         <li class="li">SDC Record <a class="xref" href="../Pipeline_Design/SDCRecordFormat.html#concept_qkk_mwk_br" title="SDC Record is a proprietary data format that Data Collector uses to generate error records. Data Collector can also use the data format to read and write data.">
           <img class="image" id="task_l3t_sdm_hdb__d47442e1293" src="../../../reusable-content/datacollector/../../datacollector/UserGuide/Graphics/icon_moreInfo.png" height="12" width="12" /></a></li>

         <li class="li">Text</li>

         <li class="li">Whole File <a class="xref" href="../Data_Formats/WholeFile.html#concept_nfc_qkh_xw">
           <img class="image" id="task_l3t_sdm_hdb__d47442e1302" src="../../../reusable-content/datacollector/../../datacollector/UserGuide/Graphics/icon_moreInfo.png" height="12" width="12" /></a></li>

         <li class="li">XML</li>

        </ul>
</td>

                                </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">For delimited data, on the <span class="keyword wintitle">Data Format</span> tab, configure the
                    following properties:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_l3t_sdm_hdb__d47645e1380" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr>
                                    <th class="entry" valign="top" width="30%" id="d278575e2643">Delimited Property</th>

                                    <th class="entry" valign="top" width="70%" id="d278575e2646">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr>
       <td class="entry" valign="top" width="30%" headers="d278575e2643 ">Compression Format <a class="xref" href="Origins_overview.html#concept_uxr_g52_qs" title="Origins that read files can read uncompressed, compressed files, archives, and compressed archives.">
         <img class="image" id="task_l3t_sdm_hdb__d47442e1229" src="../../../reusable-content/datacollector/../../datacollector/UserGuide/Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

       <td class="entry" valign="top" width="70%" headers="d278575e2646 ">The compression format of the files:<ul class="ul" id="task_l3t_sdm_hdb__d47442e1233">
         <li class="li">None - Processes only uncompressed files.</li>

         <li class="li">Compressed File - Processes files compressed by the supported compression formats.</li>

         <li class="li">Archive - Processes files archived by the supported archive formats.</li>

         <li class="li">Compressed Archive - Processes files archived and compressed by the supported archive
          and compression formats.</li>

        </ul>
</td>

      </tr>

                                <tr>
       <td class="entry" valign="top" width="30%" headers="d278575e2643 ">File Name Pattern within Compressed Directory</td>

       <td class="entry" valign="top" width="70%" headers="d278575e2646 ">For archive and compressed archive files, file name pattern that represents the files
        to process within the compressed directory. You can use UNIX-style wildcards, such as an
        asterisk or question mark. For example, *.json.<p class="p">Default is *, which processes all
         files.</p>
</td>

      </tr>

                                <tr id="task_l3t_sdm_hdb__d47645e1412">
                                    <td class="entry" valign="top" width="30%" headers="d278575e2643 ">Delimiter Format Type</td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e2646 ">Delimiter format type. Use one of the following options:
                                            <ul class="ul" id="task_l3t_sdm_hdb__ul_s5z_b3z_3r">
                        <li class="li"><span class="ph uicontrol">Default CSV</span> - File that includes comma-separated
                              values. Ignores empty lines in the file.</li>

                        <li class="li"><span class="ph uicontrol">RFC4180 CSV</span> - Comma-separated file that strictly
                              follows RFC4180 guidelines.</li>

                        <li class="li"><span class="ph uicontrol">MS Excel CSV</span> - Microsoft Excel comma-separated
                              file.</li>

                        <li class="li"><span class="ph uicontrol">MySQL CSV</span> - MySQL comma-separated file.</li>

                        <li class="li"><span class="ph uicontrol">Postgres CSV</span> - Postgres comma-separated
                              file.</li>

                        <li class="li"><span class="ph uicontrol">Postgres Text</span> - Postgres text file.</li>

                        <li class="li"><span class="ph uicontrol">Tab-Separated Values</span> - File that includes
                              tab-separated values.</li>

                        <li class="li"><span class="ph uicontrol">Custom</span> - File that uses user-defined delimiter,
                              escape, and quote characters.</li>

                  </ul>
</td>

                                </tr>

                                <tr id="task_l3t_sdm_hdb__d47645e1425">
                                    <td class="entry" valign="top" width="30%" headers="d278575e2643 ">Header Line</td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e2646 ">Indicates whether a file contains a header line, and
                                        whether to use the header line.</td>

                                </tr>

                                <tr id="task_l3t_sdm_hdb__d47645e1434">
                                    <td class="entry" valign="top" width="30%" headers="d278575e2643 ">Allow Extra Columns</td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e2646 ">When processing data with a header line, allows
                                        processing records with more columns than exist in the
                                        header line.</td>

                                </tr>

                                <tr id="task_l3t_sdm_hdb__d47645e1444">
                                    <td class="entry" valign="top" width="30%" headers="d278575e2643 ">Extra Column Prefix</td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e2646 ">Prefix to use for any additional columns. Extra columns
                                        are named using the prefix and sequential increasing
                                        integers as follows:
                                            <samp class="ph codeph">&lt;prefix&gt;&lt;integer&gt;</samp>. <p class="p">For
                                            example, _extra_1. Default is _extra_.</p>
</td>

                                </tr>

                                <tr id="task_l3t_sdm_hdb__d47645e1458">
                                    <td class="entry" valign="top" width="30%" headers="d278575e2643 ">Max Record Length (chars)</td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e2646 ">Maximum length of a record in characters. Longer records
                                        are not read. <p class="p"><span class="ph">This property can be limited by the <span class="ph">Data Collector</span> parser
                        buffer size. For more information, see <a class="xref" href="Origins_overview.html#concept_svg_2zl_d1b">Maximum Record Size</a>.</span></p>
</td>

                                </tr>

                                <tr id="task_l3t_sdm_hdb__d47645e1469">
                                    <td class="entry" valign="top" width="30%" headers="d278575e2643 ">Delimiter Character</td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e2646 ">Delimiter character for a custom delimiter format. Select
                                        one of the available options or use Other to enter a custom
                                            character.<p class="p">You can enter a Unicode control character
                                            using the format \u<em class="ph i">NNNN</em>, where <em class="ph i">N</em> is a
                                            hexadecimal digit from the numbers 0-9 or the letters
                                            A-F. For example, enter \u0000 to use the null character
                                            as the delimiter or \u2028 to use a line separator as
                                            the delimiter.</p>
<p class="p">Default is the pipe character ( |
                                            ).</p>
</td>

                                </tr>

                                <tr id="task_l3t_sdm_hdb__d47645e1488">
                                    <td class="entry" valign="top" width="30%" headers="d278575e2643 ">Escape Character</td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e2646 ">Escape character for a custom file type.</td>

                                </tr>

                                <tr id="task_l3t_sdm_hdb__d47645e1497">
                                    <td class="entry" valign="top" width="30%" headers="d278575e2643 ">Quote Character</td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e2646 ">Quote character for a custom file type.</td>

                                </tr>

                                <tr id="task_l3t_sdm_hdb__d47645e1506">
                                    <td class="entry" valign="top" width="30%" headers="d278575e2643 ">Root Field Type <a class="xref" href="../Data_Formats/DelimitedDataRootFieldTypes.html">
                                            <img class="image" id="task_l3t_sdm_hdb__d47645e1512" src="../../../reusable-content/datacollector/../../datacollector/UserGuide/Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e2646 ">Root field type to use:<ul class="ul" id="task_l3t_sdm_hdb__d47645e1516">
                                            <li class="li">List-Map - Generates an indexed list of data.
                                                Enables you to use standard functions to process
                                                data. Use for new pipelines.</li>

                                            <li class="li">List - Generates a record with an indexed list with
                                                a map for header and value. Requires the use of
                                                delimited data functions to process data. Use only
                                                to maintain pipelines created before 1.1.0.</li>

                                        </ul>
</td>

                                </tr>

                                <tr id="task_l3t_sdm_hdb__d47645e1527">
                                    <td class="entry" valign="top" width="30%" headers="d278575e2643 ">Lines to Skip</td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e2646 ">Lines to skip before reading data. </td>

                                </tr>

                                <tr id="task_l3t_sdm_hdb__d47645e1536">
                                    <td class="entry" valign="top" width="30%" headers="d278575e2643 ">Parse NULLs</td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e2646 ">Replaces the specified string constant with null
                                        values.</td>

                                </tr>

                                <tr id="task_l3t_sdm_hdb__d47645e1545">
                                    <td class="entry" valign="top" width="30%" headers="d278575e2643 ">NULL Constant</td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e2646 ">String constant to replace with null values.</td>

                                </tr>

                                <tr>
       <td class="entry" valign="top" width="30%" headers="d278575e2643 ">Charset</td>

       <td class="entry" valign="top" width="70%" headers="d278575e2646 ">Character encoding of the files to be processed.</td>

      </tr>

                                <tr>
       <td class="entry" valign="top" width="30%" headers="d278575e2643 ">Ignore Ctrl Characters <a class="xref" href="../Pipeline_Design/ControlCharacters.html" title="You can use several stages to remove control characters - such as escape or end-of-transmission characters - from data. Remove control characters to avoid creating invalid records.">
         <img class="image" id="task_l3t_sdm_hdb__d47442e731" src="../../../reusable-content/datacollector/../../datacollector/UserGuide/Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

       <td class="entry" valign="top" width="70%" headers="d278575e2646 ">Removes all ASCII control characters except for the tab, line feed, and carriage
        return characters.</td>

      </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">For JSON data, on the <span class="keyword wintitle">Data Format</span> tab, configure the
                    following properties:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_l3t_sdm_hdb__d47645e1590" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr>
                                    <th class="entry" valign="top" width="30%" id="d278575e2936">JSON Property</th>

                                    <th class="entry" valign="top" width="70%" id="d278575e2939">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr>
       <td class="entry" valign="top" width="30%" headers="d278575e2936 ">Compression Format <a class="xref" href="Origins_overview.html#concept_uxr_g52_qs" title="Origins that read files can read uncompressed, compressed files, archives, and compressed archives.">
         <img class="image" id="task_l3t_sdm_hdb__d47442e1229" src="../../../reusable-content/datacollector/../../datacollector/UserGuide/Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

       <td class="entry" valign="top" width="70%" headers="d278575e2939 ">The compression format of the files:<ul class="ul" id="task_l3t_sdm_hdb__d47442e1233">
         <li class="li">None - Processes only uncompressed files.</li>

         <li class="li">Compressed File - Processes files compressed by the supported compression formats.</li>

         <li class="li">Archive - Processes files archived by the supported archive formats.</li>

         <li class="li">Compressed Archive - Processes files archived and compressed by the supported archive
          and compression formats.</li>

        </ul>
</td>

      </tr>

                                <tr>
       <td class="entry" valign="top" width="30%" headers="d278575e2936 ">File Name Pattern within Compressed Directory</td>

       <td class="entry" valign="top" width="70%" headers="d278575e2939 ">For archive and compressed archive files, file name pattern that represents the files
        to process within the compressed directory. You can use UNIX-style wildcards, such as an
        asterisk or question mark. For example, *.json.<p class="p">Default is *, which processes all
         files.</p>
</td>

      </tr>

                                <tr id="task_l3t_sdm_hdb__d47645e1622">
                                    <td class="entry" valign="top" width="30%" headers="d278575e2936 ">JSON Content</td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e2939 ">Type of JSON content. Use one of the following options: <div class="p">
                                            <ul class="ul" id="task_l3t_sdm_hdb__d47645e1631">
                                                <li class="li">Array of Objects </li>

                                                <li class="li">Multiple Objects</li>

                                            </ul>

                                        </div>
</td>

                                </tr>

                                <tr id="task_l3t_sdm_hdb__d47645e1642">
                                    <td class="entry" valign="top" width="30%" headers="d278575e2936 ">Maximum Object Length (chars)</td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e2939 ">Maximum number of characters in a JSON object. <p class="p">Longer
                                            objects are diverted to the pipeline for error handling.
                                                </p>
<p class="p"><span class="ph">This property can be limited by the <span class="ph">Data Collector</span> parser
                        buffer size. For more information, see <a class="xref" href="Origins_overview.html#concept_svg_2zl_d1b">Maximum Record Size</a>.</span></p>
</td>

                                </tr>

                                <tr>
       <td class="entry" valign="top" width="30%" headers="d278575e2936 ">Charset</td>

       <td class="entry" valign="top" width="70%" headers="d278575e2939 ">Character encoding of the files to be processed.</td>

      </tr>

                                <tr>
       <td class="entry" valign="top" width="30%" headers="d278575e2936 ">Ignore Ctrl Characters <a class="xref" href="../Pipeline_Design/ControlCharacters.html" title="You can use several stages to remove control characters - such as escape or end-of-transmission characters - from data. Remove control characters to avoid creating invalid records.">
         <img class="image" id="task_l3t_sdm_hdb__d47442e731" src="../../../reusable-content/datacollector/../../datacollector/UserGuide/Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

       <td class="entry" valign="top" width="70%" headers="d278575e2939 ">Removes all ASCII control characters except for the tab, line feed, and carriage
        return characters.</td>

      </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">For log data, on the <span class="keyword wintitle">Data Format</span> tab, configure the
                    following properties:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_l3t_sdm_hdb__d47645e1701" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr>
                                    <th class="entry" valign="top" width="30%" id="d278575e3081">Log Property</th>

                                    <th class="entry" valign="top" width="70%" id="d278575e3084">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr>
       <td class="entry" valign="top" width="30%" headers="d278575e3081 ">Compression Format <a class="xref" href="Origins_overview.html#concept_uxr_g52_qs" title="Origins that read files can read uncompressed, compressed files, archives, and compressed archives.">
         <img class="image" id="task_l3t_sdm_hdb__d47442e1229" src="../../../reusable-content/datacollector/../../datacollector/UserGuide/Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

       <td class="entry" valign="top" width="70%" headers="d278575e3084 ">The compression format of the files:<ul class="ul" id="task_l3t_sdm_hdb__d47442e1233">
         <li class="li">None - Processes only uncompressed files.</li>

         <li class="li">Compressed File - Processes files compressed by the supported compression formats.</li>

         <li class="li">Archive - Processes files archived by the supported archive formats.</li>

         <li class="li">Compressed Archive - Processes files archived and compressed by the supported archive
          and compression formats.</li>

        </ul>
</td>

      </tr>

                                <tr>
       <td class="entry" valign="top" width="30%" headers="d278575e3081 ">File Name Pattern within Compressed Directory</td>

       <td class="entry" valign="top" width="70%" headers="d278575e3084 ">For archive and compressed archive files, file name pattern that represents the files
        to process within the compressed directory. You can use UNIX-style wildcards, such as an
        asterisk or question mark. For example, *.json.<p class="p">Default is *, which processes all
         files.</p>
</td>

      </tr>

                                <tr id="task_l3t_sdm_hdb__d47645e1733">
                                    <td class="entry" valign="top" width="30%" headers="d278575e3081 ">Log Format <a class="xref" href="../Data_Formats/LogFormats.html" title="When you use an origin to read log data, you define the format of the log files to be read.">
                                            <img class="image" id="task_l3t_sdm_hdb__d47645e1739" src="../../../reusable-content/datacollector/../../datacollector/UserGuide/Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e3084 ">Format of the log files. Use one of the following
                                            options:<ul class="ul" id="task_l3t_sdm_hdb__d47645e1743">
                                            <li class="li">Common Log Format</li>

                                            <li class="li">Combined Log Format</li>

                                            <li class="li">Apache Error Log Format</li>

                                            <li class="li">Apache Access Log Custom Format</li>

                                            <li class="li">Regular Expression</li>

                                            <li class="li">Grok Pattern</li>

                                            <li class="li">Log4j</li>

                                            <li class="li">Common Event Format (CEF)</li>

                                            <li class="li">Log Event Extended Format (LEEF)</li>

                                        </ul>
</td>

                                </tr>

                                <tr id="task_l3t_sdm_hdb__d47645e1775">
                                    <td class="entry" valign="top" width="30%" headers="d278575e3081 ">Max Line Length</td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e3084 ">Maximum length of a log line. The origin truncates longer
                                        lines. <p class="p"><span class="ph">This property can be limited by the <span class="ph">Data Collector</span> parser
                        buffer size. For more information, see <a class="xref" href="Origins_overview.html#concept_svg_2zl_d1b">Maximum Record Size</a>.</span></p>
</td>

                                </tr>

                                <tr id="task_l3t_sdm_hdb__d47645e1786">
                                    <td class="entry" valign="top" width="30%" headers="d278575e3081 ">Retain Original Line</td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e3084 ">Determines how to treat the original log line. Select to
                                        include the original log line as a field in the resulting
                                            record.<p class="p">By default, the original line is
                                            discarded.</p>
</td>

                                </tr>

                                <tr>
       <td class="entry" valign="top" width="30%" headers="d278575e3081 ">Charset</td>

       <td class="entry" valign="top" width="70%" headers="d278575e3084 ">Character encoding of the files to be processed.</td>

      </tr>

                                <tr>
       <td class="entry" valign="top" width="30%" headers="d278575e3081 ">Ignore Ctrl Characters <a class="xref" href="../Pipeline_Design/ControlCharacters.html" title="You can use several stages to remove control characters - such as escape or end-of-transmission characters - from data. Remove control characters to avoid creating invalid records.">
         <img class="image" id="task_l3t_sdm_hdb__d47442e731" src="../../../reusable-content/datacollector/../../datacollector/UserGuide/Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

       <td class="entry" valign="top" width="70%" headers="d278575e3084 ">Removes all ASCII control characters except for the tab, line feed, and carriage
        return characters.</td>

      </tr>

                            </tbody>

                        </table>
</div>

                    <ul class="ul" id="task_l3t_sdm_hdb__d47645e1811">
                        <li class="li">When you select <span class="ph uicontrol">Apache Access Log Custom Format</span>,
                            use Apache log format strings to define the <span class="ph uicontrol">Custom Log
                                Format</span>.</li>

                        <li class="li">When you select <span class="ph uicontrol">Regular Expression</span>, enter the
                            regular expression that describes the log format, and then map the
                            fields that you want to include to each regular expression group.</li>

                        <li class="li">When you select <span class="ph uicontrol">Grok Pattern</span>, you can use the
                                <span class="ph uicontrol">Grok Pattern Definition</span> field to define
                            custom grok patterns. You can define a pattern on each line. <p class="p">In the
                                    <span class="ph uicontrol">Grok Pattern</span> field, enter the pattern to
                                use to parse the log. You can use a predefined grok patterns or
                                create a custom grok pattern using patterns defined in
                                    <span class="ph uicontrol">Grok Pattern Definition</span>.</p>
<p class="p">For more
                                information about defining grok patterns and supported grok
                                patterns, see <a class="xref" href="../Apx-GrokPatterns/GrokPatterns_title.html#concept_vdk_xjb_wr" title="You can use the grok patterns in this appendix to define the structure of log data.">Defining Grok Patterns</a>.</p>
</li>

                        <li class="li">When you select <span class="ph uicontrol">Log4j</span>, define the following properties:<div class="p">
                                
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_l3t_sdm_hdb__d47645e1856" class="table" frame="border" border="1" rules="all">
                                        
                                        
                                        <thead class="thead" align="left">
                                            <tr>
                                                <th class="entry" valign="top" width="22.22222222222222%" id="d278575e3299">Log4j Property</th>

                                                <th class="entry" valign="top" width="77.77777777777779%" id="d278575e3302">Description</th>

                                            </tr>

                                        </thead>

                                        <tbody class="tbody">
                                            <tr>
                                                <td class="entry" valign="top" width="22.22222222222222%" headers="d278575e3299 ">On Parse Error</td>

                                                <td class="entry" valign="top" width="77.77777777777779%" headers="d278575e3302 ">Determines how to handle information that
                                                  cannot be parsed:<ul class="ul" id="task_l3t_sdm_hdb__d47645e1885">
                                                  <li class="li">Skip and Log Error - Skips reading the line
                                                  and logs a stage error.</li>

                                                  <li class="li">Skip, No Error - Skips reading the line and
                                                  does not log an error.</li>

                                                  <li class="li">Include as Stack Trace - Includes information
                                                  that cannot be parsed as a stack trace to the
                                                  previously-read log line. The information is added
                                                  to the message field for the last valid log
                                                  line.</li>

                                                  </ul>
</td>

                                            </tr>

                                            <tr>
                                                <td class="entry" valign="top" width="22.22222222222222%" headers="d278575e3299 ">Use Custom Log Format</td>

                                                <td class="entry" valign="top" width="77.77777777777779%" headers="d278575e3302 ">Allows you to define a custom log
                                                  format.</td>

                                            </tr>

                                            <tr>
                                                <td class="entry" valign="top" width="22.22222222222222%" headers="d278575e3299 ">Custom Format</td>

                                                <td class="entry" valign="top" width="77.77777777777779%" headers="d278575e3302 ">Use log4j variables to define a custom log
                                                  format. </td>

                                            </tr>

                                        </tbody>

                                    </table>
</div>

                            </div>
</li>

                    </ul>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">For protobuf data, on the <span class="keyword wintitle">Data Format</span> tab, configure the
                    following properties:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_l3t_sdm_hdb__d47645e2055" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr>
                                    <th class="entry" valign="top" width="30%" id="d278575e3376">Protobuf Property</th>

                                    <th class="entry" valign="top" width="70%" id="d278575e3379">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr>
       <td class="entry" valign="top" width="30%" headers="d278575e3376 ">Compression Format <a class="xref" href="Origins_overview.html#concept_uxr_g52_qs" title="Origins that read files can read uncompressed, compressed files, archives, and compressed archives.">
         <img class="image" id="task_l3t_sdm_hdb__d47442e1229" src="../../../reusable-content/datacollector/../../datacollector/UserGuide/Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

       <td class="entry" valign="top" width="70%" headers="d278575e3379 ">The compression format of the files:<ul class="ul" id="task_l3t_sdm_hdb__d47442e1233">
         <li class="li">None - Processes only uncompressed files.</li>

         <li class="li">Compressed File - Processes files compressed by the supported compression formats.</li>

         <li class="li">Archive - Processes files archived by the supported archive formats.</li>

         <li class="li">Compressed Archive - Processes files archived and compressed by the supported archive
          and compression formats.</li>

        </ul>
</td>

      </tr>

                                <tr>
       <td class="entry" valign="top" width="30%" headers="d278575e3376 ">File Name Pattern within Compressed Directory</td>

       <td class="entry" valign="top" width="70%" headers="d278575e3379 ">For archive and compressed archive files, file name pattern that represents the files
        to process within the compressed directory. You can use UNIX-style wildcards, such as an
        asterisk or question mark. For example, *.json.<p class="p">Default is *, which processes all
         files.</p>
</td>

      </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d278575e3376 ">Protobuf Descriptor File </td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e3379 ">Descriptor file (.desc) to use. The descriptor file must
                                        be in the <span class="ph">Data Collector</span> resources directory, <samp class="ph codeph">$SDC_RESOURCES</samp>.
                                                <p class="p"><span class="ph">For more information about environment variables, see
                              <a class="xref" href="../Configuration/DCEnvironmentConfig.html#concept_rng_qym_qr" title="Data Collector includes environment variables that define the directories used to store configuration, data, log, and resource files.When you run Data Collector as a service, Data Collector runs as the system user account and group defined in environment variables. The default system user and group are named sdc.Increase or decrease the Data Collector Java heap size as necessary, based on the resources available on the host machine. By default, the Java heap size is 1024 MB. You can enable remote debugging to debug a Data Collector instance running on a remote machine. You can define the Java garbage collector that Data Collector uses. By default, Data Collector uses the Concurrent Mark Sweep (CMS) garbage collector.Data Collector includes a Java Security Manager that is enabled by default. You can edit the SDC_ROOT_CLASSPATH environment variable to define the path to JAR files to be added to the Data Collector root classloader.">Data Collector Environment Configuration</a>.</span> For information about generating the descriptor file,
                                            see <a class="xref" href="../Data_Formats/Protobuf-Prerequisites.html" title="Perform the following prerequisites before reading or writing protobuf data.">Protobuf Data Format Prerequisites</a>.</p>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d278575e3376 ">Message Type</td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e3379 ">The fully-qualified name for the message type to use when
                                        reading data.<p class="p">Use the following format:
                                                <samp class="ph codeph">&lt;package name&gt;.&lt;message
                                            type&gt;</samp>. </p>
Use a message type defined in the
                                        descriptor file.</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d278575e3376 ">Delimited Messages</td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e3379 ">Indicates if a file might include more than one protobuf
                                        message.</td>

                                </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">For SDC Record data, on the <span class="keyword wintitle">Data Format</span> tab, configure the
                    following properties:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_l3t_sdm_hdb__d47645e2157" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr>
                                    <th class="entry" valign="top" width="30%" id="d278575e3538">SDC Record Property</th>

                                    <th class="entry" valign="top" width="70%" id="d278575e3541">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr>
       <td class="entry" valign="top" width="30%" headers="d278575e3538 ">Compression Format <a class="xref" href="Origins_overview.html#concept_uxr_g52_qs" title="Origins that read files can read uncompressed, compressed files, archives, and compressed archives.">
         <img class="image" id="task_l3t_sdm_hdb__d47442e1229" src="../../../reusable-content/datacollector/../../datacollector/UserGuide/Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

       <td class="entry" valign="top" width="70%" headers="d278575e3541 ">The compression format of the files:<ul class="ul" id="task_l3t_sdm_hdb__d47442e1233">
         <li class="li">None - Processes only uncompressed files.</li>

         <li class="li">Compressed File - Processes files compressed by the supported compression formats.</li>

         <li class="li">Archive - Processes files archived by the supported archive formats.</li>

         <li class="li">Compressed Archive - Processes files archived and compressed by the supported archive
          and compression formats.</li>

        </ul>
</td>

      </tr>

                                <tr>
       <td class="entry" valign="top" width="30%" headers="d278575e3538 ">File Name Pattern within Compressed Directory</td>

       <td class="entry" valign="top" width="70%" headers="d278575e3541 ">For archive and compressed archive files, file name pattern that represents the files
        to process within the compressed directory. You can use UNIX-style wildcards, such as an
        asterisk or question mark. For example, *.json.<p class="p">Default is *, which processes all
         files.</p>
</td>

      </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">For text data, on the <span class="keyword wintitle">Data Format</span> tab, configure the
                    following properties:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_l3t_sdm_hdb__d47645e2307" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr>
                                    <th class="entry" valign="top" width="30%" id="d278575e3616">Text Property</th>

                                    <th class="entry" valign="top" width="70%" id="d278575e3619">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr>
       <td class="entry" valign="top" width="30%" headers="d278575e3616 ">Compression Format <a class="xref" href="Origins_overview.html#concept_uxr_g52_qs" title="Origins that read files can read uncompressed, compressed files, archives, and compressed archives.">
         <img class="image" id="task_l3t_sdm_hdb__d47442e1229" src="../../../reusable-content/datacollector/../../datacollector/UserGuide/Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

       <td class="entry" valign="top" width="70%" headers="d278575e3619 ">The compression format of the files:<ul class="ul" id="task_l3t_sdm_hdb__d47442e1233">
         <li class="li">None - Processes only uncompressed files.</li>

         <li class="li">Compressed File - Processes files compressed by the supported compression formats.</li>

         <li class="li">Archive - Processes files archived by the supported archive formats.</li>

         <li class="li">Compressed Archive - Processes files archived and compressed by the supported archive
          and compression formats.</li>

        </ul>
</td>

      </tr>

                                <tr>
       <td class="entry" valign="top" width="30%" headers="d278575e3616 ">File Name Pattern within Compressed Directory</td>

       <td class="entry" valign="top" width="70%" headers="d278575e3619 ">For archive and compressed archive files, file name pattern that represents the files
        to process within the compressed directory. You can use UNIX-style wildcards, such as an
        asterisk or question mark. For example, *.json.<p class="p">Default is *, which processes all
         files.</p>
</td>

      </tr>

                                <tr id="task_l3t_sdm_hdb__d47645e2339">
                                    <td class="entry" valign="top" width="30%" headers="d278575e3616 ">Max Line Length</td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e3619 ">Maximum number of characters allowed for a line. Longer
                                        lines are truncated.<p class="p">Adds a boolean field to the record to
                                            indicate if it was truncated. The field name is
                                            Truncated. </p>
<p class="p"><span class="ph">This property can be limited by the <span class="ph">Data Collector</span> parser
                        buffer size. For more information, see <a class="xref" href="Origins_overview.html#concept_svg_2zl_d1b">Maximum Record Size</a>.</span></p>
</td>

                                </tr>

                                <tr id="task_l3t_sdm_hdb__d47645e2352">
                                    <td class="entry" valign="top" width="30%" headers="d278575e3616 ">Use Custom Delimiter <a class="xref" href="../Data_Formats/TextCDelim.html#concept_lg2_gcg_jx">
                                            <img class="image" id="task_l3t_sdm_hdb__d47645e2358" src="../../../reusable-content/datacollector/../../datacollector/UserGuide/Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e3619 ">Uses custom delimiters to define records instead of line
                                        breaks. </td>

                                </tr>

                                <tr id="task_l3t_sdm_hdb__d47645e2364">
                                    <td class="entry" valign="top" width="30%" headers="d278575e3616 ">Custom Delimiter</td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e3619 ">One or more characters to use to define records. </td>

                                </tr>

                                <tr id="task_l3t_sdm_hdb__d47645e2374">
                                    <td class="entry" valign="top" width="30%" headers="d278575e3616 ">Include Custom Delimiter</td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e3619 ">Includes delimiter characters in the record.</td>

                                </tr>

                                <tr>
       <td class="entry" valign="top" width="30%" headers="d278575e3616 ">Charset</td>

       <td class="entry" valign="top" width="70%" headers="d278575e3619 ">Character encoding of the files to be processed.</td>

      </tr>

                                <tr>
       <td class="entry" valign="top" width="30%" headers="d278575e3616 ">Ignore Ctrl Characters <a class="xref" href="../Pipeline_Design/ControlCharacters.html" title="You can use several stages to remove control characters - such as escape or end-of-transmission characters - from data. Remove control characters to avoid creating invalid records.">
         <img class="image" id="task_l3t_sdm_hdb__d47442e731" src="../../../reusable-content/datacollector/../../datacollector/UserGuide/Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

       <td class="entry" valign="top" width="70%" headers="d278575e3619 ">Removes all ASCII control characters except for the tab, line feed, and carriage
        return characters.</td>

      </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">For whole files on the <span class="keyword wintitle">Data Format</span> tab, configure the
                    following property:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_l3t_sdm_hdb__d47645e2420" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr>
                                    <th class="entry" valign="top" width="30%" id="d278575e3773">Whole File Property</th>

                                    <th class="entry" valign="top" width="70%" id="d278575e3776">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d278575e3773 ">Buffer Size (bytes)</td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e3776 ">Size of the buffer to use to transfer data.</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d278575e3773 ">Rate per Second <a class="xref" href="../Data_Formats/WholeFile.html#concept_prp_jzd_py">
                                            <img class="image" src="../../../reusable-content/datacollector/../../datacollector/UserGuide/Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e3776 ">Transfer rate to use. <p class="p">Enter a number to specify a rate
                                            in bytes per second. Use an expression to specify a rate
                                            that uses a different unit of measure per second, e.g.
                                            ${5 * MB}. Use -1 to opt out of this property. </p>
<p class="p">By
                                            default, the origin does not use a transfer rate.
                                        </p>
</td>

                                </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">For XML data, on the <span class="keyword wintitle">XML</span> tab, configure the following
                    properties:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_l3t_sdm_hdb__d47645e2575" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr>
                                    <th class="entry" valign="top" width="30%" id="d278575e3836">XML Property</th>

                                    <th class="entry" valign="top" width="70%" id="d278575e3839">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr>
       <td class="entry" valign="top" width="30%" headers="d278575e3836 ">Compression Format <a class="xref" href="Origins_overview.html#concept_uxr_g52_qs" title="Origins that read files can read uncompressed, compressed files, archives, and compressed archives.">
         <img class="image" id="task_l3t_sdm_hdb__d47442e1229" src="../../../reusable-content/datacollector/../../datacollector/UserGuide/Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

       <td class="entry" valign="top" width="70%" headers="d278575e3839 ">The compression format of the files:<ul class="ul" id="task_l3t_sdm_hdb__d47442e1233">
         <li class="li">None - Processes only uncompressed files.</li>

         <li class="li">Compressed File - Processes files compressed by the supported compression formats.</li>

         <li class="li">Archive - Processes files archived by the supported archive formats.</li>

         <li class="li">Compressed Archive - Processes files archived and compressed by the supported archive
          and compression formats.</li>

        </ul>
</td>

      </tr>

                                <tr id="task_l3t_sdm_hdb__d47645e2602">
                                    <td class="entry" valign="top" width="30%" headers="d278575e3836 ">Delimiter Element <a class="xref" href="../Data_Formats/XMLDFormat.html#concept_tmc_4bc_dy">
                                            <img class="image" id="task_l3t_sdm_hdb__d47645e2608" src="../../../reusable-content/datacollector/../../datacollector/UserGuide/Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e3839 ">
                                        <div class="p">Delimiter to use to generate records. Omit a delimiter to
                                            treat the entire XML document as one record. Use one of
                                            the following:<ul class="ul" id="task_l3t_sdm_hdb__d47645e2614">
                                                <li class="li">An XML element directly under the root element.
                                                  <p class="p">Use the XML element name without surrounding
                                                  angle brackets ( &lt; &gt; ) . For example, msg
                                                  instead of &lt;msg&gt;. </p>
</li>

                                                <li class="li">A simplified XPath expression that specifies the
                                                  data to use.<p class="p">Use a simplified XPath expression
                                                  to access data deeper in the XML document or data
                                                  that requires a more complex access
                                                  method.</p>
<p class="p">For more information about valid
                                                  syntax, see <a class="xref" href="../Data_Formats/XMLDFormat.html#concept_tmc_4bc_dy">Simplified XPath Syntax</a>.</p>
</li>

                                            </ul>
</div>

                                    </td>

                                </tr>

                                <tr id="task_l3t_sdm_hdb__d47645e2633">
                                    <td class="entry" valign="top" width="30%" headers="d278575e3836 ">Include Field XPaths <a class="xref" href="../Data_Formats/XMLDFormat.html#concept_w3k_1ch_qz">
                                            <img class="image" src="../../../reusable-content/datacollector/../../datacollector/UserGuide/Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e3839 ">Includes the XPath to each parsed XML element and XML
                                        attribute in field attributes. Also includes each namespace
                                        in an xmlns record header attribute. <p class="p">When not selected,
                                            this information is not included in the record. By
                                            default, the property is not selected.</p>
<div class="p">
                                            <div class="note note"><span class="notetitle">Note:</span> <span class="ph">Field attributes and record header attributes are
                        written to destination systems automatically only when you use the SDC RPC
                        data format in destinations. For more information about working with field
                        attributes and record header attributes, and how to include them in records,
                        see <a class="xref" href="../Pipeline_Design/FieldAttributes.html#concept_xfm_wtp_1z" title="Field attributes are attributes that provide additional information about each field that you can use in pipeline logic, as needed.">Field Attributes</a> and <a class="xref" href="../Pipeline_Design/RecordHeaderAttributes.html#concept_wn2_jcz_dz">Record Header Attributes</a>.</span></div>

                                        </div>
</td>

                                </tr>

                                <tr id="task_l3t_sdm_hdb__d47645e2652">
                                    <td class="entry" valign="top" width="30%" headers="d278575e3836 ">Namespaces </td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e3839 ">Namespace prefix and URI to use when parsing the XML
                                        document. Define namespaces when the XML element being used
                                        includes a namespace prefix or when the XPath expression
                                        includes namespaces.<p class="p">For information about using
                                            namespaces with an XML element, see <a class="xref" href="../Data_Formats/XMLDFormat.html#concept_ilc_r3g_2y">Using XML Elements with Namespaces</a>.</p>
<p class="p">For information about using namespaces with
                                            XPath expressions, see <a class="xref" href="../Data_Formats/XMLDFormat.html#concept_mkk_3zj_dy">Using XPath Expressions with Namespaces</a>.</p>
<p class="p">Using <a class="xref" href="../Pipeline_Configuration/SimpleBulkEdit.html#concept_alb_b3y_cbb">simple or bulk edit mode</a>, click the
                                                <span class="ph uicontrol">Add</span> icon to add additional
                                            namespaces.</p>
</td>

                                </tr>

                                <tr id="task_l3t_sdm_hdb__d47645e2677">
                                    <td class="entry" valign="top" width="30%" headers="d278575e3836 ">Output Field Attributes</td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e3839 ">Includes XML attributes and namespace declarations in the
                                        record as field attributes. When not selected, XML
                                        attributes and namespace declarations are included in the
                                        record as fields.<div class="note note"><span class="notetitle">Note:</span> <span class="ph">Field attributes are automatically included in
                        records written to destination systems only when you use the SDC RPC data
                        format in the destination.</span> For more information about working with field
                                            attributes, see <a class="xref" href="../Pipeline_Design/FieldAttributes.html#concept_xfm_wtp_1z" title="Field attributes are attributes that provide additional information about each field that you can use in pipeline logic, as needed.">Field Attributes</a>.</div>
<p class="p">By default, the property is not
                                            selected.</p>
</td>

                                </tr>

                                <tr id="task_l3t_sdm_hdb__d47645e2694">
                                    <td class="entry" valign="top" width="30%" headers="d278575e3836 ">Max Record Length (chars) </td>

                                    <td class="entry" valign="top" width="70%" headers="d278575e3839 ">
                                        <p class="p">The maximum number of characters in a record. Longer
                                            records are diverted to the pipeline for error handling. </p>

                                        <p class="p"><span class="ph">This property can be limited by the <span class="ph">Data Collector</span> parser
                        buffer size. For more information, see <a class="xref" href="Origins_overview.html#concept_svg_2zl_d1b">Maximum Record Size</a>.</span></p>

                                    </td>

                                </tr>

                                <tr>
       <td class="entry" valign="top" width="30%" headers="d278575e3836 ">Charset</td>

       <td class="entry" valign="top" width="70%" headers="d278575e3839 ">Character encoding of the files to be processed.</td>

      </tr>

                                <tr>
       <td class="entry" valign="top" width="30%" headers="d278575e3836 ">Ignore Ctrl Characters <a class="xref" href="../Pipeline_Design/ControlCharacters.html" title="You can use several stages to remove control characters - such as escape or end-of-transmission characters - from data. Remove control characters to avoid creating invalid records.">
         <img class="image" id="task_l3t_sdm_hdb__d47442e731" src="../../../reusable-content/datacollector/../../datacollector/UserGuide/Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

       <td class="entry" valign="top" width="70%" headers="d278575e3839 ">Removes all ASCII control characters except for the tab, line feed, and carriage
        return characters.</td>

      </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
</ol>

    </div>

</div>
</div>
<div class="navfooter"><!---->
<span class="navparent"><a class="link" href="../../../datacollector/UserGuide/Origins/Origins_title.html" title="Origins"><span class="navheader_label">Parent topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Origins</span></a></span>  </div><div class="footer" id="webhelp_copyright_information"><!-- Copyright 2018 StreamSets Inc. --><!-- SDC google analytics --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-60917135-3', 'auto');
  ga('send', 'pageview');
</script></div>
</body>
</html>