
<!DOCTYPE html
  SYSTEM "about:legacy-compat">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><meta name="description" content="Dataflow triggers are instructions for the event framework to kick off tasks in response to events that occur in the pipeline. For example, you can use dataflow triggers to start a MapReduce job after ..." /><meta name="copyright" content="(C) Copyright 2005" /><meta name="DC.rights.owner" content="(C) Copyright 2005" /><meta name="DC.Type" content="concept" /><meta name="DC.Title" content="Dataflow Triggers (a.k.a. Event Framework)" /><meta name="DC.Relation" scheme="URI" content="../DPM/UnregisterSDCwithDPM.html#concept_ldb_sr5_cx" /><meta name="DC.Relation" scheme="URI" content="../Hive_Drift_Solution/HiveDriftSolution_title.html#concept_fjj_zcf_2w" /><meta name="DC.Format" content="XHTML" /><meta name="DC.Identifier" content="concept_xxd_f5r_kx" /><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/commonltr.css"><!----></link><title>Dataflow Triggers (a.k.a. Event Framework)</title><!--  Generated with Oxygen version 18.1, build number 2016112217.  --><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/webhelp_topic.css"><!----></link><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/skins/skin.css" /><link rel="stylesheet" type="text/css" href="../skin.css" /><script type="text/javascript"><!--
            
            var prefix = "../index.html";
            
            --></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-1.11.3.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.cookie.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-ui.custom.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.highlight-3.js"><!----></script><script type="text/javascript" charset="utf-8" src="../oxygen-webhelp/resources/js/webhelp_topic.js"><!----></script>
<!--
    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
-->
</head>
<body onload="highlightSearchTerm()" class="frmBody">
<table class="nav"><tbody><tr><td colspan="2"><div id="printlink"><a href="javascript:window.print();" title="Print this page"></a></div><div id="permalink"><a href="#" title="Link to this page"></a></div></td></tr><tr><td style="width:75%;"><span class="topic_breadcrumb_links"></span></td><td><span id="topic_navigation_links" class="navheader">
<span class="navprev"><a class="link" href="../DPM/UnregisterSDCwithDPM.html#concept_ldb_sr5_cx" title="Unregister Data Collector from DPM"><span class="navheader_label">Previous topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Unregister Data Collector from DPM</span></a></span>  
<span class="navnext"><a class="link" href="../Hive_Drift_Solution/HiveDriftSolution_title.html#concept_fjj_zcf_2w" title="Drift Synchronization Solution (a.k.a. Hive Drift Solution)"><span class="navheader_label">Next topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Drift Synchronization Solution (a.k.a. Hive Drift Solution)</span></a></span>  </span></td></tr></tbody></table>
<div class="nested0" id="concept_xxd_f5r_kx">
 <h1 class="title topictitle1">Dataflow Triggers (a.k.a. Event Framework)</h1>

<div class="related-links"></div>
<div class="topic concept nested1" id="concept_cph_5h4_lx">
    <h2 class="title topictitle2">Dataflow Triggers Overview</h2>

    <div class="body conbody">
        <p class="p"><dfn class="term">Dataflow
                triggers</dfn> are instructions for the event framework to kick off tasks in
            response to events that occur in the pipeline. For example, you can use dataflow
            triggers to start a MapReduce job after the pipeline writes a file to HDFS. Or you might
            use a dataflow trigger to stop a pipeline after the JDBC Query Consumer origin processes
            all available data. </p>

        <div class="p">The event framework consist of the following components:<dl class="dl">
                
                    <dt class="dt dlterm">event generation</dt>

                    <dd class="dd">The event framework generates pipeline-related events and stage-related
                        events. The framework generates pipeline events only when the pipeline
                        starts and stops. The framework generates stage events when specific
                        stage-related actions take place. The action that generates an event differs
                        from stage to stage and is related to how the stage processes data. </dd>

                    <dd class="dd">For example, the Hive Metastore destination updates the Hive metastore, so
                        it generates events each time it changes the metastore. In contrast, the
                        Hadoop FS destination writes files to HDFS, so it generates events each time
                        it closes a file. </dd>

                    <dd class="dd">Events produce <dfn class="term">event records</dfn>. Pipeline-related event records
                        are passed immediately to the specified event consumer. Stage-related event
                        records are passed through the pipeline in an <dfn class="term">event stream</dfn>. </dd>

                
                
                    <dt class="dt dlterm">task execution</dt>

                    <dd class="dd">To trigger a task, you need an <dfn class="term">executor</dfn>. Executor stages
                        perform tasks in <span class="ph">Data
                  Collector</span> or external systems. Each time an executor receives an event, it performs
                        the specified task.</dd>

                    <dd class="dd">For example, the Hive Query executor runs user-defined Hive or Impala
                        queries each time it receives an event, and the MapReduce executor triggers
                        a MapReduce job when it receives events. Within <span class="ph">Data
                  Collector</span>, the Pipeline Finisher executor stops a pipeline upon receiving an event,
                        transitioning the pipeline to a Finished state. </dd>

                
                
                    <dt class="dt dlterm">event storage</dt>

                    <dd class="dd">To store event information, pass the event to a destination. The destination
                        writes the event records to the destination system, just like any other
                        data.</dd>

                    <dd class="dd">For example, you might store event records to keep an audit trail of the
                        files that the pipeline origin reads. </dd>

                
            </dl>
</div>

    </div>

</div>
<div class="topic concept nested1" id="concept_amg_2qr_t1b">
    <h2 class="title topictitle2">Pipeline Event Generation</h2>

    <div class="body conbody">
        <p class="p">The event
            framework generates pipeline events at specific points in the pipeline lifecycle. You
            can configure the pipeline properties to pass each event to an executor or to another
            pipeline for more complex processing.</p>

        <div class="p">The event framework generates the following pipeline-related events:<dl class="dl">
                
                    <dt class="dt dlterm">Pipeline Start</dt>

                    <dd class="dd">The pipeline start event is generated <span class="ph">as the pipeline initializes, immediately after it starts and
                        before individual stages are initialized</span>. This can allow time for an executor to perform a task before stages
                        initialize. </dd>

                    <dd class="dd">Most executors wait for confirmation that a task completes. As a result, the
                        pipeline waits for the executor to complete the task before continuing with
                        stage initialization. For example, if you configure the JDBC Query executor
                        to truncate a table before the pipeline begins, the pipeline waits until the
                        task is complete before processing any data. </dd>

                    <dd class="dd">
                        <p class="p">The MapReduce executor and Spark executor kick off jobs and do not wait
                            for the submitted jobs to complete. When you use one of these executors,
                            the pipeline waits only for successful job submission before continuing
                            with stage initialization.</p>

                        <p class="p">If the executor fails to process the event, for example if a Hive Query
                            Executor fails to execute the specified query or if the query fails,
                            then the initialization phase fails and the pipeline does not start.
                            Instead the pipeline transitions to a failure state.</p>

                    </dd>

                
                
                    <dt class="dt dlterm">Pipeline Stop</dt>

                    <dd class="dd">The pipeline stop event is generated <span class="ph">as the pipeline stops, either manually, programmatically, or due
                        to a failure. The stop event is generated after all stages have completed
                        processing and cleaning up temporary resources, such as removing temporary
                        files.</span> This allows an executor to perform a task after pipeline processing is
                        complete, before the pipeline fully stops.</dd>

                    <dd class="dd">
                        <p class="p">Similar to start event consumers, the behavior of the executor that
                            consumes the event determines whether the pipeline waits for the
                            executor task to complete before allowing the pipeline to stop. Also, if
                            the processing of the pipeline stop event fails for any reason, the
                            pipeline transitions to a failed state even though the data processing
                            was successful.</p>

                    </dd>

                
            </dl>
</div>

        <div class="p">Pipeline events differ from stage events as follows:<ul class="ul" id="concept_amg_2qr_t1b__ul_n1m_k5r_t1b">
                <li class="li"><strong class="ph b">Virtual processing</strong> - Unlike stage events, pipeline events are not
                    processed by stages that you configure in the canvas. They are passed to an
                    event consumer that you configure in the pipeline properties.<p class="p">The event
                        consumer does not display in the pipelineâ€™s canvas. As a result, pipeline
                        events are also not visualized in data preview or pipeline monitoring.
                    </p>
</li>

                <li class="li"><strong class="ph b">Single-use events</strong> - You can configure only one event consumer for each
                    event type within the pipeline properties: one for the Start event and one for
                    the Stop event. <p class="p">When necessary, you can pass pipeline events to another
                        pipeline. In the event consuming pipeline, you can include as many stages as
                        you need for more complex processing. </p>
</li>

            </ul>
</div>

        <p class="p">For a case study that describes a couple ways to use pipeline events, see <a class="xref" href="EventFramework-Title.html#concept_vrh_jrs_bbb">Case Study: Offloading Data from Relational Sources to Hadoop</a>.</p>

    </div>

<div class="topic concept nested2" id="concept_ggx_mtr_t1b">
 <h3 class="title topictitle3">Using Pipeline Events</h3>

 <div class="body conbody">
  <p class="p">You can configure pipeline
            events to be consumed by an executor or another pipeline. </p>

        <p class="p">Pass an event to an executor when the executor can perform all of the tasks that you
            need. You can configure one executor for each event type.</p>

        <p class="p">Pass an event to another pipeline when you need to perform more complex tasks in the
            consuming pipeline, such as passing the event to multiple executors or to an executor
            and destination for storage. </p>

 </div>

<div class="topic concept nested3" id="concept_shm_2cy_t1b">
 <h4 class="title topictitle4">Pass to an Executor</h4>

 <div class="body conbody">
  <p class="p">You can
            configure a pipeline to pass each event type to an executor stage. This allows you to
            trigger a task when the pipeline starts or stops. You configure the behavior for each
            event type separately. And you can discard any event that you do not want to use.</p>

        <div class="p">
            <div class="note note"><span class="notetitle">Note:</span> If the specified executor fails to process the event, for example if a Shell
                executor fails to execute a script, the pipeline transitions to a failure
                state.</div>

        </div>

        <div class="p">To pass a pipeline event to the executor, perform the following steps:<ol class="ol" id="concept_shm_2cy_t1b__ol_zmf_13y_t1b">
                <li class="li">In the pipeline properties, select the executor that you want to consume the
                    event.</li>

                <li class="li">In the pipeline properties, configure the executor to perform the task.</li>

            </ol>
</div>

        <div class="section" id="concept_shm_2cy_t1b__section_lw4_jgj_v1b"><h5 class="title sectiontitle">Example</h5>
            
            <p class="p">Say you want to send an email when the pipeline starts. First, you configure the
                pipeline to use the Email executor for the pipeline start event. Since you don't
                need the Stop event, you can simply use the default discard option: </p>

            <p class="p"><img class="image" id="concept_shm_2cy_t1b__image_ftv_kjy_t1b" src="../Graphics/PEvent-Executor.png" height="317" width="527" /></p>

            <p class="p">Then, also in the pipeline properties, you configure the Email executor. You can
                configure a condition for the email to be sent. If you omit the condition, the
                executor sends the email each time it receives an event:</p>

            <p class="p"><img class="image" id="concept_shm_2cy_t1b__image_x4y_3my_t1b" src="../Graphics/PEvent-ConfigConsumer.png" height="267" width="574" /></p>

        </div>

 </div>

</div>
<div class="topic concept nested3" id="concept_sbh_fcy_t1b">
 <h4 class="title topictitle4">Pass to Another Pipeline</h4>

 <div class="body conbody">
  <p class="p">Pass
            pipeline events to another pipeline to perform more complex processing than simply
            passing the event to a single consumer. The event-consuming pipeline must use the SDC
            RPC origin, then can include as many other stages as you require. </p>

        <div class="note note"><span class="notetitle">Note:</span> When you pass a pipeline event to another pipeline, the event-consuming pipeline does
            not report processing failures back to the event-generating pipeline automatically. For
            example, if you pass a pipeline event to a pipeline that has an executor that fails to
            complete its task, the failure is not reported back to the event-generating
            pipeline.</div>

        <p class="p">To achieve the same behavior as passing to an executor, where a processing failure causes
            the event-generating pipeline to fail, configure the relevant stages to stop the
            event-consuming pipeline upon error. Upon error, the event-consuming pipeline then stops
            and passes the message back to the event-generating pipeline, which then transitions to
            a failure state.</p>

        <p class="p">For example, say you pass a pipeline event to a pipeline that routes the event to two
            executors. To ensure that the event-generating pipeline fails if either of the executors
            fail, configure the On Record Error property on the General tab of both executors,
            setting the property to Stop Pipeline. </p>

        <p class="p">This causes the event-consuming pipeline to stop on error, which causes the
            event-generating pipeline to transition to a failure state.</p>

        <div class="p">To pass an event to another pipeline, perform the following steps:<ol class="ol" id="concept_sbh_fcy_t1b__ol_uvt_pgy_t1b">
                <li class="li">Configure the pipeline to consume the event.</li>

                <li class="li">Configure the event-generating pipeline to pass the event to the event-consuming
                    pipeline, including details from the SDC RPC origin.</li>

                <li class="li">Start the event-consuming pipeline before you start the event-generating
                    pipeline.</li>

            </ol>
</div>

        <div class="section" id="concept_sbh_fcy_t1b__section_bwq_pjj_v1b"><h5 class="title sectiontitle">Example</h5>
            
            <p class="p">Say you want the Stop event to trigger a shell script that kicks off another process
                and a JDBC query. To do this, first configure the event-consuming pipeline. Use an
                SDC RPC origin and note the highlighted properties, because you will use them to
                configure the event-generating pipeline: </p>

            <p class="p"><img class="image" id="concept_sbh_fcy_t1b__image_llr_cxx_t1b" src="../Graphics/PEvents-AnotherPipe.png" height="306" width="371" /></p>

            <p class="p">Then you configure the event-generating pipeline to pass the Stop event to your new
                pipeline. Note that if you don't need to use the Start event, you can simply use the
                default discard option:</p>

            <p class="p"><img class="image" id="concept_sbh_fcy_t1b__image_ugk_sxx_t1b" src="../Graphics/PEvent-StopEventConfig.png" height="301" width="469" /></p>

            <p class="p">Then you configure the Stop Event - Write to Another Pipeline properties, using the
                SDC RPC details from your event-consuming pipeline: </p>

            <p class="p"><img class="image" id="concept_sbh_fcy_t1b__image_wb5_fyx_t1b" src="../Graphics/PEvent-Stop-PipelineConfig.png" height="298" width="477" /></p>

        </div>

 </div>

</div>
</div>
</div>
<div class="topic concept nested1" id="concept_zrl_mhn_lx">
 <h2 class="title topictitle2">Stage Event Generation</h2>

 <div class="body conbody">
  <p class="p">You can configure certain
            stages to generate events. Event generation differs from stage to stage, based on the
            way the stage processes data. For details about each the event generation for each
            stage, see "Event Generation" in the stage documentation.</p>

        <div class="p">The following table lists event-generating stages and when they can generate events:
                
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="concept_zrl_mhn_lx__table_pzz_thf_lx" class="table" frame="border" border="1" rules="all">
                    
                    
                    <thead class="thead" align="left">
                        <tr>
                            <th class="entry" valign="top" width="30%" id="d157336e393">Stage</th>

                            <th class="entry" valign="top" width="70%" id="d157336e396">Generates events when the stage...</th>

                        </tr>

                    </thead>

                    <tbody class="tbody">
                        <tr>
                            <td class="entry" valign="top" width="30%" headers="d157336e393 ">Directory origin</td>

                            <td class="entry" valign="top" width="70%" headers="d157336e396 ">
                                <ul class="ul" id="concept_zrl_mhn_lx__ul_opf_5ck_px">
                                    <li class="li">Starts processing a file.</li>

                                    <li class="li">Completes processing a file. </li>

                                    <li class="li">Completes processing all available files and the configured
                                        batch wait time has elapsed.</li>

                                </ul>

                                <p class="p">For more information, see <a class="xref" href="../Origins/Directory.html#concept_ttg_vgn_qx">"Event Generation" for the Directory origin</a>. </p>

                            </td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="30%" headers="d157336e393 ">File Tail origin</td>

                            <td class="entry" valign="top" width="70%" headers="d157336e396 ">
                                <ul class="ul" id="concept_zrl_mhn_lx__ul_as2_12n_qx">
                                    <li class="li">Starts processing a file.</li>

                                    <li class="li">Completes processing a file. </li>

                                </ul>

                                <p class="p">For more information, see <a class="xref" href="../Origins/FileTail.html#concept_gwn_c32_px">"Event Generation" for the File Tail origin</a>. </p>

                            </td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="30%" headers="d157336e393 ">Google BigQuery</td>

                            <td class="entry" valign="top" width="70%" headers="d157336e396 ">
                                <ul class="ul" id="concept_zrl_mhn_lx__ul_rns_2jx_q1b">
                                    <li class="li">Successfully completes a query.</li>

                                </ul>

                                <p class="p">For more information, see <a class="xref" href="../Origins/BigQuery.html#concept_vsm_khx_q1b">"Event
                                        Generation" for the Google BigQuery origin</a>. </p>

                            </td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="30%" headers="d157336e393 ">JDBC Query Consumer origin</td>

                            <td class="entry" valign="top" width="70%" headers="d157336e396 ">
                                <ul class="ul" id="concept_zrl_mhn_lx__ul_fsn_ct5_lz">
                                    <li class="li">Completes processing all data returned by a query. </li>

                                    <li class="li">Successfully completes a query.</li>

                                    <li class="li">Fails to complete a query.</li>

                                </ul>

                                <p class="p">For more information, see <a class="xref" href="../Origins/JDBCConsumer.html#concept_o1c_kwr_kz">"Event Generation"
                                        for the JDBC Query Consumer origin</a>. </p>

                            </td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="30%" headers="d157336e393 ">JDBC Multitable Consumer origin</td>

                            <td class="entry" valign="top" width="70%" headers="d157336e396 ">
                                <ul class="ul">
                                    <li class="li">Completes processing the data returned by the queries for
                                        all tables.</li>

                                </ul>

                                <p class="p">For more information, see <a class="xref" href="../Origins/MultiTableJDBCConsumer.html#concept_wjj_gzs_kz">"Event
                                        Generation" for the JDBC Multitable Consumer origin</a>. </p>

                            </td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="30%" headers="d157336e393 ">Oracle CDC Client origin</td>

                            <td class="entry" valign="top" width="70%" headers="d157336e396 ">
                                <ul class="ul" id="concept_zrl_mhn_lx__ul_brj_zv1_vy">
                                    <li class="li">Reads DDL statements in the redo log. </li>

                                </ul>

                                <p class="p">For more information, see <a class="xref" href="../Origins/OracleCDC.html#concept_h2t_hx1_vy">"Event
                                        Generation" for the Oracle CDC Client origin</a>. </p>

                            </td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="30%" headers="d157336e393 ">Salesforce origin</td>

                            <td class="entry" valign="top" width="70%" headers="d157336e396 ">
                                <ul class="ul" id="concept_zrl_mhn_lx__ul_djn_rfb_mz">
                                    <li class="li">Completes processing all data returned by a query. </li>

                                </ul>

                                <p class="p">For more information, see <a class="xref" href="../Origins/Salesforce.html#concept_cvb_bvr_kz">"Event
                                    Generation" for the Salesforce origin</a>. </p>

                            </td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="30%" headers="d157336e393 ">SQL Server CDC Client origin</td>

                            <td class="entry" valign="top" width="70%" headers="d157336e396 ">
                                <ul class="ul" id="concept_zrl_mhn_lx__ul_dfr_cxg_x1b">
                                    <li class="li">Completes processing the data in the associated CDC
                                        tables.</li>

                                </ul>

                                <p class="p">For more information, see <a class="xref" href="../Origins/SQLServerCDC.html#concept_byp_dgv_s1b">"Event
                                    Generation" for the SQL Server CDC Client origin</a>. </p>

                            </td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="30%" headers="d157336e393 ">SQL Server Change Tracking origin</td>

                            <td class="entry" valign="top" width="70%" headers="d157336e396 ">
                                <ul class="ul" id="concept_zrl_mhn_lx__ul_jmt_5gv_s1b">
                                    <li class="li">Completes processing the data in all specified change
                                        tracking tables.</li>

                                </ul>

                                <p class="p">For more information, see <a class="xref" href="../Origins/SQLServerChange.html#concept_byp_dgv_s1b">"Event
                                    Generation" for the SQL Server Change Tracking origin</a>. </p>

                            </td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="30%" headers="d157336e393 ">Groovy Evaluator processor</td>

                            <td class="entry" valign="top" width="70%" headers="d157336e396 ">
                                <ul class="ul" id="concept_zrl_mhn_lx__ul_cj5_tnp_1y">
                                    <li class="li">Runs a script that generates events.</li>

                                </ul>

                                <p class="p">For more information, see <a class="xref" href="../Processors/Groovy.html#concept_qcz_ssq_1y">"Event
                                    Generation" for the Groovy Evaluator processor</a>. </p>

                            </td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="30%" headers="d157336e393 ">JavaScript Evaluator processor</td>

                            <td class="entry" valign="top" width="70%" headers="d157336e396 ">
                                <ul class="ul" id="concept_zrl_mhn_lx__ul_qtz_vnp_1y">
                                    <li class="li">Runs a script that generates events.</li>

                                </ul>

                                <p class="p">For more information, see <a class="xref" href="../Processors/JavaScript.html#concept_mkv_wgh_cy">"Event
                                    Generation" for the JavaScript Evaluator processor</a>. </p>

                            </td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="30%" headers="d157336e393 ">Jython Evaluator processor</td>

                            <td class="entry" valign="top" width="70%" headers="d157336e396 ">
                                <ul class="ul" id="concept_zrl_mhn_lx__ul_ujt_wnp_1y">
                                    <li class="li">Runs a script that generates events.</li>

                                </ul>

                                <p class="p">For more information, see <a class="xref" href="../Processors/Jython.html#concept_zhd_chh_cy">"Event
                                    Generation" for the Jython Evaluator processor</a>. </p>

                            </td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="30%" headers="d157336e393 ">Amazon S3 destination</td>

                            <td class="entry" valign="top" width="70%" headers="d157336e396 ">
                                <ul class="ul" id="concept_zrl_mhn_lx__ul_lb3_zck_px">
                                    <li class="li">Completes writing to an object.</li>

                                    <li class="li">Completes streaming a whole file.</li>

                                </ul>

                                <p class="p">For more information, see <a class="xref" href="../Destinations/AmazonS3.html#concept_aqq_tt2_px">"Event
                                    Generation" for the Amazon S3 destination</a>. </p>

                            </td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="30%" headers="d157336e393 ">Azure Data Lake Store destination</td>

                            <td class="entry" valign="top" width="70%" headers="d157336e396 ">
                                <ul class="ul" id="concept_zrl_mhn_lx__ul_wny_nw4_vz">
                                    <li class="li">Closes a file.</li>

                                    <li class="li">Completes streaming a whole file.</li>

                                </ul>

                                <p class="p">For more information, see <a class="xref" href="../Destinations/DataLakeStore.html#concept_eck_fm3_vz">"Event
                                    Generation" for the Azure Data Lake Store destination</a>. </p>

                            </td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="30%" headers="d157336e393 ">Hadoop FS destination</td>

                            <td class="entry" valign="top" width="70%" headers="d157336e396 ">
                                <ul class="ul" id="concept_zrl_mhn_lx__ul_x5c_cdk_px">
                                    <li class="li">Closes a file.</li>

                                    <li class="li">Completes streaming a whole file.</li>

                                </ul>

                                <p class="p">For more information, see <a class="xref" href="../Destinations/HadoopFS-destination.html#concept_bvb_rxj_px">"Event
                                    Generation" for the Hadoop FS destination</a>. </p>

                            </td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="30%" headers="d157336e393 ">Hive Metastore destination</td>

                            <td class="entry" valign="top" width="70%" headers="d157336e396 ">
                                <ul class="ul" id="concept_zrl_mhn_lx__ul_fgw_mt4_qx">
                                    <li class="li">Updates the Hive metastore by creating a table, adding
                                        columns, or creating a partition. </li>

                                    <li class="li">Generates and writes a new Avro schema file.</li>

                                </ul>

                                <p class="p">For more information, see <a class="xref" href="../Destinations/HiveMetastore.html#concept_drg_lwc_rx">"Event
                                    Generation" for the Hive Metastore destination</a>. </p>

                            </td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="30%" headers="d157336e393 ">Local FS destination</td>

                            <td class="entry" valign="top" width="70%" headers="d157336e396 ">
                                <ul class="ul" id="concept_zrl_mhn_lx__ul_flq_ddk_px">
                                    <li class="li">Closes a file.</li>

                                    <li class="li">Completes streaming a whole file.</li>

                                </ul>

                                <p class="p">For more information, see <a class="xref" href="../Destinations/LocalFS.html#concept_in1_fcm_px">"Event
                                    Generation" for the Local FS destination</a>. </p>

                            </td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="30%" headers="d157336e393 ">MapR FS destination</td>

                            <td class="entry" valign="top" width="70%" headers="d157336e396 ">
                                <ul class="ul" id="concept_zrl_mhn_lx__ul_m11_gdk_px">
                                    <li class="li">Closes a file.</li>

                                    <li class="li">Completes streaming a whole file.</li>

                                </ul>

                                <p class="p">For more information, see <a class="xref" href="../Destinations/MapRFS.html#concept_bqd_3qb_rx">"Event
                                    Generation" for the MapR FS destination</a>. </p>

                            </td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="30%" headers="d157336e393 ">HDFS File Metadata executor</td>

                            <td class="entry" valign="top" width="70%" headers="d157336e396 ">
                                <ul class="ul" id="concept_zrl_mhn_lx__ul_fmy_4t4_qx">
                                    <li class="li">Changes file metadata, such as the file name, location, or
                                        permissions.</li>

                                    <li class="li">Creates an empty file.</li>

                                    <li class="li">Removes a file or directory.</li>

                                </ul>

                                <p class="p">For more information, see <a class="xref" href="../Executors/HDFSMetadata.html#concept_vhl_mfj_rx">"Event
                                    Generation" for the HDFS File Metadata executor</a>. </p>

                            </td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="30%" headers="d157336e393 ">Hive Query executor</td>

                            <td class="entry" valign="top" width="70%" headers="d157336e396 ">
                                <ul class="ul" id="concept_zrl_mhn_lx__ul_qsl_bsk_my">
                                    <li class="li">Determines that the submitted query completed
                                        successfully.</li>

                                    <li class="li">Determines that the submitted query failed to complete.</li>

                                </ul>

                                <p class="p">For more information, see <a class="xref" href="../Executors/HiveQuery.html#concept_arl_xx3_my">"Event
                                    Generation" for the Hive Query executor</a>. </p>

                            </td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="30%" headers="d157336e393 ">MapR FS File Metadata executor</td>

                            <td class="entry" valign="top" width="70%" headers="d157336e396 ">
                                <ul class="ul">
                                    <li class="li">Changes file metadata, such as the file name, location, or
                                        permissions.</li>

                                    <li class="li">Creates an empty file.</li>

                                    <li class="li">Removes a file or directory.</li>

                                </ul>

                                <p class="p">For more information, see <a class="xref" href="../Executors/MapRFSFileMeta.html#concept_vhl_mfj_rx">"Event
                                    Generation" for the MapR FS File Metadata executor</a>. </p>

                            </td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="30%" headers="d157336e393 ">MapReduce executor</td>

                            <td class="entry" valign="top" width="70%" headers="d157336e396 ">
                                <ul class="ul" id="concept_zrl_mhn_lx__ul_fy2_qt4_qx">
                                    <li class="li">Starts a MapReduce job.</li>

                                </ul>

                                <p class="p">For more information, see <a class="xref" href="../Executors/MapReduce.html#concept_e1s_sm5_sx">"Event
                                    Generation" for the MapReduce executor</a>. </p>

                            </td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="30%" headers="d157336e393 ">Spark executor</td>

                            <td class="entry" valign="top" width="70%" headers="d157336e396 ">
                                <ul class="ul" id="concept_zrl_mhn_lx__ul_xyc_hyg_gz">
                                    <li class="li">Starts a Spark application.</li>

                                </ul>

                                <p class="p">For more information, see <a class="xref" href="../Executors/Spark.html#concept_xmx_1wg_gz">"Event
                                    Generation" for the Spark executor</a>. </p>

                            </td>

                        </tr>

                    </tbody>

                </table>
</div>
</div>

 </div>

<div class="topic concept nested2" id="concept_sjr_nrx_4x">
 <h3 class="title topictitle3">Using Stage Events</h3>

 <div class="body conbody">
        <p class="p">You can use
            stage-related events in any way that suits your needs. When configuring the event stream
            for stage events, you can add additional stages to the stream. For example, you might
            use a Stream Selector to route different types of events to different executors. But you
            cannot merge the event stream with a data stream. </p>

        <div class="p">There are two general types of event streams that you might create: <ul class="ul" id="concept_sjr_nrx_4x__ul_pr4_qz2_zx">
                <li class="li">Task execution streams that route events to an executor to perform a task. </li>

                <li class="li">Event storage streams that route events to a destination to store event
                    information.</li>

            </ul>
You can, of course, configure an event stream that performs both tasks by routing
            event records to both an executor and a destination. You can also configure event
            streams to route data to multiple executors and destinations, as needed.</div>

 </div>

<div class="topic concept nested3" id="concept_ivh_xy2_zx">
 <h4 class="title topictitle4">Task Execution Streams</h4>

 <div class="body conbody">
  <p class="p">A task execution stream
            routes event records from the event-generating stage to an executor stage. The executor
            performs a task each time it receives an event record. </p>

        <p class="p">For example, you have a pipeline that reads from Kafka and writes files to HDFS:</p>

        <p class="p"><img class="image" id="concept_ivh_xy2_zx__image_b5z_25q_tx" src="../Graphics/Event-ParquetBasicPipe.png" height="81" width="581" /></p>

        <p class="p">When Hadoop FS closes a file, you would like the file moved to a different directory and
            the file permissions changed to read-only. </p>

        <p class="p">Leaving the rest of the pipeline as is, you can enable event handling in the Hadoop FS
            destination, connect it to the HDFS File Metadata executor, and configure the HDFS File
            Metadata executor to files and change permissions. The resulting pipeline looks like
            this:</p>

        <p class="p"><img class="image" id="concept_ivh_xy2_zx__image_c5z_25q_tx" src="../Graphics/Event-EventPipe.png" height="150" width="635" /></p>

        <p class="p">If you needed to set permissions differently based on the file name or location, you
            could use a Stream Selector to route the event records accordingly, then use two HDFS
            File Metadata executors to alter file permissions, as follows:</p>

        <p class="p"><img class="image" id="concept_ivh_xy2_zx__image_os2_nqf_zx" src="../Graphics/Event-EventPipe-SSelector.png" height="160" width="639" /></p>

 </div>

</div>
<div class="topic concept nested3" id="concept_bfd_31f_zx">
 <h4 class="title topictitle4">Event Storage Streams</h4>

 <div class="body conbody">
        <p class="p">An event storage stream routes event records from the
            event-generating stage to a destination. The destination writes the event record to a
            destination system.</p>

        <p class="p">Event records include information about the event in record header attributes and record
            fields. You can add processors to the event stream to enrich the event record before
            writing it to the destination. </p>

        <p class="p">For example, you have a pipeline that uses the Directory origin to process weblogs:</p>

        <p class="p"><img class="image" id="concept_bfd_31f_zx__image_g5w_m1f_zx" src="../Graphics/Event-Directory.png" height="98" width="637" /></p>

        <p class="p">Directory generates event records each time it starts and completes reading a file, and
            the event record includes a field with the file path of the file. For auditing purposes,
            you'd like to write this information to a database table.</p>

        <p class="p">Leaving the rest of the pipeline as is, you can enable event handling for the Directory
            origin and simply connect it to the JDBC Producer as follows:</p>

        <p class="p"><img class="image" id="concept_bfd_31f_zx__image_d2y_1vq_tx" src="../Graphics/Event-Directory-JDBC.png" height="166" width="540" /></p>

        <div class="p">But you want to know when events occur. The Directory event record stores the event
            creation time in the sdc.event.creation_timestamp record header attribute. So you can
            use an Expression Evaluator with the following expression to add the creation date and
            time to the record:
            <pre class="pre codeblock">${record:attribute('sdc.event.creation_timestamp')}</pre>
</div>

        <div class="p">And if you have multiple pipelines writing events to the same location, you can use the
            following expression to include the pipeline name in the event record as
            well:<pre class="pre codeblock">${pipeline:name()}</pre>
</div>

        <p class="p">The Expression Evaluator and the final pipeline looks like this:</p>

        <p class="p"><img class="image" id="concept_bfd_31f_zx__image_e2y_1vq_tx" src="../Graphics/Event-Directory-ExpJDBC.png" height="328" width="572" /></p>

    </div>

</div>
</div>
</div>
<div class="topic concept nested1" id="concept_rxg_shn_lx">
 <h2 class="title topictitle2">Executors</h2>

 <div class="body conbody">
        <div class="p">
            Executors perform
            tasks when they receive event records. You can use the following executor stages for
            event handling:<dl class="dl">
                
                    <dt class="dt dlterm">Amazon S3 executor</dt>

                    <dd class="dd">Creates new Amazon S3 objects for the specified content or adds tags to
                        existing Amazon S3 objects upon receiving an event. </dd>

                    <dd class="dd"><span class="ph">You can use the executor in any logical way, such as writing
                        information from an event record to an S3 object or tagging objects after
                        they are written by the Amazon S3 destination.</span></dd>

                
                
                    <dt class="dt dlterm">Email executor</dt>

                    <dd class="dd">Sends a custom email to the configured recipients upon receiving an event.
                        You can optionally configure a condition that determines when to send the
                        email. </dd>

                    <dd class="dd">You can use the executor in any logical way, such as sending an email each
                        time the Azure Data Lake Store destination completes streaming a whole file.
                    </dd>

                
                
                    <dt class="dt dlterm">Hive Query executor</dt>

                    <dd class="dd">Executes user-defined Hive or Impala queries for each event.</dd>

                    <dd class="dd"><span class="ph">You can use the executor in any logical way, such as
                        running Hive or Impala queries after the Hive Metadata destination updates
                        the Hive metastore, or after the Hadoop FS or MapR FS destination closes
                        files.</span></dd>

                    <dd class="dd">For example, you might use the Hive Query executor as part of the <a class="xref" href="../Hive_Drift_Solution/HiveDriftSolution_title.html#concept_phk_bdf_2w">Drift Synchronization Solution for Hive</a> if you read data with
                        Impala. Impala requires you to run the Invalidate Metadata command when the
                        table structure or data changes. </dd>

                    <dd class="dd">Instead of trying to time this action manually, you can use the Hive Query
                        executor to submit the command automatically each time the Hive Metastore
                        destination changes the structure of a table and each time the Hadoop FS
                        destination closes a file. </dd>

                
                
                    <dt class="dt dlterm">HDFS File Metadata executor</dt>

                    <dd class="dd">Changes file metadata, creates an empty file, or removes a file or directory
                        from HDFS or a local file system upon receiving an event. <p class="p">When changing
                            file metadata, the executor can rename and move files in addition to
                            specifying the owner and group, and updating permissions and ACLs for
                            files. When creating an empty file, the executor can specify the owner
                            and group and set permissions and ACLs for the file. When removing files
                            and directories, the executor performs the task recursively.</p>
</dd>

                    <dd class="dd"><span class="ph">You can use the executor in any logical way, such as
                        changing file metadata after receiving file closure events from the Hadoop
                        FS or Local FS destinations.</span>
                    </dd>

                
                
                    <dt class="dt dlterm">Pipeline Finisher executor</dt>

                    <dd class="dd">Stops the pipeline when it receives an event, transitioning the pipeline to
                        a Finished state. Allows the pipeline to complete all expected processing
                        before stopping.</dd>

                    <dd class="dd"><span class="ph">You can use the Pipeline Finisher in any logical way, such as
                        stopping a pipeline upon receiving a no-more-data event from the JDBC Query
                        Consumer origin.</span> This enables you to achieve "batch" processing - stopping the pipeline
                        when all available data is processed rather than leaving the pipeline to sit
                        idle indefinitely. </dd>

                    <dd class="dd">For example, you might use the Pipeline Finisher executor with the JDBC
                        Multitable Consumer to stop the pipeline when it processes all queried data
                        in the specified tables.</dd>

                
                
                    <dt class="dt dlterm">JDBC Query executor</dt>

                    <dd class="dd">Connects to a database using JDBC and runs the specified SQL query. </dd>

                    <dd class="dd">Use to run a SQL query on a database after an event occurs.</dd>

                
                
                    <dt class="dt dlterm">MapR FS File Metadata executor</dt>

                    <dd class="dd">Changes file metadata, creates an empty file, or removes a file or directory
                        in MapR FS upon receiving an event. When changing file metadata, the
                        executor can rename and move files in addition to specifying the owner and
                        group, and updating permissions and ACLs for files. When creating an empty
                        file, the executor can specify the owner and group and set permissions and
                        ACLs for the file. When removing files and directories, the executor
                        performs the task recursively.</dd>

                    <dd class="dd"><span class="ph">You can use the executor in any logical way, such as
                        creating an empty file after the MapR FS destination closes a file.</span>
                    </dd>

                
                
                    <dt class="dt dlterm">MapReduce executor</dt>

                    <dd class="dd">Connects to HDFS or MapR FS and starts a MapReduce job for each event. </dd>

                    <dd class="dd"><span class="ph">You can use the executor in any logical way, such as running
                        MapReduce jobs after the Hadoop FS or MapR FS destination closes files.</span> For example, you might use the MapReduce executor with the Hadoop FS
                        destination to convert Avro files to Parquet when Hadoop FS closes a file. </dd>

                
                
                    <dt class="dt dlterm">Shell executor</dt>

                    <dd class="dd">Executes a user-defined shell script for each event. </dd>

                
                
                    <dt class="dt dlterm">Spark executor</dt>

                    <dd class="dd">Connects to Spark on YARN or Databricks and starts a Spark application for
                        each event.</dd>

                    <dd class="dd"><span class="ph">You can use the executor in any logical way, such as running Spark
                        applications after the Hadoop FS, MapR FS, or Amazon S3 destination closes
                        files.</span> For example, you might use the Spark executor with the Hadoop FS
                        destination to convert Avro files to Parquet when Hadoop FS closes a file. </dd>

                
            </dl>
</div>

 </div>

</div>
<div class="topic concept nested1" id="concept_scs_3hh_tx">
 <h2 class="title topictitle2">Logical Pairings</h2>

 <div class="body conbody">
  <p class="p">You can use events in
            any way that works for your needs. The following tables outline some logical pairings of
            event generation with executors and destinations.</p>

        <div class="section" id="concept_scs_3hh_tx__section_nnn_2ng_x1b"><h3 class="title sectiontitle">Pipeline Events</h3>
            
            <div class="p">
                
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="concept_scs_3hh_tx__table_cyw_55r_t1b" class="table" frame="border" border="1" rules="all">
                        
                        
                        <thead class="thead" align="left">
                            <tr>
                                <th class="entry" valign="top" width="30%" id="d157336e1307">Pipeline Event Type</th>

                                <th class="entry" valign="top" width="70%" id="d157336e1310">Event Consumer</th>

                            </tr>

                        </thead>

                        <tbody class="tbody">
                            <tr>
                                <td class="entry" valign="top" width="30%" headers="d157336e1307 ">Pipeline Start</td>

                                <td class="entry" valign="top" width="70%" headers="d157336e1310 ">
                                    <ul class="ul" id="concept_scs_3hh_tx__ul_vrw_ypy_t1b">
                                        <li class="li">Any single executor, except Pipeline Finisher.</li>

                                        <li class="li">Another pipeline for additional processing.</li>

                                    </ul>

                                </td>

                            </tr>

                            <tr>
                                <td class="entry" valign="top" width="30%" headers="d157336e1307 ">Pipeline Stop</td>

                                <td class="entry" valign="top" width="70%" headers="d157336e1310 ">
                                    <ul class="ul" id="concept_scs_3hh_tx__ul_v2t_1qy_t1b">
                                        <li class="li">Any single executor, except Pipeline Finisher.</li>

                                        <li class="li">Another pipeline for additional processing.</li>

                                    </ul>

                                </td>

                            </tr>

                        </tbody>

                    </table>
</div>

            </div>

        </div>

        <div class="section" id="concept_scs_3hh_tx__section_rdr_fng_x1b"><h3 class="title sectiontitle">Origin Events</h3>
            
            <div class="p">
                
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="concept_scs_3hh_tx__table_pwj_d4p_1y" class="table" frame="border" border="1" rules="all">
                        
                        
                        <thead class="thead" align="left">
                            <tr>
                                <th class="entry" valign="top" width="30%" id="d157336e1377">Event Generating Origin</th>

                                <th class="entry" valign="top" width="70%" id="d157336e1380">Event Consumer</th>

                            </tr>

                        </thead>

                        <tbody class="tbody">
                            <tr>
                                <td class="entry" valign="top" width="30%" headers="d157336e1377 ">Directory</td>

                                <td class="entry" valign="top" width="70%" headers="d157336e1380 ">
                                    <ul class="ul" id="concept_scs_3hh_tx__ul_wvy_h4p_1y">
                                        <li class="li">Email executor to send email each time the origin starts
                                            or completes processing a file. </li>

                                        <li class="li">Pipeline Finisher executor to stop the pipeline after
                                            processing all available files.</li>

                                        <li class="li">Any destination for event storage.</li>

                                    </ul>

                                </td>

                            </tr>

                            <tr>
                                <td class="entry" valign="top" width="30%" headers="d157336e1377 ">File Tail</td>

                                <td class="entry" valign="top" width="70%" headers="d157336e1380 ">
                                    <ul class="ul" id="concept_scs_3hh_tx__ul_hgh_34p_1y">
                                        <li class="li">Email executor to send email each time the origin starts
                                            or completes processing a file. </li>

                                        <li class="li">Any destination for event storage.</li>

                                    </ul>

                                </td>

                            </tr>

                            <tr>
                                <td class="entry" valign="top" width="30%" headers="d157336e1377 ">Google BigQuery</td>

                                <td class="entry" valign="top" width="70%" headers="d157336e1380 ">
                                    <ul class="ul" id="concept_scs_3hh_tx__ul_u3c_hng_x1b">
                                        <li class="li">Email executor to send email each time the origin
                                            successfully completes a query.</li>

                                        <li class="li">Any destination for event storage.</li>

                                    </ul>

                                </td>

                            </tr>

                            <tr>
                                <td class="entry" valign="top" width="30%" headers="d157336e1377 ">JDBC Multitable Consumer</td>

                                <td class="entry" valign="top" width="70%" headers="d157336e1380 ">
                                    <ul class="ul" id="concept_scs_3hh_tx__ul_a1m_jx5_lz">
                                        <li class="li">Pipeline Finisher executor to stop the pipeline after
                                            processing queried data from all tables.</li>

                                        <li class="li">Email executor to send email when the origin completes
                                            processing all data returned by queries.</li>

                                        <li class="li">Any destination for event storage.</li>

                                    </ul>

                                </td>

                            </tr>

                            <tr>
                                <td class="entry" valign="top" width="30%" headers="d157336e1377 ">JDBC Query Consumer</td>

                                <td class="entry" valign="top" width="70%" headers="d157336e1380 ">
                                    <ul class="ul" id="concept_scs_3hh_tx__ul_pn2_csc_yz">
                                        <li class="li">Route the no-more-data event to the Pipeline Finisher
                                            executor to stop the pipeline after processing queried
                                            data.</li>

                                        <li class="li">Email executor to send email each time the origin
                                            successfully completes a query, fails to complete a
                                            query, or completes processing all available data.</li>

                                        <li class="li">Any destination for event storage.</li>

                                    </ul>

                                </td>

                            </tr>

                            <tr>
                                <td class="entry" valign="top" width="30%" headers="d157336e1377 ">Oracle CDC Client</td>

                                <td class="entry" valign="top" width="70%" headers="d157336e1380 ">
                                    <ul class="ul" id="concept_scs_3hh_tx__ul_xdj_1x1_vy">
                                        <li class="li">Email executor to send email each time it reads DDL
                                            statements in the redo logs.</li>

                                        <li class="li">Any destination for event storage.</li>

                                    </ul>

                                </td>

                            </tr>

                            <tr>
                                <td class="entry" valign="top" width="30%" headers="d157336e1377 ">Salesforce</td>

                                <td class="entry" valign="top" width="70%" headers="d157336e1380 ">
                                    <ul class="ul" id="concept_scs_3hh_tx__ul_v3c_hng_x1b">
                                        <li class="li">Pipeline Finisher executor to stop the pipeline after
                                            processing queried data.</li>

                                        <li class="li">Email executor to send email when the origin completes
                                            processing all data returned by a query.</li>

                                        <li class="li">Any destination for event storage.</li>

                                    </ul>

                                </td>

                            </tr>

                            <tr>
                                <td class="entry" valign="top" width="30%" headers="d157336e1377 ">SQL Server Change Tracking</td>

                                <td class="entry" valign="top" width="70%" headers="d157336e1380 ">
                                    <ul class="ul" id="concept_scs_3hh_tx__ul_qgj_4hv_s1b">
                                        <li class="li">Pipeline Finisher executor to stop the pipeline after
                                            processing available data.</li>

                                    </ul>

                                </td>

                            </tr>

                        </tbody>

                    </table>
</div>

            </div>

        </div>

        <div class="section" id="concept_scs_3hh_tx__section_bcz_jng_x1b"><h3 class="title sectiontitle">Processor Events</h3>
            
            <div class="p">
                
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="concept_scs_3hh_tx__table_mc4_dfc_yz" class="table" frame="border" border="1" rules="all">
                        
                        
                        <thead class="thead" align="left">
                            <tr>
                                <th class="entry" valign="top" width="30%" id="d157336e1565">Event Generating Processor</th>

                                <th class="entry" valign="top" width="70%" id="d157336e1568">Event Consumer</th>

                            </tr>

                        </thead>

                        <tbody class="tbody">
                            <tr>
                                <td class="entry" valign="top" width="30%" headers="d157336e1565 ">Groovy Evaluator</td>

                                <td class="entry" valign="top" width="70%" headers="d157336e1568 ">
                                    <ul class="ul" id="concept_scs_3hh_tx__ul_qpx_jfc_yz">
                                        <li class="li">Any logical executor. </li>

                                        <li class="li">Any destination for event storage.</li>

                                    </ul>

                                </td>

                            </tr>

                            <tr>
                                <td class="entry" valign="top" width="30%" headers="d157336e1565 ">JavaScript Evaluator</td>

                                <td class="entry" valign="top" width="70%" headers="d157336e1568 ">
                                    <ul class="ul" id="concept_scs_3hh_tx__ul_xsy_kfc_yz">
                                        <li class="li">Any logical executor. </li>

                                        <li class="li">Any destination for event storage.</li>

                                    </ul>

                                </td>

                            </tr>

                            <tr>
                                <td class="entry" valign="top" width="30%" headers="d157336e1565 ">Jython Evaluator</td>

                                <td class="entry" valign="top" width="70%" headers="d157336e1568 ">
                                    <ul class="ul" id="concept_scs_3hh_tx__ul_gl1_lfc_yz">
                                        <li class="li">Any logical executor. </li>

                                        <li class="li">Any destination for event storage.</li>

                                    </ul>

                                </td>

                            </tr>

                        </tbody>

                    </table>
</div>

            </div>

        </div>

        <div class="section" id="concept_scs_3hh_tx__section_rjw_kng_x1b"><h3 class="title sectiontitle">Destination Events</h3>
            
            <div class="p">
                
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="concept_scs_3hh_tx__table_r5m_b3h_tx" class="table" frame="border" border="1" rules="all">
                        
                        
                        <thead class="thead" align="left">
                            <tr>
                                <th class="entry" valign="top" width="30%" id="d157336e1653">Event Generating Destination</th>

                                <th class="entry" valign="top" width="70%" id="d157336e1656">Event Consumer</th>

                            </tr>

                        </thead>

                        <tbody class="tbody">
                            <tr>
                                <td class="entry" valign="top" width="30%" headers="d157336e1653 ">Amazon S3 </td>

                                <td class="entry" valign="top" width="70%" headers="d157336e1656 ">
                                    <ul class="ul" id="concept_scs_3hh_tx__ul_tyv_24p_1y">
                                        <li class="li">Amazon S3 executor to add tags to closed objects or
                                            whole files.</li>

                                        <li class="li">Spark executor to run a Spark application after closing
                                            an object or whole file. </li>

                                        <li class="li">Email executor to send email after closing an object or
                                            whole file. </li>

                                        <li class="li">Any destination for event storage.</li>

                                    </ul>

                                </td>

                            </tr>

                            <tr>
                                <td class="entry" valign="top" width="30%" headers="d157336e1653 ">Azure Data Lake Store</td>

                                <td class="entry" valign="top" width="70%" headers="d157336e1656 ">
                                    <ul class="ul" id="concept_scs_3hh_tx__ul_swb_lng_x1b">
                                        <li class="li">Email executor to send email after closing a file or
                                            streaming a whole file. </li>

                                        <li class="li">Any destination for event storage.</li>

                                    </ul>

                                </td>

                            </tr>

                            <tr>
                                <td class="entry" valign="top" width="30%" headers="d157336e1653 ">Hadoop FS</td>

                                <td class="entry" valign="top" width="70%" headers="d157336e1656 ">
                                    <ul class="ul" id="concept_scs_3hh_tx__ul_hbx_blh_tx">
                                        <li class="li">HDFS File Metadata executor to change file metadata,
                                            create an empty file, or remove a file or directory
                                            after closing a file.</li>

                                        <li class="li">Hive Query executor to run Hive or Impala queries after
                                            closing a file. <p class="p">Particularly useful when using the
                                                Drift Synchronization Solution for Hive with Impala.
                                            </p>
</li>

                                        <li class="li">MapReduce executor to run a MapReduce job after closing
                                            a file.</li>

                                        <li class="li">Spark executor to run a Spark application after closing
                                            a file. </li>

                                        <li class="li">Email executor to send email after closing a file or
                                            streaming a whole file. </li>

                                        <li class="li">Any destination for event storage.</li>

                                    </ul>

                                </td>

                            </tr>

                            <tr>
                                <td class="entry" valign="top" width="30%" headers="d157336e1653 ">Hive Metastore </td>

                                <td class="entry" valign="top" width="70%" headers="d157336e1656 ">
                                    <ul class="ul" id="concept_scs_3hh_tx__ul_kkj_dlh_tx">
                                        <li class="li">Hive Query executor to run Hive or Impala queries after
                                            the destination changes table structures.
                                                <p class="p">Particularly useful when using the Drift
                                                Synchronization Solution for Hive with Impala.
                                            </p>
</li>

                                        <li class="li">HDFS File Metadata executor to change file metadata,
                                            create an empty file, or remove a file or directory
                                            after writing an Avro schema file.</li>

                                        <li class="li">Email executor to send email each time the destination
                                            changes the Hive metastore.</li>

                                        <li class="li">Any destination for event storage.</li>

                                    </ul>

                                </td>

                            </tr>

                            <tr>
                                <td class="entry" valign="top" width="30%" headers="d157336e1653 ">Local FS</td>

                                <td class="entry" valign="top" width="70%" headers="d157336e1656 ">
                                    <ul class="ul" id="concept_scs_3hh_tx__ul_p3l_glh_tx">
                                        <li class="li">HDFS File Metadata executor to change file metadata,
                                            create an empty file, or remove a file or directory
                                            after closing a file.</li>

                                        <li class="li">Email executor to send email after the destination
                                            closes a file or streams a whole file. </li>

                                        <li class="li">Any destination for event storage.</li>

                                    </ul>

                                </td>

                            </tr>

                            <tr>
                                <td class="entry" valign="top" width="30%" headers="d157336e1653 ">MapR FS</td>

                                <td class="entry" valign="top" width="70%" headers="d157336e1656 ">
                                    <ul class="ul" id="concept_scs_3hh_tx__ul_rvh_dsq_tx">
                                        <li class="li">MapR FS File Metadata executor to change file metadata,
                                            create an empty file, or remove a file or directory
                                            after closing a file.</li>

                                        <li class="li">MapReduce executor to run a MapReduce job after closing
                                            a file.</li>

                                        <li class="li">Spark executor to run a Spark application after closing
                                            a file. </li>

                                        <li class="li">Email executor to send email each time the destination
                                            closes a file or streams a whole file. </li>

                                        <li class="li">Any destination for event storage.</li>

                                    </ul>

                                </td>

                            </tr>

                        </tbody>

                    </table>
</div>

            </div>

        </div>

        <div class="section" id="concept_scs_3hh_tx__section_tjh_lng_x1b"><h3 class="title sectiontitle">Executor Events</h3>
            
            <div class="p">
                
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="concept_scs_3hh_tx__table_sc4_4nh_tx" class="table" frame="border" border="1" rules="all">
                        
                        
                        <thead class="thead" align="left">
                            <tr>
                                <th class="entry" valign="top" width="30%" id="d157336e1838">Event Generating Executor</th>

                                <th class="entry" valign="top" width="70%" id="d157336e1841">Event Consumer</th>

                            </tr>

                        </thead>

                        <tbody class="tbody">
                            <tr>
                                <td class="entry" valign="top" width="30%" headers="d157336e1838 ">HDFS File Metadata executor</td>

                                <td class="entry" valign="top" width="70%" headers="d157336e1841 ">
                                    <ul class="ul" id="concept_scs_3hh_tx__ul_i4j_wfc_yz">
                                        <li class="li">Email executor to send email each time the executor
                                            changes file metadata.</li>

                                        <li class="li">Any destination for event storage.</li>

                                    </ul>

                                </td>

                            </tr>

                            <tr>
                                <td class="entry" valign="top" width="30%" headers="d157336e1838 ">Hive Query executor</td>

                                <td class="entry" valign="top" width="70%" headers="d157336e1841 ">
                                    <ul class="ul" id="concept_scs_3hh_tx__ul_xmw_dk3_xz">
                                        <li class="li">Email executor to send email when a query succeeds or
                                            fails.</li>

                                        <li class="li">Any destination for event storage.</li>

                                    </ul>

                                </td>

                            </tr>

                            <tr>
                                <td class="entry" valign="top" width="30%" headers="d157336e1838 ">MapR FS File Metadata executor</td>

                                <td class="entry" valign="top" width="70%" headers="d157336e1841 ">
                                    <ul class="ul" id="concept_scs_3hh_tx__ul_mjp_tr4_2bb">
                                        <li class="li">Email executor to send email each time the executor
                                            changes file metadata.</li>

                                        <li class="li">Any destination for event storage.</li>

                                    </ul>

                                </td>

                            </tr>

                            <tr>
                                <td class="entry" valign="top" width="30%" headers="d157336e1838 ">MapReduce executor</td>

                                <td class="entry" valign="top" width="70%" headers="d157336e1841 ">
                                    <ul class="ul" id="concept_scs_3hh_tx__ul_qlz_lng_x1b">
                                        <li class="li">Email executor to send email each time the executor
                                            starts a MapReduce job. </li>

                                        <li class="li">Any destination for event storage.</li>

                                    </ul>

                                </td>

                            </tr>

                            <tr>
                                <td class="entry" valign="top" width="30%" headers="d157336e1838 ">Spark executor</td>

                                <td class="entry" valign="top" width="70%" headers="d157336e1841 ">
                                    <ul class="ul" id="concept_scs_3hh_tx__ul_hf4_sk3_xz">
                                        <li class="li">Email executor to send email each time the Spark
                                            executor starts a Spark application. </li>

                                        <li class="li">Any destination for event storage.</li>

                                    </ul>

                                </td>

                            </tr>

                        </tbody>

                    </table>
</div>

            </div>

        </div>

 </div>

</div>
<div class="topic concept nested1" id="concept_gvh_5wm_xx">
 <h2 class="title topictitle2">Event Records</h2>

 <div class="body conbody">
  <p class="p">Event records are records
            that are created when a stage or pipeline event occurs.</p>

        <p class="p">Most event records pass general event information in record headers, such as when the
            event occurred. They also can include event-specific details in record fields, like the
            name and location of the output file that was closed. </p>

        <p class="p">Event records generated by the File Tail origin are the exception - they include all
            event information in record fields. </p>

 </div>

<div class="topic concept nested2" id="concept_zft_rdq_tx">
 <h3 class="title topictitle3">Event Record Header Attributes</h3>

 <div class="body conbody">
        <p class="p">In addition
            to the standard record header attributes, most event records include record header
            attributes for event information such as the event type and when the event occurred. </p>

        <div class="p">As with any record header attribute, you can use the Expression Evaluator and the
            record:attribute function to include record header attribute information as a field in
            the record. For example, when storing event records, you most likely want to include the
            time of the event in the event record, using the following expression in an Expression
            Evaluator:<pre class="pre codeblock">${record:attribute('sdc.event.creation_timestamp')}</pre>
</div>

        <p class="p">Note that all record header attributes are String values. For more information about
            working with record header attributes, see <a class="xref" href="../Pipeline_Design/RecordHeaderAttributes.html#concept_wn2_jcz_dz">Record Header Attributes</a>.</p>

        <div class="p">Most events include the following event record header attributes. The exception, File
            Tail, writes all of the event information to record fields. 
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="concept_zft_rdq_tx__table_bxn_dhs_5x" class="table" frame="border" border="1" rules="all">
                    
                    
                    <thead class="thead" align="left">
                        <tr>
                            <th class="entry" valign="top" width="30%" id="d157336e2011">Event Record Header Attribute</th>

                            <th class="entry" valign="top" width="70%" id="d157336e2014">Description</th>

                        </tr>

                    </thead>

                    <tbody class="tbody">
                        <tr>
                            <td class="entry" valign="top" width="30%" headers="d157336e2011 ">sdc.event.type</td>

                            <td class="entry" valign="top" width="70%" headers="d157336e2014 ">Event type. Defined by the stage that generates the event. <p class="p">For
                                    information about the event types available for an event
                                    generating stage, see the stage documentation.</p>
</td>

                        </tr>

                        <tr>
       <td class="entry" valign="top" width="30%" headers="d157336e2011 ">sdc.event.version</td>

       <td class="entry" valign="top" width="70%" headers="d157336e2014 ">An integer that indicates the version of the event record type.</td>

      </tr>

                        <tr>
       <td class="entry" valign="top" width="30%" headers="d157336e2011 ">sdc.event.creation_timestamp</td>

       <td class="entry" id="concept_zft_rdq_tx__d29271e2083" valign="top" width="70%" headers="d157336e2014 ">Epoch timestamp when the stage created the event.
       </td>

      </tr>

                    </tbody>

                </table>
</div>
</div>

        <div class="note note"><span class="notetitle">Note:</span> <span class="ph"><span class="ph" id="concept_zft_rdq_tx__d29129e3838">Stage-generated event records
                              differ from stage to stage.</span> For a description of stage events,
                        see "Event Record" in the documentation for the event-generating stage. For
                        a description of pipeline events, see <a class="xref" href="../Pipeline_Configuration/PipelineConfiguration_title.html#concept_cv3_nqt_51b">Pipeline Event Records</a>.</span></div>

    </div>

</div>
</div>
<div class="topic concept nested1" id="concept_awx_xlq_cy">
    <h2 class="title topictitle2">Viewing Events in Data Preview, Snapshot, and Monitor Mode</h2>

    <div class="body conbody">
        <p class="p">When generated, stage events display in data preview, Monitor mode, and snapshot as event
            records. Once a record leaves the event-generating stage, it is treated like a standard
            record.</p>

        <p class="p">Pipeline events generated by the event framework do not display in data preview or
            monitor mode. However, you can enable data preview to generate and process pipeline
            events.</p>

    </div>

<div class="topic concept nested2" id="concept_hfy_ryv_cy">
 <h3 class="title topictitle3">Viewing Stage Events in Data Preview and Snapshot</h3>

 <div class="body conbody">
  <p class="p">In data preview and when reviewing snapshots of
            data, stage-related event records display in the event-generating stage marked as "event
            records," and they appear below the batch of standard records.</p>

        <p class="p">After leaving the stage, the record displays like any other record. </p>

        <p class="p">For example, the Directory origin below generates an event record as it starts reading a
            file for data preview:</p>

        <p class="p"><img class="image" id="concept_hfy_ryv_cy__image_jcz_5qq_cy" src="../Graphics/Event-DataPreview.png" height="393" width="424" /></p>

        <p class="p">When you select the Local FS destination where the event record is written, you see that
            the same event record no longer displays the event record label. It is treated like any
            other record: </p>

        <p class="p"><img class="image" id="concept_hfy_ryv_cy__image_wlx_4sq_cy" src="../Graphics/Event-DataPreview-Dest.png" height="390" width="414" /></p>

 </div>

</div>
<div class="topic concept nested2" id="concept_zwr_kzv_cy">
 <h3 class="title topictitle3">Viewing Stage Events in Monitor Mode</h3>

 <div class="body conbody">
  <p class="p">In Monitor mode, the
            event-generating stage provides statistics about stage-related event records. Once the
            event records leave the stage, Monitor mode treats event records like any other
            record.</p>

        <p class="p">For example, when you run and monitor the pipeline featured above, the Directory origin
            information displays event records in its statistics:</p>

        <p class="p"><img class="image" id="concept_zwr_kzv_cy__image_s1s_g5q_cy" src="../Graphics/Event-Monitor-Origin.png" height="438" width="875" /></p>

        <p class="p">Notice, in the Record Throughput chart, that you can hover over graphics to get the exact
            number of records that they represent.</p>

        <p class="p">And when you select the Local FS destination where the event record is written, Monitor
            mode displays statistics for the records written to the destination. At this point, the
            event records are treated like any other record:</p>

        <p class="p"><img class="image" id="concept_zwr_kzv_cy__image_op2_v5q_cy" src="../Graphics/Event-Monitor-Dest.png" height="438" width="861" /></p>

 </div>

</div>
</div>
<div class="topic concept nested1" id="concept_spk_wm5_51b">
 <h2 class="title topictitle2">Executing Pipeline Events in Data Preview</h2>

 <div class="body conbody">
  <p class="p">You can enable pipeline event execution in data preview to test event processing.</p>

        <p class="p">When you enable pipeline event generation while previewing data, the event framework
            generates the pipeline start event when you start data preview and the stop event when
            you stop data preview. If you configured the pipeline to pass the events to executors or
            to event-consuming pipelines, the events are passed and trigger the additional
            processing.</p>

        <p class="p">To enable generating pipeline event execution during data preview, use the Enable
            Pipeline Execution data preview property. </p>

 </div>

</div>
<div class="topic concept nested1" id="concept_jkm_rnz_kx">
    <h2 class="title topictitle2">Case Study: Parquet Conversion</h2>

    <div class="body conbody">
        <p class="p">Say you want to store data on HDFS using the
            columnar format, Parquet. But <span class="ph">Data
                  Collector</span>
            doesn't have a Parquet data format. How do you do it? </p>

        <p class="p">The event framework was created for exactly this purpose. The simple addition of an event
            stream to the pipeline enables the automatic conversion of Avro files to Parquet. </p>

        <p class="p">You can use the Spark executor to trigger a Spark application or the MapReduce executor
            to trigger a MapReduce job. This case study uses the MapReduce executor.</p>

        <p class="p">It's just a few simple steps:</p>

        <div class="p">
            <ol class="ol" id="concept_jkm_rnz_kx__ol_oh1_4gm_lx">
                <li class="li">Create the pipeline you want to use.<p class="p">Use the origin and processors that you
                        need, like any other pipeline. And then, configure Hadoop FS to write Avro
                        data to HDFS.</p>
<p class="p">The Hadoop FS destination generates events each time it
                        closes a file. This is perfect because we want to convert files to Parquet
                        only after they are fully written. </p>
<div class="p">
                        <div class="note note"><span class="notetitle">Note:</span> To avoid running unnecessary numbers of MapReduce jobs, configure the
                            destination to create files as large as the destination system can
                            comfortably handle.</div>

                    </div>
<p class="p"><img class="image" id="concept_jkm_rnz_kx__image_x2b_yvm_lx" src="../Graphics/Event-ParquetBasicPipe.png" height="87" width="622" /></p>
</li>

                <li class="li">Configure Hadoop FS to generate events. <p class="p">On the <span class="keyword wintitle">General</span>
                        tab of the destination, select the <span class="ph uicontrol">Produce Events</span>
                        property.</p>
<p class="p">With this property selected, the event output stream becomes
                        available and Hadoop FS generates an event record each time it closes an
                        output file. The event record includes the file path for the closed file.
                            </p>
<p class="p"><img class="image" id="concept_jkm_rnz_kx__image_ibh_mvm_lx" src="../Graphics/Event-ParquetHDFS.png" height="302" width="700" /></p>
</li>

                <li class="li">Connect the Hadoop FS event output stream to a MapReduce executor. <p class="p">Now, each
                        time the MapReduce executor receives an event, it triggers the jobs that you
                        configure it to run.</p>
<p class="p"><img class="image" id="concept_jkm_rnz_kx__image_wjn_k5m_lx" src="../Graphics/Event-ParquetPipe.png" height="176" width="812" /></p>
</li>

                <li class="li">Configure the MapReduce executor to run a job that converts the completed Avro
                    file to Parquet.<p class="p">In the MapReduce executor, configure the MapReduce
                        configuration details and select the Avro to Parquet job type. Then, on the
                        Avro to Parquet tab, configure the details for the job. </p>
<p class="p"><img class="image" id="concept_jkm_rnz_kx__image_rzm_frb_tx" src="../Graphics/Event-Parquet-MapReduce.png" height="387" width="646" /></p>
<p class="p">The <span class="ph uicontrol">Input Avro File</span> property
                        default, <samp class="ph codeph">${record:value('/filepath')}</samp>, runs the job on the
                        file specified in the Hadoop FS file closure event record. </p>
</li>

            </ol>

        </div>

        <p class="p">With this event stream added to the pipeline, each time the Hadoop FS destination closes
            a file, it generates an event. When the MapReduce executor receives the event, it kicks
            off a MapReduce job that converts the Avro file to Parquet. Simple!</p>

    </div>

</div>
<div class="topic concept nested1" id="concept_szz_xwm_lx">
    <h2 class="title topictitle2">Case Study: Impala Metadata Updates for DDS for Hive</h2>

    <div class="body conbody">
        <p class="p">You love the <a class="xref" href="../Hive_Drift_Solution/HiveDriftSolution_title.html#concept_phk_bdf_2w">Drift
                Synchronization Solution for Hive</a> because it automatically updates the Hive
            metastore when needed. But if you've been using it with Impala, you've been trying to
            time the Invalidate Metadata command after each metadata change and file write.</p>

        <p class="p">Instead of running the command manually, you use the event framework in your Drift
            Synchronization Solution for Hive pipeline to execute the command automatically. </p>

        <p class="p">Enable both the Hive Metastore destination and the Hadoop FS destination to generate
            events. You can connect both event streams to a single Hive Query executor. The executor
            then runs the Invalidate Metadata command each time the Hive Metastore destination
            changes the Hive metastore and each time Hadoop FS writes a file to a Hive table.</p>

        <p class="p">Here's how it works:</p>

        <p class="p">The following Drift Synchronization Solution for Hive pipeline reads files from a
            directory. The Hive Metadata processor evaluates the data for structural changes. It
            passes data to Hadoop FS and metadata records to the Hive Metastore destination. Hive
            Metastore creates and updates tables in Hive based on the metadata records it
            receives:</p>

        <p class="p"><img class="image" id="concept_szz_xwm_lx__image_lz5_414_lx" src="../Graphics/Event-HDS-BasicPipe.png" height="188" width="412" /></p>

        <div class="p">
            <ol class="ol" id="concept_szz_xwm_lx__ol_mtf_tzn_lx">
                <li class="li">Configure the Hive Metastore destination to generate events.<p class="p">On the
                            <span class="keyword wintitle">General</span> tab, select the <span class="ph uicontrol">Produce
                            Events</span> property.</p>
<p class="p">Now, the event output stream becomes
                        available, and Hive Metastore destination generates an event record every
                        time it updates the Hive metastore. The event record contains the name of
                        the table that was created or updated.</p>
<p class="p"><img class="image" id="concept_szz_xwm_lx__image_cht_bc4_lx" src="../Graphics/Event-HDS-HMetastore.png" height="425" width="467" /></p>
</li>

                <li class="li">We also need to add an event stream to the Hadoop FS destination so we can run
                    the Invalidate Metadata command each time the destination writes a file to Hive.
                    So in the Hadoop FS destination, on the <span class="keyword wintitle">General</span> tab, select
                        <span class="ph uicontrol">Produce Events</span>.<p class="p">With this property selected the
                        event output stream becomes available, and Hadoop FS generates an event
                        record every time it closes a file:</p>
<p class="p"><img class="image" id="concept_szz_xwm_lx__image_c35_qns_5x" src="../Graphics/Event-HDS-HDFS.png" height="337" width="435" /></p>
</li>

                <li class="li">The event record generated by the Hadoop FS destination does not include the
                    table name required by the Hive Query executor, but it contains the table name
                    in the file path. So add an Expression Evaluator processor to the event stream.
                    Create a new Table field and use the following
                        expression:<pre class="pre codeblock"><span class="ph" id="concept_szz_xwm_lx__d29129e4033">`${file:pathElement(record:value('/filepath'), -3)}`.`${file:pathElement(record:value('/filepath'), -2)}`</span></pre>
<div class="p">This
                        expression uses the path in the Filepath field of the event record and
                        performs the following calculations:<ul class="ul" id="concept_szz_xwm_lx__ul_umt_2sm_5x">
                            <li class="li">Extracts the third-to-last section of the path and uses it as the
                                database name. </li>

                            <li class="li">Extracts the second-to-last section of the path and uses it as the
                                table name.</li>

                        </ul>
</div>
<p class="p">So when Hadoop FS completes a file, it writes the path of the
                        written file in the filepath field, such as users/logs/server1weblog.txt.
                        And the expression above properly interprets the database and table name as:
                        logs.server1weblog.</p>
<p class="p"><img class="image" id="concept_szz_xwm_lx__image_pcc_3zm_5x" src="../Graphics/Event-HDS-Expression.png" height="342" width="718" /></p>
</li>

                <li class="li">Add the Hive Query executor and connect the Hive Metastore destination and the
                    Expression Evaluator to the executor. Then configure the Hive Query executor.<div class="p">
                        <div class="note note"><span class="notetitle">Note:</span> To use the Hive Query executor with Impala, you need to use the Impala
                            JDBC driver. By default, <span class="ph">Data
                  Collector</span> uses an open source Hive JDBC driver. So if you haven't already,
                                <a class="xref" href="https://www.cloudera.com/downloads/connectors/impala/jdbc/2-5-37.html" target="_blank">download the driver from Cloudera
                            </a>, then <a class="xref" href="../Configuration/ExternalLibs.html#concept_pdv_qlw_ft" title="After you've set up the external directory, use the Package Manager within Data Collector to install external libraries.">install
                                the driver</a>. Note that the download page also includes a link
                            to documentation for the driver. </div>

                    </div>
<p class="p">Once the Impala JDBC driver is installed, in the Hive Query executor,
                        configure the Hive configuration details on the <span class="keyword wintitle">Hive</span>
                        tab. If you have any trouble configuring the URL, see the Impala driver
                        information in our <a class="xref" href="https://ask.streamsets.com/question/7/how-do-you-configure-a-hive-impala-jdbc-driver-for-data-collector/" target="_blank">Ask StreamSets post</a>. </p>
<div class="p">Then,
                        on the <span class="keyword wintitle">Query</span> tab, enter the following
                        query:<pre class="pre codeblock">invalidate metadata ${record:value('/table')}</pre>
</div>
<p class="p">This
                        query refreshes the Impala cache for the specified table. And the table is
                        either the table in the Hive Metastore event record that was just updated or
                        the table where Hadoop FS wrote a file. </p>
<p class="p">Here's the final
                        pipeline:</p>
<p class="p"><img class="image" id="concept_szz_xwm_lx__image_ccr_hpr_mx" src="../Graphics/Event-HDS-HiveQueryDeets.png" height="410" width="637" /></p>
<p class="p">With these new event streams,
                        each time the Hive Metastore destination creates a table, partition or
                        column, and each time the Hadoop FS destination completes writing a file,
                        the destinations generate event records. When the Hive Query executor
                        receives an event record, it runs the Invalidate Metadata command so Impala
                        can update its cache with the new information. Done!</p>
</li>

            </ol>

        </div>

    </div>

</div>
<div class="topic concept nested1" id="concept_d1q_xl4_lx">
    <h2 class="title topictitle2">Case Study: Output File Management</h2>

    <div class="body conbody">
        <p class="p">By default, the Hadoop FS destination creates a
            complex set of directories for output files and late record files, keeping files open
            for writing based on stage configuration. That's great, but once the files are complete,
            you'd like the files moved to a different location. And while you're at it, it would be
            nice to set the permissions for the written files. </p>

        <p class="p">So what do you do? </p>

        <div class="p">Add an event stream to the pipeline to manage output files when the Hadoop FS destination
            is done writing them. Then, use the HDFS File Metadata executor in the event
                stream.<div class="note note"><span class="notetitle">Note:</span> If you prefer, instead of using the Hadoop FS destination with the HDFS
                File Metadata executor, you can use the Local FS destination with the HDFS File
                Metadata executor or the MapR FS destination with the MapR FS File Metadata
                executor.</div>
</div>

        <p class="p">Here's a pipeline that reads from a database using JDBC, performs some processing, and
            writes to HDFS:</p>

        <p class="p"><img class="image" id="concept_d1q_xl4_lx__image_y1d_zm4_lx" src="../Graphics/Event-Move-BasicPipe.png" height="102" width="651" /></p>

        <ol class="ol" id="concept_d1q_xl4_lx__ol_fvh_5m4_lx">
            <li class="li">To add an event stream, first configure Hadoop FS to generate events:<p class="p">On the
                        <span class="keyword wintitle">General</span> tab of the Hadoop FS destination, select the
                        <span class="ph uicontrol">Produce Events</span> property. </p>
<p class="p">Now, the event output
                    stream becomes available, and Hadoop FS generates an event record each time it
                    closes an output file. The Hadoop FS event record includes fields for the file
                    name, path, and size.</p>
<p class="p"><img class="image" id="concept_d1q_xl4_lx__image_n5q_5p4_lx" src="../Graphics/Event-Move-HDFS.png" height="300" width="630" /></p>
</li>

            <li class="li">Connect the Hadoop FS event output stream to a HDFS File Metadata executor.<p class="p">Now,
                    each time the HDFS File Metadata executor receives an event, it triggers the
                    tasks that you configure it to run.</p>
<p class="p"><img class="image" id="concept_d1q_xl4_lx__image_x4w_kq4_lx" src="../Graphics/Event-Move-HDFSMetadata.png" height="172" width="675" /></p>
</li>

            <li class="li">Configure the HDFS File Metadata executor to move the files to the directory that
                you want and set the permissions for the file.<p class="p">In the HDFS File Metadata executor,
                    configure the HDFS configuration details on the <span class="keyword wintitle">HDFS</span> tab.
                    Then, on the <span class="keyword wintitle">Tasks</span> tab, select <span class="ph uicontrol">Change Metadata
                        on Existing File</span> configure the changes that you want to make.
                    </p>
<p class="p">In this case, you want to move files to
                        <span class="ph filepath">/new/location</span>, and set the file permissions to 0440 to
                    allow the user and group read access to the files:</p>
<p class="p"><img class="image" id="concept_d1q_xl4_lx__image_uk1_l1c_tx" src="../Graphics/Event-Move-FileMetadata-props.png" height="381" width="703" /></p>
</li>

        </ol>

        <p class="p">With this event stream added to the pipeline, each time the Hadoop FS destination closes
            a file, it generates an event record. When the HDFS File Metadata executor receives the
            event record, it moves the file and sets the file permissions. No muss, no fuss.</p>

    </div>

</div>
<div class="topic concept nested1" id="concept_kff_ykv_lz">
 <h2 class="title topictitle2">Case Study: Stop the Pipeline</h2>

 <div class="body conbody">
  <p class="p">Say your dataflow topology updates a database table
            daily at 4 am. Rather than have the pipeline process the data in a few minutes and sit
            idle for the rest of the day, you want to kick off the pipeline, have it process all
            data and then stop - just like old school batch processing. And you'd like to have the
            pipeline let you know when it has stopped. </p>

        <p class="p">To do this, simply route the no-more-data event record to the Pipeline Finisher executor
            and configure notification.</p>

        <div class="p">The following origins generate no-more-data
                        events:<ul class="ul" id="concept_kff_ykv_lz__d29129e4434">
                        <li class="li">Directory origin</li>

                        <li class="li">JDBC Query Consumer origin</li>

                        <li class="li">JDBC Multitable Consumer origin</li>

                        <li class="li">Salesforce origin</li>

                        <li class="li">SQL Server CDC Client origin</li>

                        <li class="li">SQL Server Change Tracking origin</li>

                  </ul>
</div>

        <p class="p">We'll use the JDBC Query Consumer to show a more complex scenario. </p>

        <p class="p">Here's the basic pipeline that reads from a database, performs some processing, and
            writes to HDFS: </p>

        <p class="p"><img class="image" id="concept_kff_ykv_lz__image_zy1_kkw_lz" src="../Graphics/Event-StopPipe-Basic.png" height="82" width="634" /></p>

        <div class="p">To configure the pipeline to stop after processing all available queried data:<ol class="ol" id="concept_kff_ykv_lz__ol_ivb_qmw_lz">
                <li class="li">Configure the origin to generate events:<p class="p">On the <span class="keyword wintitle">General</span>
                        tab of the JDBC Query Consumer origin, select the <span class="ph uicontrol">Produce
                            Events</span> property. </p>
<p class="p">The event output stream becomes
                        available:</p>
<p class="p"><img class="image" id="concept_kff_ykv_lz__image_b5v_znw_lz" src="../Graphics/Event-StopPipe-Event.png" height="229" width="574" /></p>
<p class="p">The JDBC Query Consumer
                        generates several types of events: query success, query failure, and
                        no-more-data. We know this because you checked the <a class="xref" href="../Origins/JDBCConsumer.html#concept_rzl_s1t_kz">Event
                            Record section</a> of the JDBC Query Consumer documentation. Every
                        event-generating stage has event details in a similar section. </p>
<p class="p">The
                        query success and failure events can be useful, so you might use a Stream
                        Selector to route those records to a separate event stream. But let's say we
                        don't care about those events, we just want the no-more-data event to pass
                        to the Pipeline Finisher executor. </p>
</li>

                <li class="li">Connect the event output stream to the Pipeline Finisher executor. <p class="p">At this
                        point, all events that the origin generates come to the executor. Since the
                        JDBC Query Consumer origin generates multiple event types, this setup might
                        cause the executor to stop the pipeline too soon.</p>
</li>

                <li class="li">To ensure that only the no-more-data event enters the executor, configure a
                        precondition.<p class="p">With a precondition, only records that meet the specified
                        condition can enter the stage. </p>
<p class="p">We know that each event record
                        includes the event type in the sdc.event.type record header attribute. So to
                        ensure that only no-more-data events enter the stage, we can use the
                        following expression in the
                    precondition:</p>
<pre class="pre codeblock">${record:eventType() == 'no-more-data'}</pre>
</li>

                <li class="li">Records that don't meet the precondition go to the stage for error handling, so
                    to avoid storing error records that we don't care about â€“ that is, the query
                    success and failure events â€“ let's also set the <span class="ph uicontrol">On Record
                        Error</span> property to <span class="ph uicontrol">Discard</span>.<p class="p">So here's
                        the Pipeline Finisher: </p>
<p class="p"><img class="image" id="concept_kff_ykv_lz__image_ucl_4qw_lz" src="../Graphics/Event-StopPipe-Finisher.png" height="354" width="563" /></p>
</li>

                <li class="li">Now, to get notified when the Pipeline Finisher stops the pipeline, configure
                    the pipeline to send an email when the pipeline state changes to Finished.
                        <div class="p">You can use this option when <span class="ph">Data
                  Collector</span> is <a class="xref" href="../Configuration/DCConfig.html#concept_it1_wwg_xz">set
                            up to send email</a>. You can alternatively use the pipeline state
                        notification to send a webhook, or use an <a class="xref" href="../Executors/Email.html#concept_sjs_sfp_qz">Email executor</a>
                        in the pipeline to send a customized email. Since we only need a simple
                        notification, let's send a basic email based on the pipeline state: <ol class="ol" type="a" id="concept_kff_ykv_lz__ol_wcz_wsb_yz">
                            <li class="li">Click in the canvas to view the pipeline configuration, and click
                                the <span class="keyword wintitle">Notifications</span> tab. </li>

                            <li class="li">In the <span class="ph uicontrol">Notify on Pipeline State Changes</span>,
                                leave the <span class="ph uicontrol">Finished</span> state and remove the other
                                default states. </li>

                            <li class="li">Then, enter the email addresses to receive the email:</li>

                        </ol>
</div>
<p class="p"><img class="image" id="concept_kff_ykv_lz__image_qmr_cpv_xz" src="../Graphics/Event-StopPipe-Notification.png" height="141" width="642" /></p>
</li>

            </ol>
That's it!</div>

        <p class="p">With this setup, the JDBC Query Consumer passes a no-more-data event when it completes
            processing all data returned by the query, and the Pipeline Finisher executor stops the
            pipeline and transitions the pipeline to a Finished state. All other events generated by
            the origin are discarded. <span class="ph">Data
                  Collector</span>
            sends notification so you know when the pipeline finishes, and the next time you want to
            process more data, you can just start the pipeline again. </p>

 </div>

</div>
<div class="topic concept nested1" id="concept_vrh_jrs_bbb">
 <h2 class="title topictitle2">Case Study: Offloading Data from Relational Sources to Hadoop</h2>

 <div class="body conbody">
  <p class="p">Say you want to batch-load data from a set of
            database tables to Hive, basically replacing an old Apache Sqoop implementation. Before
            processing new data, you want to delete the previous tables. And you'd like to create a
            notification file when the pipeline stops to trigger subsequent actions from other
            applications, like a _SUCCESS file to launch a MapReduce job.</p>

        <div class="p">Here's how the tasks break down:<dl class="dl">
                
                    <dt class="dt dlterm">Batch processing</dt>

                    <dd class="dd">
                        <p class="p">To perform batch processing, where the pipeline stops automatically after
                            all processing is complete, you use an origin that creates the
                            no-more-data event, and you pass that event to the Pipeline Finisher
                            executor. We'll step through this quickly, but for a case study centered
                            on the Pipeline Finisher, see <a class="xref" href="EventFramework-Title.html#concept_kff_ykv_lz">Case Study: Stop the Pipeline</a>. </p>

                        <p class="p">To process database data, we can use the JDBC Multitable Consumer - it
                            generates the no-more-data event and can spawn multiple threads for
                            greater throughput. For a list of origins that generate the no-more-data
                            event, see <a class="xref" href="../Executors/PipelineFinisher.html#concept_dct_z3v_j1b">Related Event Generating Stages</a> in the Pipeline Finisher documentation.</p>

                    </dd>

                
                
                    <dt class="dt dlterm">Remove existing data before processing new data</dt>

                    <dd class="dd">To perform tasks before the pipeline starts processing data, use the
                        pipeline start event. So, for example, if you wanted to run a shell command
                        to perform a set of tasks before the processing begins, you could use the
                        Shell executor. </dd>

                    <dd class="dd">To truncate Hive tables, we'll use the Hive Query executor.</dd>

                
                
                    <dt class="dt dlterm">Create a notification file when the pipeline stops</dt>

                    <dd class="dd">Use the pipeline stop event to perform tasks after all processing completes,
                        before the pipeline comes to a full stop. To create an empty success file,
                        we'll use the HDFS File Metadata executor.</dd>

                
            </dl>
</div>

        <div class="p">Now let's take it step-by-step:<ol class="ol" id="concept_vrh_jrs_bbb__ol_lqq_mxs_bbb">
                <li class="li">First create the pipeline that you want to use. <p class="p">We use the JDBC Multitable
                        Consumer in the following simple pipeline, but your pipeline can be as
                        complex as needed. </p>
<p class="p"><img class="image" id="concept_vrh_jrs_bbb__image_dww_szs_bbb" src="../Graphics/Event-Sqoop-Pipeline1.png" height="94" width="544" /></p>
</li>

                <li class="li">To set up batch processing, enable event generation in the origin by selecting
                    the <span class="ph uicontrol">Produce Events</span> property on the
                        <span class="keyword wintitle">General</span> tab. Then, connect the event output stream to
                    the Pipeline Finisher executor.<p class="p">Now, when the origin completes processing all
                        data, it passes a no-more-data event to the Pipeline Finisher. And after all
                        pipeline tasks are complete, the executor stops the pipeline.</p>
<p class="p"><img class="image" id="concept_vrh_jrs_bbb__image_v3j_1bt_bbb" src="../Graphics/Event-Sqoop-OriginFinisher.png" height="325" width="540" /></p>
<div class="note note"><span class="notetitle">Note:</span> The JDBC Multitable Consumer origin generates only
                        the no-more-data event, so you don't need to use a Stream Selector or
                        executor precondition to manage other event types. But if the origin you
                        want to use generates additional event types, you should ensure that only
                        the no-more-data event is routed to the Pipeline Finisher. For details, see
                        the <a class="xref" href="EventFramework-Title.html#concept_kff_ykv_lz">Stop the
                            Pipeline case study</a>. </div>
</li>

                <li class="li">To truncate Hive tables before processing begins, configure the pipeline to pass
                    the pipeline start event to the Hive Query executor.<p class="p">To do this, on the
                            <span class="ph uicontrol">General</span> tab, you configure the <span class="ph uicontrol">Start
                            Event</span> property, selecting the Hive Query executor as
                        follows:</p>
<p class="p"><img class="image" id="concept_vrh_jrs_bbb__image_bzp_w2t_bbb" src="../Graphics/Event-Sqoop-StartEvent.png" height="331" width="619" /></p>
<p class="p">Notice, a <span class="keyword wintitle">Start
                            Event - Hive Query</span> tab now displays. This is because the
                        executors for pipeline start and stop events do not display in the pipeline
                        canvas - you configure the selected executor as part of the pipeline
                        properties.</p>
<p class="p">Also note that you can pass each type of pipeline event to
                        one executor or to another pipeline for more complex processing. For more
                        information about pipeline events, see <a class="xref" href="EventFramework-Title.html#concept_amg_2qr_t1b">Pipeline Event Generation</a>.</p>
</li>

                <li class="li">To configure the executor, click the <span class="keyword wintitle">Start Event - Hive
                        Query</span> tab. <p class="p">You configure the connection properties as needed,
                        then specify the query to use. In this case, you can use the following
                        query, filling in the table
                        name:</p>
<pre class="pre codeblock">TRUNCATE TABLE IF EXISTS &lt;table name&gt;</pre>
<p class="p">Also,
                        select <span class="ph uicontrol">Stop on Query Failure</span>. This ensures that the
                        pipeline stops and avoids performing any processing when the executor cannot
                        complete the truncate query. The properties should look like
                            this:</p>
<p class="p"><img class="image" id="concept_vrh_jrs_bbb__image_ovf_jht_bbb" src="../Graphics/Event-Sqoop-HiveQuery.png" height="480" width="552" /></p>
<p class="p">With this configuration, when
                        you start the pipeline, the Hive Query executor truncates the specified
                        table before data processing begins. And when the truncate completes
                        successfully, the pipeline begins processing.</p>
</li>

                <li class="li">Now, to generate a success file after all processing is complete, you perform
                    similar steps with the<span class="ph uicontrol"> Stop Event</span> property. <p class="p">Configure
                        the pipeline to pass the pipeline stop event to the HDFS File Metadata
                        executor as follows:</p>
<p class="p"><img class="image" id="concept_vrh_jrs_bbb__image_c3s_vj5_bbb" src="../Graphics/Event-Sqoop-StopEvent.png" height="319" width="609" /></p>
</li>

                <li class="li">Then, on the <span class="keyword wintitle">Stop Event - HDFS File Metadata</span> tab, specify
                    the connection information and configure the executor to create the success file
                    in the required directory with the specified name.<p class="p"><img class="image" id="concept_vrh_jrs_bbb__image_r44_2m5_bbb" src="../Graphics/Event-Sqoop-HDFSFileMeta.png" height="313" width="633" /></p>
<p class="p">With these configurations in place, when you start
                        the pipeline, the Hive Query executor truncates the table specified in the
                        query, then pipeline processing begins. When the JDBC Multitable Consumer
                        completes processing all available data, it passes a no-more-date event to
                        the Pipeline Finisher executor. </p>
<p class="p">The Pipeline Finisher executor allows
                        the pipeline stop event to trigger the HDFS File Metadata executor to create
                        the empty file, then brings the pipeline to a graceful stop. Batch job
                        complete!</p>
</li>

            </ol>
</div>

 </div>

</div>
<div class="topic concept nested1" id="concept_t2t_lp5_xz">
 <h2 class="title topictitle2">Case Study: Sending Email</h2>

 <div class="body conbody">
  <p class="p">You can configure
            a pipeline to send email upon <a class="xref" href="../Pipeline_Configuration/PipelineConfiguration_title.html#concept_mtn_k4j_rz">pipeline
                state change</a> and upon <a class="xref" href="../Alerts/RulesAlerts_title.html#task_f3v_1hw_1r" title="You can define the email addresses to receive metric and data alerts. When an alert triggers an email, the Data Collector sends an email to every address in the list.">triggering an
                alert</a>. Both methods can be useful in their own way. In this case study, we'll
            use an Email executor to send email upon receiving an event. </p>

        <p class="p">Say you have a pipeline that reads from a database using the JDBC Query Consumer origin,
            uses a Jython Evaluator to evaluate the data and generate events for invalid
            transactions, and writes to HDFS. You want this pipeline to send two types of email: one
            when the Jython Evaluator finds an invalid transaction and one when the JDBC Query
            Consumer fails to complete a query. </p>

        <p class="p">To do this, you simply route the events from the origin and processor to the Email
            executor and configure two email messages. The Email executor allows you to specify a
            condition for sending email and to use expressions to create customized email that
            provides event-related information to the recipient.</p>

        <p class="p">Say this is the original pipeline:</p>

        <p class="p"><img class="image" id="concept_t2t_lp5_xz__image_b4k_htw_xz" src="../Graphics/Event-Email-Pipe.png" height="78" width="497" /></p>

        <div class="p">
            <ol class="ol" id="concept_t2t_lp5_xz__ol_vmp_psw_xz">
                <li class="li">First, configure the JDBC Query Consumer to generate events. <p class="p">On the
                            <span class="keyword wintitle">General</span> tab of the origin, select the
                            <span class="ph uicontrol">Produce Events</span> property. </p>
<p class="p">Now, the event
                        output stream becomes available. Note that the JDBC Query Consumer generates
                        several types of events. You know this because you checked the <a class="xref" href="../Origins/JDBCConsumer.html#concept_rzl_s1t_kz">Event
                            Record</a> section of the JDBC Query Consumer documentation. Every
                        event-generating stage has event details in a similar section. </p>
</li>

                <li class="li">Now configure the Jython Evaluator to generate events in the same way.<p class="p">Of
                        course, the Jython Evaluator will only generate events if your script is set
                        up to do so. But you also need to enable the <span class="ph uicontrol">Produce
                            Events</span> property on the <span class="keyword wintitle">General</span> tab.
                    </p>
</li>

                <li class="li">Connect both event streams to an Email executor. <p class="p"><img class="image" id="concept_t2t_lp5_xz__image_zl2_gxw_xz" src="../Graphics/Event-Email-ExecutorPipe.png" height="154" width="504" /></p>
</li>

                <li class="li">Now since the JDBC Query Consumer generates several types of events, you need to
                    configure the Email executor to send the first email only after receiving a
                    query failure event:<ol class="ol" type="a" id="concept_t2t_lp5_xz__ol_ghk_ymc_yz">
                        <li class="li">The query failure event has the jdbc-query-failure event type, so on the
                                <span class="keyword wintitle">Email</span> tab, you use the following condition:
                            <pre class="pre codeblock">${record:eventType() == 'jdbc-query-failure'}</pre>
</li>

                        <li class="li">All of the email properties allow expressions, so for the email subject,
                            you might include the pipeline name as
                            follows:<pre class="pre codeblock">Query failure in ${pipeline:title()}!</pre>
</li>

                        <li class="li">When you compose the email body, you might use additional expressions to
                            include information about the pipeline and information included in the
                            event record in the email. <p class="p">Remember, the Event Record documentation
                                lists all header attributes and fields in the event record, so you
                                can refer to it when configuring the email. For more information
                                about using expressions in email, see <a class="xref" href="../Executors/Email.html#concept_tgb_vbm_wz">Using Expressions</a>.</p>
<div class="p">For this email, you might include the following
                                information:
                                <pre class="pre codeblock">Pipeline ${pipeline:title()} encountered an error. 

At ${time:millisecondsToDateTime(record:eventCreation() * 1000)}, the JDBC Query
Consumer failed to complete the following query: ${record:value('/query')}

Only the following number of rows were processed: ${record:value('/row-count')} </pre>
</div>
</li>

                    </ol>
<p class="p">The email configuration looks like this:</p>
<p class="p"><img class="image" id="concept_t2t_lp5_xz__image_cck_wbx_xz" src="../Graphics/Event-Email-Origin.png" height="381" width="612" /></p>
</li>

                <li class="li">Click the <span class="ph uicontrol">Add</span> icon to configure the email for the Jython
                    Evaluator events. <p class="p">Since you want to send an email for each event that the
                        Jython Evaluator generates, for the condition, you can use the event type
                        defined in the script. Let's say it's "invalidTransaction". As with the
                        first email, you can include additional information about the pipeline and
                        data from the event record in the email body, as follows:</p>
<p class="p"><img class="image" id="concept_t2t_lp5_xz__image_i2g_ydx_xz" src="../Graphics/Event-Email-Jython.png" /></p>
</li>

            </ol>

        </div>

        <p class="p">When you run the pipeline, the specified email recipients receive custom messages each
            time the Email executor receives the specified events. And the email recipients can act
            on the information included in the email without further ado. </p>

 </div>

</div>
<div class="topic concept nested1" id="concept_ocb_nnl_px">
 <h2 class="title topictitle2">Case Study: Event Storage </h2>

 <div class="body conbody">
  <p class="p">Store
            event records to preserve an audit trail of the events that occur. You can store event
            records from any event-generating stage. For this case study, say you want to keep a log
            of the files written to HDFS by the following pipeline: </p>

        <p class="p"><img class="image" id="concept_ocb_nnl_px__image_csj_gwl_px" src="../Graphics/Event-Storage.png" height="94" width="531" /></p>

        <div class="p">To do this, you simply:<ol class="ol" id="concept_ocb_nnl_px__ol_xgc_3wl_px">
                <li class="li">Configure the Hadoop FS destination to generate events.<p class="p">On the
                            <span class="keyword wintitle">General</span> tab, select the <span class="ph uicontrol">Produce
                            Events</span> property </p>
<p class="p">Now the event output stream becomes
                        available, and the destination generates an event each time it closes a
                        file. For this destination, each event record includes fields for the file
                        name, file path, and size of the closed file. </p>
<p class="p"><img class="image" id="concept_ocb_nnl_px__image_qbc_1hj_yx" src="../Graphics/Event-Storage-HDFS.png" height="271" width="544" /></p>
</li>

                <li class="li">You can write the event records to any destination, but let's assume you want to
                    write them to HDFS as well:<p class="p"><img class="image" id="concept_ocb_nnl_px__image_orv_fyl_px" src="../Graphics/Event-Storage-HDFS-2.png" height="150" width="580" /></p>
<p class="p">You could be done right there,
                        but you want to include the time of the event in the record, so you know
                        exactly when the Hadoop FS destination closed a file. </p>
</li>

                <li class="li"> All event records include the event creation time in the
                    sdc.event.creation_timestamp record header attribute, so you can add an
                    Expression Evaluator to the pipeline and use the following expression to include
                    the creation time in the
                        record:<pre class="pre codeblock">${record:attribute('sdc.event.creation_timestamp')}</pre>
<p class="p">The
                        resulting pipeline looks like this: </p>
<p class="p"><img class="image" id="concept_ocb_nnl_px__image_sqc_3bq_tx" src="../Graphics/Event-Storage-EEval.png" height="318" width="742" /></p>
<p class="p">Note that event creation time is expressed as an
                        epoch or Unix timestamp, such as 1477698601031. And record header attributes
                        provide data as strings.</p>
<div class="p">
                        <div class="note tip"><span class="tiptitle">Tip:</span> You can use time functions to convert timestamps to
                            different data types. For more information, see <a class="xref" href="../Expression_Language/Functions.html#concept_lhz_pyp_1r" title="Use record functions to determine information about a record, such as the stage that created it or whether a field exists in the record. Use delimited data record functions to process delimited data with the list root field type. If you configured an origin to process the delimited data with the list-map root field type, you can use standard record functions.Error record functions provide information about error records. Use error functions to process error records.Use Base64 functions to encode or decode data using Base64.Use data drift functions to create alerts when data drift occurs. You can use these functions in data drift rules.Use time functions to return the current time or to transform datetime data.">Functions</a>.</div>

                    </div>
</li>

            </ol>
</div>

 </div>

</div>
<div class="topic concept nested1" id="concept_azz_tq4_lx">
 <h2 class="title topictitle2">Summary</h2>

 <div class="body conbody">
        <div class="p">Here are the key points about
            dataflow triggers and the event framework: <ol class="ol" id="concept_azz_tq4_lx__ol_fz4_ygh_tx">
                <li class="li">You can use the event framework in any pipeline where the logic suits your
                    needs. </li>

                <li class="li">The event framework generates pipeline-related events and stage-related
                    events.</li>

                <li class="li">Pipeline events are generated when the pipeline starts and stops. For details,
                    see <a class="xref" href="EventFramework-Title.html#concept_amg_2qr_t1b">Pipeline Event Generation</a>.</li>

                <li class="li">You can configure each pipeline event type to pass to a single executor or to
                    another pipeline for more complex processing.</li>

                <li class="li">Stage events are generated based on the processing logic of the stage. For a
                    list of event-generating stages, see <a class="xref" href="EventFramework-Title.html#concept_zrl_mhn_lx">Stage Event Generation</a>.</li>

                <li class="li">Events generate <dfn class="term">event records</dfn> to pass relevant information
                    regarding the event, such as the path to the file that was closed. <p class="p"><span class="ph"><span class="ph" id="concept_azz_tq4_lx__d29129e3838">Stage-generated event records
                              differ from stage to stage.</span> For a description of stage events,
                        see "Event Record" in the documentation for the event-generating stage. For
                        a description of pipeline events, see <a class="xref" href="../Pipeline_Configuration/PipelineConfiguration_title.html#concept_cv3_nqt_51b">Pipeline Event Records</a>.</span>
                    </p>
</li>

                <li class="li">In the simplest use case, you can route stage event records to a destination to
                    save event information.</li>

                <li class="li">To trigger a task with an event, use an executor stage. <p class="p">For a list of logical
                        event generation and executor pairings, see <a class="xref" href="EventFramework-Title.html#concept_scs_3hh_tx">Logical Pairings</a>.</p>
</li>

                <li class="li">You can add processors to event streams for stage events or to consuming
                    pipelines for pipeline events.<p class="p">For example, you might add an Expression
                        Evaluator to add the event generation time to an event record before writing
                        it to a destination. Or, you might use a Stream Selector to route different
                        types of event records to different executors.</p>
</li>

                <li class="li">When working with stage events, you cannot merge event streams with data
                    streams. </li>

                <li class="li">You can use the Dev Data Generator and To Event development stages to generate
                    events for pipeline development and testing. For more information about the
                    development stages, see <a class="xref" href="../Pipeline_Design/DevStages.html#concept_czx_ktn_ht">Development Stages</a>.</li>

                <li class="li">In data preview and in Monitor mode, stage-generated event records display
                    separately in the event-generating stage. Afterwards, they are treated like any
                    standard record. </li>

                <li class="li">You can configure data preview to generate and execute pipeline events.</li>

            </ol>
 For examples of how you might use the event framework, see the case studies
            earlier in this chapter. </div>

 </div>

</div>
</div>
<div class="navfooter"><!---->
<span class="navprev"><a class="link" href="../DPM/UnregisterSDCwithDPM.html#concept_ldb_sr5_cx" title="Unregister Data Collector from DPM"><span class="navheader_label">Previous topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Unregister Data Collector from DPM</span></a></span>  
<span class="navnext"><a class="link" href="../Hive_Drift_Solution/HiveDriftSolution_title.html#concept_fjj_zcf_2w" title="Drift Synchronization Solution (a.k.a. Hive Drift Solution)"><span class="navheader_label">Next topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Drift Synchronization Solution (a.k.a. Hive Drift Solution)</span></a></span>  </div><div class="footer" id="webhelp_copyright_information"><!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

--><!-- SDC google analytics --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-60917135-3', 'auto');
  ga('send', 'pageview');
</script></div>
</body>
</html>