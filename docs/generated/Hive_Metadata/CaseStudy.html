
<!DOCTYPE html
  SYSTEM "about:legacy-compat">
<html xml:lang="en-us" lang="en-us">
<head><meta name="description" content="Let's say you have a Data Collector pipeline that writes log data to Kafka. The File Tail origin in the pipeline processes data from several different web services, tagging each record with a &#34;tag&#34; ..."></meta><meta http-equiv="Content-Type" content="text/html; charset=utf-8"></meta><meta name="copyright" content="(C) Copyright 2005"></meta><meta name="DC.rights.owner" content="(C) Copyright 2005"></meta><meta name="DC.Type" content="concept"></meta><meta name="DC.Title" content="Case Study"></meta><meta name="DC.Relation" scheme="URI" content="../Hive_Metadata/HiveDriftSolution_title.html"></meta><meta name="DC.Format" content="XHTML"></meta><meta name="DC.Identifier" content="concept_a1w_kkn_fw"></meta><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/commonltr.css"><!----></link><title>Case Study</title><!--  Generated with Oxygen version 17.1, build number 2016020417.  --><meta http-equiv="Content-Type" content="text/html; charset=utf-8"></meta><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/webhelp_topic.css"><!----></link><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/skins/skin.css"><!----></link><link rel="stylesheet" type="text/css" href="../skin.css"></link><script type="text/javascript"><!--
          
          var prefix = "../index.html";
          
          --></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-1.8.2.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.cookie.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-ui.custom.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.highlight-3.js"><!----></script><script type="text/javascript" charset="utf-8" src="../oxygen-webhelp/resources/js/webhelp_topic.js"><!----></script><!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
--></head>
<body onload="highlightSearchTerm()" class="frmBody">
<table class="nav"><tbody><tr><td colspan="2"><div id="printlink"><a href="javascript:window.print();" title="Print this page"></a></div><div id="permalink"><a href="#" title="Link to this page"></a></div></td></tr><tr><td width="75%"><a class="navheader_parent_path" href="../Hive_Metadata/HiveDriftSolution_title.html" title="Hive Drift Solution">Hive Drift Solution</a></td><td><div class="navheader">
<span class="navparent"><a class="link" href="../Hive_Metadata/HiveDriftSolution_title.html" title="Hive Drift Solution"><span class="navheader_label">Parent topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Hive Drift Solution</span></a></span>  </div></td></tr></tbody></table>
<div class="nested0" id="concept_a1w_kkn_fw">
 <h1 class="title topictitle1">Case Study</h1>

 <div class="body conbody">
  <p class="p">Let's say you have a
                <span class="ph">Data
                  Collector</span> pipeline that writes log data to Kafka. The File Tail origin in the pipeline
            processes data from several different web services, tagging each record with a "tag"
            header attribute that identifies the service that generated the data. </p>

        <p class="p">Now you want a new pipeline to pass the data to HDFS where it can be stored and reviewed,
            and you'd like the data written to tables based on the web service that generated the
            data. Note that you could also write the data to MapR FS -- the steps are almost
            identical to this case study, you'd just use a different destination.</p>

        <p class="p">To do this, add and configure a Kafka Consumer to read the data into the pipeline, then
            connect it to a Hive Metadata processor. The processor assesses the record structure and
            generates a metadata record that describes any required Hive metadata changes. Using the
            tag header attribute and other user-defined expressions, a Hive Metadata processor can
            determine the database, table, and partition to use for the target directory and write
            that information along with the Avro schema to the record header, including file roll
            indicator when necessary.</p>

        <p class="p">You connect the Hive Metadata processor metadata output stream to a Hive Metastore
            destination. The destination, upon receiving the metadata record from the Hive Metadata
            processor, creates or updates Hive tables as needed. </p>

        <p class="p">You connect the Hive Metadata processor data output stream to a Hadoop FS destination and
            configure it to use the information in record headers. The destination then writes each
            record where it wants to go using the target directory and Avro schema in the record
            header, and rolling files when needed. </p>

        <p class="p">Now let's take a closer look... </p>

 </div>

<div class="related-links"></div>
<div class="topic concept nested1" id="concept_fzk_mmn_fw">
 <h2 class="title topictitle2">The Hive Metadata Processor</h2>

 <div class="body conbody">
  <div class="p">You set up the Kafka Consumer and connect it to the Hive Metadata processor. When you configure
            the processor, you have a few things to consider in addition to your basic connection
            details: <ol class="ol" id="concept_fzk_mmn_fw__ol_fzm_bmv_fw">
                <li class="li">Which database should the records be written to? <p class="p">Hadoop FS will do the
                        writing, but the processor needs to know where the records should
                        go.</p>
<p class="p">Let's write to the Hive default database. To do that, you can
                        leave the database property empty.</p>
</li>

                <li class="li">What tables should the records be written to?<div class="p">The pipeline supplying the data
                        to Kafka uses the "tag" header attribute to indicate the originating web
                        service. To use the tag attribute to write to tables, you use the following
                        expression for the table name:
                        <pre class="pre codeblock">${record:attribute('tag')}</pre>
</div>
</li>

                <li class="li">What partitions, if any, do you want to use? <div class="p">Let's create daily partitions
                        using datetime variables for the partition value expression as
                        follows:<pre class="pre codeblock">${YYYY()}-${MM()}-${DD()}</pre>
</div>
</li>

                <li class="li">How do you want to configure the precision and scale for decimal fields?
                        <p class="p">Though the data from the web services contains no decimal data that you
                        are aware of, to prevent new decimal data from generating error records,
                        configure the decimal field expressions. </p>
<p class="p">The default expressions are
                        for data generated by the JDBC Consumer. You can replace them with other
                        expressions or with constants. </p>
</li>

            </ol>
</div>

        <p class="p"> At this point, your pipeline would look like this: </p>

        <p class="p"><img class="image" id="concept_fzk_mmn_fw__image_g5b_34n_fw" src="../Graphics/HiveMeta-Ex-Processor.png" height="314" width="562"></img></p>

        <p class="p">With this configuration, Hadoop FS writes every record to the Hive table listed in the
            tag attribute and to the daily partition based on the time of processing.</p>

 </div>

</div>
<div class="topic concept nested1" id="concept_vh3_s4n_fw">
 <h2 class="title topictitle2">The Hive Metastore Destination</h2>

 
 <div class="body conbody"><p class="shortdesc">Now to process the metadata records - and to create and update tables in Hive - you need
        the Hive Metastore destination.</p>

  <p class="p">Connect the destination to the second output stream of the processor and configure the
            destination. The Hive Metastore just needs to know how to connect to Hive, so
            configuration of this destination is a breeze - all the work happens under the covers. </p>

        <p class="p">The destination connects to Hive the same way the processor does so your destination
            should look something like this, using the same Hive connection information:</p>

        <p class="p"><img class="image" id="concept_vh3_s4n_fw__image_imx_5pn_fw" src="../Graphics/HiveMeta-Ex-Dest.png" height="301" width="490"></img></p>

        <p class="p">The magic here is: if the destination gets a metadata record that says you need a new
            table for a new web service, it creates the table with all the necessary columns so you
            can write the record (that triggered that metadata record) to the table.</p>

        <p class="p">And if the structure of the record going to a table changes, like adding a couple new
            fields, the destination updates the table in Hive so the record can be written to the
            table.</p>

        <p class="p">That covers the metadata, but what about the data?  </p>

 </div>

</div>
<div class="topic concept nested1" id="concept_jzr_ypn_fw">
 <h2 class="title topictitle2">The Hadoop FS Destination</h2>

 
 <div class="body conbody"><p class="shortdesc">The Hadoop FS destination can write data to HDFS using record header attributes. If you
        wanted, you could use the MapR FS destination to write to MapR FS instead. Record header
        attributes contain the write details that the Hive Metadata processor generates and adds to
        records. </p>

  <p class="p">To write data to HDFS, you connect the Hadoop FS destination to the data output stream of the
            processor. You could connect the data output stream to the MapR FS destination to write
            data to MapR FS. The configuration options are essentially the same.... </p>

        <p class="p">When you configure the destination, instead of configuring a directory template, you
            configure the destination to use the directory in the record header. Instead of
            configuring an Avro schema, you indicate you have a schema in the record header. And you
            configure the destination to roll files when it sees a "roll" attribute in the record
            header. </p>

        <p class="p">The Output Files tab of the destination might look something like this:</p>

        <p class="p"><img class="image" id="concept_jzr_ypn_fw__image_sbv_xrn_fw" src="../Graphics/HiveMeta-Ex-HDFS.png" height="466" width="494"></img></p>

        <p class="p">And the Avro tab looks like this:</p>

        <p class="p"><img class="image" id="concept_jzr_ypn_fw__image_wzp_1sn_fw" src="../Graphics/HiveMeta-Ex-HDFS-Avro.png" height="276" width="469"></img></p>

        <p class="p">With this configuration, the destination uses the information in record header attributes
            to write data to HDFS. It writes each record to the directory in the targetDirectory
            header attribute, using the Avro schema in the avroSchema header attribute. And it rolls
            a file when it spots the roll attribute in a record header. </p>

        <p class="p">Note that the destination can also use Max Records in File and Max Files Size to
            determine when to roll files.</p>

 </div>

</div>
<div class="topic concept nested1" id="concept_jlr_zlk_gw">
 <h2 class="title topictitle2">Processing Data</h2>

    
    <div class="body conbody"><p class="shortdesc">Now what happens when you start the pipeline?</p>

        <p class="p">This pipeline is set up to write data to different tables based on the table name in the
            "tag" attribute that was added to the record headers in the earlier pipeline. </p>

        <div class="p">Say the table names are "weblog" and "service". For each record with "weblog" as the tag
            attribute, the Hive Metadata processor evaluates the fields in the record as follows:
                <ul class="ul" id="concept_jlr_zlk_gw__ul_iml_1mk_gw">
                <li class="li">If the fields match the existing Hive table, it just writes the necessary
                    information into the targetDirectory and avroSchema stage attributes, and Hadoop
                    FS writes the record to the weblog table.</li>

                <li class="li">If a record includes a new field, the processor generates a metadata record that
                    the Hive Metastore destination uses to update the weblog table to include the
                    new column. It also writes information to stage attributes so Hadoop FS can
                    write the record to the updated weblog table.</li>

                <li class="li">If a record has missing fields, the processor just writes information to stage
                    attributes, and Hadoop FS writes the record to HDFS with null values for the
                    missing fields.</li>

                <li class="li">If a field has been renamed, the processor treats the field as a new field,
                    generating a metadata record that the Hive Metastore destination uses to update
                    the weblog table. When Hadoop FS writes the record, data is written to the new
                    field and a null value to the old field.</li>

                <li class="li">If a data type changes for an existing field, the processor treats the record as
                    an error record.</li>

            </ul>
</div>

        <p class="p">For each record with a "service" tag, the processor performs the same actions.</p>

        <div class="note note"><span class="notetitle">Note:</span> If a record includes a new tag value, the Hive Metadata processor generates a metadata
            record that the Hive Metastore destination uses to create a new table. And Hadoop FS
            writes the record to the new table. So if you spin up a new web service, you don't need
            to touch this pipeline to have it handle the new data set. </div>

    </div>

</div>
</div>
<div class="navfooter"><!---->
<span class="navparent"><a class="link" href="../Hive_Metadata/HiveDriftSolution_title.html" title="Hive Drift Solution"><span class="navheader_label">Parent topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Hive Drift Solution</span></a></span>  </div><div class="footer"><!-- SDC google analytics --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-60917135-3', 'auto');
  ga('send', 'pageview');
</script></div>
</body>
</html>